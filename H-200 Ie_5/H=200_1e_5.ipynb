{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cWACPRL869I4","executionInfo":{"status":"ok","timestamp":1660932405058,"user_tz":420,"elapsed":3715,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[],"source":["!pip install gym >/dev/null"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2Os6feRY6ec_","executionInfo":{"status":"ok","timestamp":1660932410583,"user_tz":420,"elapsed":5539,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[],"source":["!pip install JSAnimation >/dev/null"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wotUOa_e6edP","executionInfo":{"status":"ok","timestamp":1660932410584,"user_tz":420,"elapsed":18,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[],"source":["%matplotlib inline\n","from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"R66_INeZ9nYX"},"source":["## Step 2: Playing Pong"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ngMhg3fB9aA","outputId":"687c362d-1bdb-44ac-e731-bda5ef4f9810","executionInfo":{"status":"ok","timestamp":1660932438623,"user_tz":420,"elapsed":28056,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n","Collecting ale-py~=0.7.5\n","  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 32.3 MB/s \n","\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=0c650959adffdfef21b9bb012ce0cc685c391a67292555d06d3528656de00ab2\n","  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"]}],"source":["%pip install -U gym>=0.21.0\n","%pip install -U gym[atari,accept-rom-license]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtT2GyK_6edc","outputId":"4c88a281-1b9a-471d-b107-508ef6a2a704","executionInfo":{"status":"ok","timestamp":1660932439561,"user_tz":420,"elapsed":954,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  f\"The environment {id} is out of date. You should consider \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"]}],"source":["import gym\n","env = gym.make('Pong-v0')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRE6WmXQJ1Z0","outputId":"40a4a514-5622-4559-c8d7-3615b66b86fe","executionInfo":{"status":"ok","timestamp":1660932439562,"user_tz":420,"elapsed":15,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(6)"]},"metadata":{},"execution_count":6}],"source":["env.action_space"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yl_9d4HFJ31W","outputId":"596c9bac-e494-483d-d951-654477974fcb","executionInfo":{"status":"ok","timestamp":1660932439563,"user_tz":420,"elapsed":13,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0, 255, (210, 160, 3), uint8)"]},"metadata":{},"execution_count":7}],"source":["env.observation_space"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trwRXI-h6eeI","outputId":"f54e521a-15df-4168-efe9-618e668171ee","executionInfo":{"status":"ok","timestamp":1660932439957,"user_tz":420,"elapsed":404,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  \"The argument mode in render method is deprecated; \"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n","  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  \"Core environment is written in old step API which returns one bool instead of two. \"\n"]},{"output_type":"stream","name":"stdout","text":["Episode finished without success, accumulated reward = -19.0\n"]}],"source":["# Run a demo of the environment\n","observation = env.reset()\n","cumulated_reward = 0\n","\n","frames = []\n","for t in range(1000):\n","#     print(observation)\n","    frames.append(env.render(mode = 'rgb_array'))\n","    # very stupid agent, just makes a random action within the allowd action space\n","    action = env.action_space.sample()\n","#     print(\"Action: {}\".format(t+1))    \n","    observation, reward, done, info = env.step(action)\n","#     print(reward)\n","    cumulated_reward += reward\n","    if done:\n","        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","        break\n","print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","\n","env.close()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"3zZTecVWLLes","executionInfo":{"status":"ok","timestamp":1660932439958,"user_tz":420,"elapsed":7,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[],"source":["def sigmoid(x): \n","  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n","\n","def prepro(I):\n","  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n","  I = I[35:195] # crop\n","  I = I[::2,::2,0] # downsample by factor of 2\n","  I[I == 144] = 0 # erase background (background type 1)\n","  I[I == 109] = 0 # erase background (background type 2)\n","  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n","  return I.astype(np.float).ravel()\n","\n","def policy_forward(x):\n","  h = np.dot(model['W1'], x)\n","  h[h<0] = 0 # ReLU nonlinearity\n","  logp = np.dot(model['W2'], h)\n","  p = sigmoid(logp)\n","  return p, h # return probability of taking action 2, and hidden state\n","\n","def model_step(model, observation, prev_x):\n","  # preprocess the observation, set input to network to be difference image\n","  cur_x = prepro(observation)\n","  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","  prev_x = cur_x\n","  \n","  # forward the policy network and sample an action from the returned probability\n","  aprob, _ = policy_forward(x)\n","  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n","  \n","  return action, prev_x\n","\n","def play_game(env, model):\n","  observation = env.reset()\n","\n","  frames = []\n","  cumulated_reward = 0\n","\n","  prev_x = None # used in computing the difference frame\n","\n","  for t in range(1000):\n","      frames.append(env.render(mode = 'rgb_array'))\n","      action, prev_x = model_step(model, observation, prev_x)\n","      observation, reward, done, info = env.step(action)\n","      cumulated_reward += reward\n","      if done:\n","          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","          break\n","  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","  display_frames_as_gif(frames)\n","  env.close()"]},{"cell_type":"markdown","metadata":{"id":"6gWvZQ7AQLQt"},"source":["## Step 3: Policy Gradient from Scratch"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"eqFm7hqcItWl","executionInfo":{"status":"ok","timestamp":1660932439959,"user_tz":420,"elapsed":8,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[],"source":["import numpy as np\n","\n","# model initialization\n","H = 200 # number of hidden layer neurons\n","D = 80 * 80 # input dimensionality: 80x80 grid\n","model = {}\n","model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n","model['W2'] = np.random.randn(H) / np.sqrt(H)\n","\n","# import pickle\n","# model = pickle.load(open('model.pkl', 'rb'))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"TwjiwKisQM19","executionInfo":{"status":"ok","timestamp":1660932440334,"user_tz":420,"elapsed":382,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[],"source":["# hyperparameters\n","batch_size = 10 # every how many episodes to do a param update?\n","# learning_rate = 1e-4\n","learning_rate = 1e-5\n"," \n","gamma = 0.99 # discount factor for reward\n","decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n","  \n","grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n","rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n","\n","def discount_rewards(r):\n","  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n","  discounted_r = np.zeros_like(r, dtype=np.float32)\n","  running_add = 0\n","  for t in reversed(range(0, r.size)):\n","    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n","    running_add = running_add * gamma + r[t]\n","    discounted_r[t] = running_add\n","  return discounted_r\n","\n","def policy_backward(epx, eph, epdlogp):\n","  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n","  dW2 = np.dot(eph.T, epdlogp).ravel()\n","  dh = np.outer(epdlogp, model['W2'])\n","  dh[eph <= 0] = 0 # backpro prelu\n","  dW1 = np.dot(dh.T, epx)\n","  return {'W1':dW1, 'W2':dW2}\n","\n","def train_model(env, model, total_episodes = 100):\n","  hist = []\n","  observation = env.reset()\n","\n","  prev_x = None # used in computing the difference frame\n","  xs,hs,dlogps,drs = [],[],[],[]\n","  running_reward = None\n","  reward_sum = 0\n","  episode_number = 0\n","\n","  while True:\n","    # preprocess the observation, set input to network to be difference image\n","    cur_x = prepro(observation)\n","    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","    prev_x = cur_x\n","\n","    # forward the policy network and sample an action from the returned probability\n","    aprob, h = policy_forward(x)\n","    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n","\n","    # record various intermediates (needed later for backprop)\n","    xs.append(x) # observation\n","    hs.append(h) # hidden state\n","    y = 1 if action == 2 else 0 # a \"fake label\"\n","    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n","\n","    # step the environment and get new measurements\n","    observation, reward, done, info = env.step(action)\n","    reward_sum += reward\n","\n","    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n","\n","    if done: # an episode finished\n","      episode_number += 1\n","\n","      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n","      epx = np.vstack(xs)\n","      eph = np.vstack(hs)\n","      epdlogp = np.vstack(dlogps)\n","      epr = np.vstack(drs)\n","      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n","\n","      # compute the discounted reward backwards through time\n","      discounted_epr = discount_rewards(epr)\n","      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n","      discounted_epr -= np.mean(discounted_epr)\n","      discounted_epr /= np.std(discounted_epr)\n","\n","      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n","      grad = policy_backward(epx, eph, epdlogp)\n","      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n","\n","      # perform rmsprop parameter update every batch_size episodes\n","      if episode_number % batch_size == 0:\n","        for k,v in model.items():\n","          g = grad_buffer[k] # gradient\n","          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n","          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n","          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n","\n","      # boring book-keeping\n","      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n","      hist.append((episode_number, reward_sum, running_reward))\n","      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n","      reward_sum = 0\n","      observation = env.reset() # reset env\n","      prev_x = None\n","      if episode_number == total_episodes: return hist\n","\n","      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n","        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6Ka_5Vl9Orm","outputId":"331ec249-7275-4039-bcbd-ddf91d599270","executionInfo":{"status":"ok","timestamp":1660933469235,"user_tz":420,"elapsed":1028905,"user":{"displayName":"ACV TestPrep","userId":"09441606846061621634"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n","resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n","resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n","resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049601\n","resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.069105\n","resetting env. episode 6.000000, reward total was -19.000000. running mean: -19.068414\n","resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.087730\n","resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.106853\n","resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.125784\n","resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.144526\n","resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.153081\n","resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.171550\n","resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.189835\n","resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.207936\n","resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.225857\n","resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.233598\n","resetting env. episode 17.000000, reward total was -18.000000. running mean: -19.221262\n","resetting env. episode 18.000000, reward total was -19.000000. running mean: -19.219050\n","resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.226859\n","resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.244591\n","resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.262145\n","resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.279523\n","resetting env. episode 23.000000, reward total was -20.000000. running mean: -19.286728\n","resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.303861\n","resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.320822\n","resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.337614\n","resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.354238\n","resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.360695\n","resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.377088\n","resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.393318\n","resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.399384\n","resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.405391\n","resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.421337\n","resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.427123\n","resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.432852\n","resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.448524\n","resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.464038\n","resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.479398\n","resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.484604\n","resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.489758\n","resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.504860\n","resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.509812\n","resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.514714\n","resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.529566\n","resetting env. episode 45.000000, reward total was -20.000000. running mean: -19.534271\n","resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.548928\n","resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.563439\n","resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.577804\n","resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.592026\n","resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.606106\n","resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.620045\n","resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.633845\n","resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.637506\n","resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.641131\n","resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.654720\n","resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.668173\n","resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.671491\n","resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.684776\n","resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.697928\n","resetting env. episode 60.000000, reward total was -19.000000. running mean: -19.690949\n","resetting env. episode 61.000000, reward total was -19.000000. running mean: -19.684039\n","resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.697199\n","resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.710227\n","resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.723125\n","resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.725893\n","resetting env. episode 66.000000, reward total was -20.000000. running mean: -19.728635\n","resetting env. episode 67.000000, reward total was -18.000000. running mean: -19.711348\n","resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.714235\n","resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.727092\n","resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.739821\n","resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.742423\n","resetting env. episode 72.000000, reward total was -19.000000. running mean: -19.734999\n","resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.747649\n","resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.760173\n","resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.762571\n","resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.764945\n","resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.777296\n","resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.789523\n","resetting env. episode 79.000000, reward total was -19.000000. running mean: -19.781627\n","resetting env. episode 80.000000, reward total was -19.000000. running mean: -19.773811\n","resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.786073\n","resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.798212\n","resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.810230\n","resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.822128\n","resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.823907\n","resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.835668\n","resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.837311\n","resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.848938\n","resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.860448\n","resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.871844\n","resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.883125\n","resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.894294\n","resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.905351\n","resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.916298\n","resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.917135\n","resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.927963\n","resetting env. episode 97.000000, reward total was -19.000000. running mean: -19.918684\n","resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.909497\n","resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.920402\n","resetting env. episode 100.000000, reward total was -20.000000. running mean: -19.921198\n","resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.931986\n","resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.942666\n","resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.953239\n","resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.963707\n","resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.964070\n","resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.974429\n","resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.974685\n","resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.984938\n","resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.985089\n","resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.995238\n","resetting env. episode 111.000000, reward total was -19.000000. running mean: -19.985286\n","resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.995433\n","resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.995478\n","resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.005524\n","resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.005468\n","resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.015414\n","resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.015260\n","resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.025107\n","resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.014856\n","resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.024707\n","resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.034460\n","resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.044116\n","resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.053674\n","resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.043138\n","resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.052706\n","resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.052179\n","resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.061657\n","resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.071041\n","resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.080330\n","resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.089527\n","resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.098632\n","resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.107646\n","resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.116569\n","resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.125403\n","resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.124149\n","resetting env. episode 136.000000, reward total was -18.000000. running mean: -20.102908\n","resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.111879\n","resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.120760\n","resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.119552\n","resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.118357\n","resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.117173\n","resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.106002\n","resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.114942\n","resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.103792\n","resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.112754\n","resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.121627\n","resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.120410\n","resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.129206\n","resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.137914\n","resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.146535\n","resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.155070\n","resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.163519\n","resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.171884\n","resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.170165\n","resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.178463\n","resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.176679\n","resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.184912\n","resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.193063\n","resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.201132\n","resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.189121\n","resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.197230\n","resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.195257\n","resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.203305\n","resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.211272\n","resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.209159\n","resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.217067\n","resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.224897\n","resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.222648\n","resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.220421\n","resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.228217\n","resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.215935\n","resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.213776\n","resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.211638\n","resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.219521\n","resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.227326\n","resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.235053\n","resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.232702\n","resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.240375\n","resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.247972\n","resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.255492\n","resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.262937\n","resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.260308\n","resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.267705\n","resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.275028\n","resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.282277\n","resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.269455\n","resetting env. episode 187.000000, reward total was -18.000000. running mean: -20.246760\n","resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.234292\n","resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.241949\n","resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.249530\n","resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.257035\n","resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.264464\n","resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.261820\n","resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.249201\n","resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.256709\n","resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.254142\n","resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.261601\n","resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.258985\n","resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.266395\n","resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.253731\n","resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.251194\n","resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.258682\n","resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.266095\n","resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.263434\n","resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.270800\n","resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.278092\n","resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.285311\n","resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.292458\n","resetting env. episode 209.000000, reward total was -18.000000. running mean: -20.269533\n","resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.276838\n","resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.284069\n","resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.291229\n","resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.288316\n","resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.295433\n","resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.302479\n","resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.309454\n","resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.306360\n","resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.313296\n","resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.300163\n","resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.307161\n","resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.304090\n","resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.291049\n","resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.298138\n","resetting env. episode 224.000000, reward total was -17.000000. running mean: -20.265157\n","resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.272506\n","resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.279780\n","resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.286983\n","resetting env. episode 228.000000, reward total was -18.000000. running mean: -20.264113\n","resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.271472\n","resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.278757\n","resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.265969\n","resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.263310\n","resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.260677\n","resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.268070\n","resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.275389\n","resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.272635\n","resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.279909\n","resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.287110\n","resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.284239\n","resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.291396\n","resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.298482\n","resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.285498\n","resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.292643\n","resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.299716\n","resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.306719\n","resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.313652\n","resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.310515\n","resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.307410\n","resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.304336\n","resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.301293\n","resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.308280\n","resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.315197\n","resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.312045\n","resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.318925\n","resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.325735\n","resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.332478\n","resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.339153\n","resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.345762\n","resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.352304\n","resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.348781\n","resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.345293\n","resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.341840\n","resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.348422\n","resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.354938\n","resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.361388\n","resetting env. episode 266.000000, reward total was -18.000000. running mean: -20.337774\n","resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.344397\n","resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.350953\n","resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.357443\n","resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.353869\n","resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.360330\n","resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.366727\n","resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.373059\n","resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.369329\n","resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.365636\n","resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.361979\n","resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.368359\n","resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.374676\n","resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.380929\n","resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.387120\n","resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.373249\n","resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.379516\n","resetting env. episode 283.000000, reward total was -18.000000. running mean: -20.355721\n","resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.362164\n","resetting env. episode 285.000000, reward total was -17.000000. running mean: -20.328542\n","resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.325257\n","resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.312004\n","resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.318884\n","resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.325695\n","resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.332438\n","resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.329114\n","resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.315823\n","resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.322664\n","resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.329438\n","resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.336143\n","resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.342782\n","resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.339354\n","resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.345961\n","resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.352501\n","resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.358976\n","resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.365386\n","resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.371732\n","resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.378015\n","resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.384235\n","resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.370393\n","resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.376689\n","resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.372922\n","resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.379193\n","resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.385401\n","resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.381547\n","resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.387731\n","resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.393854\n","resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.399915\n","resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.405916\n","resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.401857\n","resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.407838\n","resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.413760\n","resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.419622\n","resetting env. episode 319.000000, reward total was -18.000000. running mean: -20.395426\n","resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.391472\n","resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.397557\n","resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.383582\n","resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.369746\n","resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.376048\n","resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.372288\n","resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.378565\n","resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.384779\n","resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.390932\n","resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.397022\n","resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.393052\n","resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.399122\n","resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.405130\n","resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.411079\n","resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.416968\n","resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.422799\n","resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.428571\n","resetting env. episode 337.000000, reward total was -18.000000. running mean: -20.404285\n","resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.410242\n","resetting env. episode 339.000000, reward total was -18.000000. running mean: -20.386140\n","resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.392278\n","resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.398355\n","resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.404372\n","resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.400328\n","resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.396325\n","resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.392362\n","resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.398438\n","resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.404454\n","resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.400409\n","resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.406405\n","resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.412341\n","resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.418218\n","resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.424035\n","resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.429795\n","resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.435497\n","resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.441142\n","resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.446731\n","resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.452263\n","resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.447741\n","resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.443263\n","resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.448831\n","resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.454342\n","resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.439799\n","resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.445401\n","resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.450947\n","resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.456437\n","resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.461873\n","resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.467254\n","resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.462582\n","resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.447956\n","resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.433476\n","resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.439142\n","resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.434750\n","resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.430403\n","resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.436099\n","resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.431738\n","resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.437420\n","resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.443046\n","resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.438616\n","resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.434230\n","resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.439887\n","resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.435488\n","resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.421133\n","resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.426922\n","resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.432653\n","resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.438326\n","resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.443943\n","resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.439504\n","resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.445109\n","resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.450658\n","resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.456151\n","resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.461590\n","resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.456974\n","resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.462404\n","resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.457780\n","resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.463202\n","resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.458570\n","resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.463984\n","resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.469344\n","resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.464651\n","resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.460005\n","resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.445404\n","resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.440950\n","resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.446541\n","resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.452076\n","resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.457555\n","resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.452979\n","resetting env. episode 407.000000, reward total was -18.000000. running mean: -20.428449\n","resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.424165\n","resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.429923\n","resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.425624\n","resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.431368\n","resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.437054\n","resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.442684\n","resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.428257\n","resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.423974\n","resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.429734\n","resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.435437\n","resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.441083\n","resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.446672\n","resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.442205\n","resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.447783\n","resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.433305\n","resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.438972\n","resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.434583\n","resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.440237\n","resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.435834\n","resetting env. episode 427.000000, reward total was -18.000000. running mean: -20.411476\n","resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.417361\n","resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.423188\n","resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.418956\n","resetting env. episode 431.000000, reward total was -18.000000. running mean: -20.394766\n","resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.380819\n","resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.387010\n","resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.393140\n","resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.389209\n","resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.395317\n","resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.391364\n","resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.397450\n","resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.383475\n","resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.389641\n","resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.385744\n","resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.381887\n","resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.368068\n","resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.364387\n","resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.370743\n","resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.357036\n","resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.363466\n","resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.359831\n","resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.366233\n","resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.362570\n","resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.358945\n","resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.365355\n","resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.371702\n","resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.367985\n","resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.374305\n","resetting env. episode 456.000000, reward total was -18.000000. running mean: -20.350562\n","resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.357056\n","resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.353486\n","resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.359951\n","resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.356351\n","resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.362788\n","resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.359160\n","resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.365568\n","resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.371913\n","resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.378193\n","resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.384411\n","resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.390567\n","resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.386662\n","resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.392795\n","resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.398867\n","resetting env. episode 471.000000, reward total was -17.000000. running mean: -20.364878\n","resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.371230\n","resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.357517\n","resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.363942\n","resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.370303\n","resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.376600\n","resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.382834\n","resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.389005\n","resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.395115\n","resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.401164\n","resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.407153\n","resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.413081\n","resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.418950\n","resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.414761\n","resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.420613\n","resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.416407\n","resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.422243\n","resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.408020\n","resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.413940\n","resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.419801\n","resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.415603\n","resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.411447\n","resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.407332\n","resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.413259\n","resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.399126\n","resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.405135\n","resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.411084\n","resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.396973\n","resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.393003\n","resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.399073\n","CPU times: user 22min 28s, sys: 10min 32s, total: 33min\n","Wall time: 17min 7s\n"]}],"source":["%time hist1 = train_model(env, model, total_episodes=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHYCDYwhlVLV","outputId":"7ee2491c-7b0b-4767-e555-8fe75135ff23"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 5.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.990000\n"]}],"source":["%time hist2 = train_model(env, model, total_episodes=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fheN9DRlWXQ"},"outputs":[],"source":["play_game(env, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AxOcQhIsKow"},"outputs":[],"source":["%time hist3 = train_model(env, model, total_episodes=1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2NblmwDsL3y"},"outputs":[],"source":["play_game(env, model)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"H=200_1e_5.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}