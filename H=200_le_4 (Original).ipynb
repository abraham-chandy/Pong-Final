{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H=200 le-4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "9e09d6c5-8b9d-4911-c97f-c07aafa10c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.7)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "6ef17a84-3563-4157-caf3-2c50438190ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "41a4b484-4f7e-4fd6-fc69-e626adec0523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "a72c9abf-bffb-45f6-f127-26ec1d6b607c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "42368b9e-2477-41ef-fb42-30ced75282c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "75de5744-f8e4-4aea-c00f-2cb2977a38cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.049010\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.048520\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.048035\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.057554\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.056979\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.066409\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.075745\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.084987\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.084138\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.093296\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.102363\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.111340\n",
            "resetting env. episode 18.000000, reward total was -18.000000. running mean: -20.090226\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.089324\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.098431\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.107446\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.116372\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.105208\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.104156\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.103115\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.112083\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.120963\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.129753\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.128455\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.137171\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.145799\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.154341\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.162798\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.161170\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.169558\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.177863\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.176084\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.184323\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.192480\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.190555\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.198649\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -20.186663\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.194796\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.202848\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.210820\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.218712\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.226525\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.234259\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.241917\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.249498\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.247003\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.244533\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.252087\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.239566\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.247171\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.254699\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.262152\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.269530\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.276835\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.274067\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.271326\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.278613\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.265827\n",
            "resetting env. episode 64.000000, reward total was -18.000000. running mean: -20.243169\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.250737\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.258229\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.265647\n",
            "resetting env. episode 68.000000, reward total was -18.000000. running mean: -20.242991\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.240561\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.248155\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.255674\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.253117\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.250586\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.258080\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.245499\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.253044\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.260514\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.257908\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.265329\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.272676\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.279949\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.287150\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.294278\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.301336\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.308322\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.315239\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.322087\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.328866\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.325577\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.332321\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.318998\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.325808\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.322550\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.329325\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.336031\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.342671\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.349244\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.355752\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.342194\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.338772\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.335385\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.332031\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.338710\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.335323\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.341970\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.338550\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.345165\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.341713\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.348296\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.354813\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.351265\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.357752\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.364175\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.370533\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.366828\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.373160\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.379428\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.385634\n",
            "resetting env. episode 119.000000, reward total was -18.000000. running mean: -20.361777\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.368160\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.354478\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.350933\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.357424\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.363850\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.350211\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.356709\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.363142\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.369510\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.365815\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.352157\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.358636\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.345049\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.351599\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.358083\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.364502\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.360857\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.357248\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.353676\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.360139\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.356538\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.352972\n",
            "resetting env. episode 142.000000, reward total was -18.000000. running mean: -20.329443\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.336148\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.342787\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.349359\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.355865\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.352307\n",
            "resetting env. episode 148.000000, reward total was -17.000000. running mean: -20.318784\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.325596\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.322340\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.319116\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.325925\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.332666\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.339339\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -20.325946\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.312686\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.309560\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.316464\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.323299\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.310066\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.316966\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.323796\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.320558\n",
            "resetting env. episode 164.000000, reward total was -18.000000. running mean: -20.297353\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.304379\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.311335\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.318222\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.315040\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.291889\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.298970\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.305981\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.302921\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.289892\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.286993\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.294123\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.301182\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.308170\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.315088\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.321937\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.328718\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.335431\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.342076\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.328656\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.335369\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.342015\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.338595\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.345209\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.341757\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.348340\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.354856\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.361308\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.357694\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.364118\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -20.340476\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.347072\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.353601\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.350065\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.356564\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.352999\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.349469\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.355974\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.352414\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.358890\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.355301\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.361748\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.358131\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.364549\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.370904\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.367195\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.363523\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.359888\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.346289\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.342826\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.349398\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.355904\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.352345\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -20.328821\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.335533\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.342178\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.348756\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.355268\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.361716\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.368098\n",
            "resetting env. episode 224.000000, reward total was -17.000000. running mean: -20.334417\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.341073\n",
            "resetting env. episode 226.000000, reward total was -18.000000. running mean: -20.317663\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.324486\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.321241\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.328029\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.324748\n",
            "resetting env. episode 231.000000, reward total was -18.000000. running mean: -20.301501\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.308486\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.315401\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.322247\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.329025\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.315734\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.322577\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.319351\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.316158\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.322996\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.329766\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.336468\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.343104\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.349673\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.346176\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.352714\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.339187\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.325795\n",
            "resetting env. episode 249.000000, reward total was -18.000000. running mean: -20.302537\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.309512\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.306417\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.303353\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.310319\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.317216\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.304044\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.291003\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.298093\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.305112\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.302061\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.309041\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.305950\n",
            "resetting env. episode 262.000000, reward total was -18.000000. running mean: -20.282891\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.290062\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.287161\n",
            "resetting env. episode 265.000000, reward total was -18.000000. running mean: -20.264290\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.271647\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.278930\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.286141\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.293279\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.290347\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.287443\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.284569\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.291723\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.288806\n",
            "resetting env. episode 275.000000, reward total was -18.000000. running mean: -20.265918\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.263259\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.250626\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.258120\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.265539\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.272883\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.280154\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.277353\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.274579\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.271834\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.279115\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.286324\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.293461\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.300526\n",
            "resetting env. episode 289.000000, reward total was -17.000000. running mean: -20.267521\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.274846\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.282097\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.289276\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.276384\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.283620\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.290783\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.297876\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.304897\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.311848\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.308729\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.315642\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.312486\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.319361\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.326167\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.332906\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.339577\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.346181\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.342719\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.349292\n",
            "resetting env. episode 309.000000, reward total was -18.000000. running mean: -20.325799\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.332541\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.329215\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.335923\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.332564\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.339238\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.345846\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.352388\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.358864\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.365275\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.371622\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.357906\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.364327\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.370684\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.356977\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.363407\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.349773\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.356275\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.362713\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.369085\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.375395\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.381641\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.387824\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.393946\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.400007\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.406006\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.401946\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.407927\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.413848\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.419709\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.425512\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.421257\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.417044\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.412874\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.398745\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.384758\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.390910\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.387001\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.393131\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.389200\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.395308\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.391355\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.387441\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.393567\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.399631\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.405635\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.411578\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.417463\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.423288\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.429055\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.434765\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.430417\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.426113\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.421852\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.407633\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.413557\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.419421\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.415227\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.421075\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.426864\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.432595\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.438269\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.443887\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.449448\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.454953\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.460404\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.455800\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.451242\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.446729\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.452262\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.457739\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.453162\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.458630\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.454044\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.459504\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.454909\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.460360\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.455756\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -20.431198\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.426886\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.422618\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.428391\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.424107\n",
            "resetting env. episode 392.000000, reward total was -17.000000. running mean: -20.389866\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.395968\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.392008\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.398088\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.404107\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.400066\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.406065\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.402005\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.407985\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.413905\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.399766\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.405768\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.401710\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.387693\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.393816\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.389878\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.395979\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.402020\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.397999\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.394019\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.390079\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.386178\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.382317\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.388494\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.394609\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.400663\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.406656\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.402589\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.398563\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.404578\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.390532\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.396627\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.402660\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.398634\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.404647\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.390601\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.396695\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.402728\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.398701\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.394714\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.390767\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.396859\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.392890\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.368961\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.365272\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.371619\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.377903\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.374124\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.380383\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.386579\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.392713\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.398786\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.384798\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.390950\n",
            "resetting env. episode 446.000000, reward total was -18.000000. running mean: -20.367041\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.353370\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.359836\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.366238\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.352576\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.359050\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.355459\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.361905\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.348286\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.344803\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.351355\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.357841\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.364263\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.370620\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.356914\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.353345\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.349812\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.356313\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.362750\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.369123\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.365432\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.371777\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.368059\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.364379\n",
            "resetting env. episode 470.000000, reward total was -18.000000. running mean: -20.340735\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.347328\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.353854\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.350316\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.346813\n",
            "resetting env. episode 475.000000, reward total was -17.000000. running mean: -20.313345\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.310211\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.307109\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.314038\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.320898\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.327689\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.324412\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.321168\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.317956\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.314776\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.321629\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.308412\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.305328\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.292275\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.279352\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.276559\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.273793\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.281055\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.288245\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.295362\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.302409\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.289384\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.286491\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.293626\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.290689\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.277783\n",
            "CPU times: user 23min 9s, sys: 10min 44s, total: 33min 53s\n",
            "Wall time: 17min 37s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "44023870-3ed0-46f5-84e7-df559e94ffc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.039800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.059402\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -19.058808\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.068220\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -19.077538\n",
            "resetting env. episode 8.000000, reward total was -18.000000. running mean: -19.066762\n",
            "resetting env. episode 9.000000, reward total was -18.000000. running mean: -19.056095\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.075534\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.084778\n",
            "resetting env. episode 12.000000, reward total was -18.000000. running mean: -19.073931\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.083191\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.092359\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -19.101436\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.110421\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.119317\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -19.118124\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.136943\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.155573\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.174018\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.192277\n",
            "resetting env. episode 23.000000, reward total was -18.000000. running mean: -19.180355\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.198551\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.216566\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.234400\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.252056\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.259535\n",
            "resetting env. episode 29.000000, reward total was -18.000000. running mean: -19.246940\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.264471\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.281826\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.299008\n",
            "resetting env. episode 33.000000, reward total was -18.000000. running mean: -19.286018\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.293157\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.310226\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.327124\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.343852\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.360414\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -19.356810\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.373242\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.389509\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.405614\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.411558\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.427442\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -19.423168\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -19.418936\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.434747\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.450399\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -19.445895\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.461437\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -19.456822\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.472254\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -19.467531\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.482856\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.498028\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.513047\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.517917\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.532738\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.547410\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.561936\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.576317\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.590554\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.604648\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.618602\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.632416\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.646091\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.659630\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.673034\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.686304\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.699441\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.702446\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.715422\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -19.718268\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.721085\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.723874\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.726635\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.739369\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.751975\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.764456\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -19.756811\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.759243\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.771651\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.783934\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -19.766095\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.768434\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.770749\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.783042\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.795212\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.807259\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.819187\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.820995\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.822785\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.834557\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.836212\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.847849\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.859371\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.870777\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.862069\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.873449\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.884714\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.895867\n",
            "resetting env. episode 102.000000, reward total was -18.000000. running mean: -19.876908\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.878139\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -19.869358\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.880664\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.891858\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.892939\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -19.884010\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.895170\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.906218\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -19.897156\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.908184\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.909102\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.920011\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -19.920811\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.931603\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.942287\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.952864\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.953336\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.963802\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.964164\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -19.974523\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.974777\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.985030\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.995179\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.005228\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.015175\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.015023\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.024873\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.034625\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.034278\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.043935\n",
            "resetting env. episode 133.000000, reward total was -18.000000. running mean: -20.023496\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.023261\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.033029\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.042698\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.052271\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.051749\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.051231\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.060719\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.070112\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.079410\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.078616\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.087830\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.096952\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.105982\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.114923\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.123773\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -20.112536\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.101410\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.110396\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.119292\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.128099\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.116818\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -20.105650\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.104594\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.093548\n",
            "resetting env. episode 158.000000, reward total was -19.000000. running mean: -20.082612\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.091786\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.080868\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.090060\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.099159\n",
            "resetting env. episode 163.000000, reward total was -17.000000. running mean: -20.068167\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.067486\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.076811\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.086043\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.095182\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.104230\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.103188\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.092156\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.081235\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.090422\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.089518\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.098623\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.097637\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.096660\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.105694\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.104637\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.103590\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.112555\n",
            "resetting env. episode 181.000000, reward total was -18.000000. running mean: -20.091429\n",
            "resetting env. episode 182.000000, reward total was -18.000000. running mean: -20.070515\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.079810\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.089011\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.098121\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.107140\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.116069\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.124908\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.133659\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.132322\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.140999\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.149589\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.158093\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.166512\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.164847\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.163199\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.161567\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.169951\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.168252\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.166569\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.174903\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.183154\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.181323\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.189510\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.197614\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.205638\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.213582\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.221446\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.209232\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.217139\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.214968\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.222818\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.220590\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.218384\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.216200\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.214038\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.211898\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.219779\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.217581\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.215405\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.213251\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.221119\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.228908\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.216619\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.204452\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.212408\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.210284\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.218181\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.215999\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.213839\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.221701\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.209484\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.217389\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.225215\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.222963\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.230733\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.238426\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.246042\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.253581\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.251045\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.248535\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.236050\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.243689\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.251252\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.258740\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.266152\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.263491\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.260856\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.258247\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.255665\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.263108\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.250477\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.247972\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.255493\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.262938\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.250308\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.257805\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.265227\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.252575\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.260049\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.247449\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.254974\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.262424\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.269800\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.277102\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.274331\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.271588\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.258872\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.266283\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.273620\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.270884\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.258175\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.265594\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.262938\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.270308\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.267605\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.274929\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.262180\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.259558\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.266962\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.264293\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.271650\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.278933\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.286144\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.293283\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.300350\n",
            "resetting env. episode 287.000000, reward total was -18.000000. running mean: -20.277346\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.274573\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.281827\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.279009\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.286219\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.283357\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.290523\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.277618\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.284842\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.291993\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.289073\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.276183\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.273421\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.270687\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.267980\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.265300\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.272647\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.269920\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.267221\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.274549\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.281803\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.278985\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.286196\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.283334\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.290500\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.297595\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.304619\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.311573\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.318457\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.315273\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.322120\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.308899\n",
            "resetting env. episode 319.000000, reward total was -18.000000. running mean: -20.285810\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.292952\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.280022\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.267222\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.264550\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.261904\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.269285\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.266592\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.263927\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.261287\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.268674\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.265988\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.263328\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.270695\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.277988\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.285208\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.282356\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.269532\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.256837\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.264268\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.261626\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.269009\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.276319\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.283556\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.290721\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.287813\n",
            "resetting env. episode 345.000000, reward total was -18.000000. running mean: -20.264935\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.252286\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.249763\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.247265\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.254793\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.242245\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.249822\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.257324\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.254751\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.252203\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.259681\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.267085\n",
            "resetting env. episode 357.000000, reward total was -18.000000. running mean: -20.244414\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.251970\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.259450\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.246855\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.234387\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.222043\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.229823\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.217524\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.225349\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.233096\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.240765\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.228357\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.226073\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.223813\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.231575\n",
            "resetting env. episode 372.000000, reward total was -17.000000. running mean: -20.199259\n",
            "resetting env. episode 373.000000, reward total was -18.000000. running mean: -20.177266\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.185494\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.183639\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.181802\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.189984\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.188084\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.176204\n",
            "resetting env. episode 380.000000, reward total was -18.000000. running mean: -20.154441\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.142897\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.151468\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.149953\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.148454\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.156969\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.165400\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.163746\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.172108\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.180387\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.178583\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.186797\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.194929\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.202980\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.210950\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.208841\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.216752\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.214585\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.222439\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.230215\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.217913\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.225733\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.233476\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.221141\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.218930\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.226741\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.214473\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.202328\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.200305\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.198302\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.206319\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.204256\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.202213\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.210191\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.218089\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.225908\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.233649\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.241313\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.238900\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.246511\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.254046\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.261505\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.258890\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.266301\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.273638\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.260902\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.268293\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.275610\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.272854\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.260125\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.267524\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.264849\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.272200\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.279478\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.266683\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.274017\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.271276\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.268564\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.265878\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.263219\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.270587\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.277881\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.285102\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.282251\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.279429\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.286635\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.283768\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.290931\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.298021\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.305041\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.291991\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.299071\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.296080\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.303119\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.300088\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.287087\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.294216\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.291274\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.298361\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.305378\n",
            "resetting env. episode 460.000000, reward total was -18.000000. running mean: -20.282324\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.289501\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.286606\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.293740\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.300802\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.297794\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.304816\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.311768\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.318650\n",
            "resetting env. episode 469.000000, reward total was -18.000000. running mean: -20.295464\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.292509\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.299584\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.296588\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.293622\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.300686\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.307679\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.304603\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.301557\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.308541\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.315456\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.302301\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.309278\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.306185\n",
            "resetting env. episode 483.000000, reward total was -18.000000. running mean: -20.283123\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.290292\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.267389\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.274715\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.281968\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.289149\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.296257\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.293294\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.300362\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.297358\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.284384\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.281540\n",
            "resetting env. episode 495.000000, reward total was -18.000000. running mean: -20.258725\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.246138\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.253676\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.251140\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.258628\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.256042\n",
            "CPU times: user 23min 40s, sys: 10min 52s, total: 34min 32s\n",
            "Wall time: 17min 54s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "23490ad9-3ac8-4899-824b-44400caa8afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "68d93a54-196d-4d91-a6f6-731aa0de23c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.009801\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.019703\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -20.009506\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.019411\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.029217\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.028925\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.028635\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.038349\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.047966\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.047486\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.047011\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.056541\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -20.045975\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.055516\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.064961\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -20.054311\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.063768\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.053130\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.062599\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.071973\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.081253\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.090441\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.099536\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.108541\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.107455\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.116381\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.125217\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.123965\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -20.102725\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.101698\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.100681\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.109674\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.118577\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.117392\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.126218\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -20.104956\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.103906\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.112867\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.121738\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.130521\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.139216\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.137824\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.126445\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.135181\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.133829\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.142491\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.151066\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.149555\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.158060\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.156479\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.164914\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.173265\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.171532\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.179817\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.178019\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.176239\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.174476\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.172732\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.181004\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.189194\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.197302\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.195329\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.193376\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.191442\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.199528\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.207533\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.215457\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.213303\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.211170\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.209058\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.216967\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.204798\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.202750\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.190722\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -20.178815\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.187027\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.195157\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.203205\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.211173\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.209061\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.216971\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.204801\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.212753\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.220625\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.228419\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.236135\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.243774\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.251336\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.258822\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.266234\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.273572\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.280836\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.288028\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.285148\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.292296\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.299373\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.296379\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.303416\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.310381\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.307278\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.304205\n",
            "resetting env. episode 105.000000, reward total was -18.000000. running mean: -20.281163\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.288351\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.295468\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.282513\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.279688\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.286891\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.294022\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.291082\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.288171\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.285289\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.282436\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.289612\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.296716\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.293749\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.300811\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.307803\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.304725\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.301678\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -20.288661\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.295775\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.282817\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.289989\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.297089\n",
            "resetting env. episode 128.000000, reward total was -18.000000. running mean: -20.274118\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.261377\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.248763\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.246275\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -20.223813\n",
            "resetting env. episode 133.000000, reward total was -17.000000. running mean: -20.191574\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.199659\n",
            "resetting env. episode 135.000000, reward total was -16.000000. running mean: -20.157662\n",
            "resetting env. episode 136.000000, reward total was -17.000000. running mean: -20.126085\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.134825\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -20.123476\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.122242\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.131019\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.129709\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.128412\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.127128\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.125856\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.134598\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.133252\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.141919\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.140500\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.149095\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.147604\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.146128\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.144667\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.133220\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.131888\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.130569\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.129263\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.117971\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.126791\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.115523\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.124368\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.133124\n",
            "resetting env. episode 162.000000, reward total was -18.000000. running mean: -20.111793\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.120675\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.109468\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.098374\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.107390\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.116316\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.125153\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.113901\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.102762\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.101735\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.100717\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.099710\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.108713\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.107626\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.116550\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.115384\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.114230\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.113088\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.121957\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.130738\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.139430\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.148036\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.156556\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.144990\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.153540\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.162005\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.170385\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.178681\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.186894\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.195025\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.203075\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.201044\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.199034\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.207043\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.194973\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.183023\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.171193\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.179481\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.187686\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.195809\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.203851\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.211813\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.219695\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.217498\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.215323\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.223169\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.230938\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.238628\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.236242\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.243880\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.241441\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.249026\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.256536\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.263971\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.271331\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.278618\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.285832\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.292973\n",
            "resetting env. episode 220.000000, reward total was -18.000000. running mean: -20.270044\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.277343\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.274570\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.271824\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.279106\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.276315\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.273552\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.280816\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.288008\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.285128\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.292277\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.279354\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.276560\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.283795\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.270957\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.258247\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.255665\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.263108\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.270477\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.277772\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.264994\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.262344\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.269721\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.277024\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.284254\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.281411\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.278597\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.285811\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.282953\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.290123\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.287222\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.294350\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.291406\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.288492\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.285607\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.292751\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.289824\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.286926\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.294056\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.291116\n",
            "resetting env. episode 260.000000, reward total was -17.000000. running mean: -20.258205\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.245623\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.253166\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.260635\n",
            "resetting env. episode 264.000000, reward total was -18.000000. running mean: -20.238028\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.245648\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.253192\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.260660\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.258053\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.265473\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.262818\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.260190\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.257588\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.265012\n",
            "resetting env. episode 274.000000, reward total was -18.000000. running mean: -20.242362\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.249938\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.237439\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.235064\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.222714\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.230487\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.218182\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.216000\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.223840\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.231601\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.229285\n",
            "resetting env. episode 285.000000, reward total was -18.000000. running mean: -20.206993\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.214923\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.202773\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.210746\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.218638\n",
            "resetting env. episode 290.000000, reward total was -18.000000. running mean: -20.196452\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.204487\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.202442\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.210418\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.208314\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.196231\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.204268\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.202226\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.210203\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.208101\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.216020\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.223860\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.231622\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.229305\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.237012\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.244642\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.252196\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.259674\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.257077\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.244506\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.252061\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.259541\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.266945\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.274276\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.281533\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.278718\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.285931\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.293071\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.300141\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.307139\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.314068\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.320927\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.307718\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.314641\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.311494\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.318379\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.325195\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.331944\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.328624\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.315338\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.322184\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.328963\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.325673\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.332416\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.329092\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.315801\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.322643\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.319417\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.326223\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.322960\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.319731\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.316533\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.313368\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.310234\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.317132\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.323961\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.330721\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.317414\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.324240\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.330997\n",
            "resetting env. episode 350.000000, reward total was -18.000000. running mean: -20.307687\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.304611\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.301564\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.308549\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.305463\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.302409\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.299385\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.306391\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.313327\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.310194\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.317092\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.313921\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.310782\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.317674\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.324497\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.331252\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.337939\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.334560\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.331214\n",
            "resetting env. episode 369.000000, reward total was -18.000000. running mean: -20.307902\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.304823\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.301775\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.298757\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.295770\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.292812\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.289884\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.276985\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.274215\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.281473\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.288658\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.275772\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.283014\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.290184\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.297282\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.304309\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.311266\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.318153\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.324972\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.331722\n",
            "resetting env. episode 389.000000, reward total was -16.000000. running mean: -20.288405\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.295521\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.282566\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.279740\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.286943\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.294073\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.291133\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.298221\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.305239\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.312187\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.309065\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.315974\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.322814\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.329586\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.336290\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.342927\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.329498\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.336203\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.342841\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.349413\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.355919\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.362359\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.368736\n",
            "resetting env. episode 412.000000, reward total was -18.000000. running mean: -20.345048\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.341598\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.348182\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.334700\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.341353\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.327940\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.334660\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.331314\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.338001\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.334621\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.331274\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.337962\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.344582\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.351136\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.357625\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.364049\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.370408\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.366704\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.373037\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.379307\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.375513\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.381758\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.387941\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.394061\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.400121\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.396120\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.392158\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.398237\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.404254\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.410212\n",
            "resetting env. episode 442.000000, reward total was -18.000000. running mean: -20.386110\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.382249\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.388426\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.394542\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.380596\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.376791\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.383023\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.389192\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.395300\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.401347\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.387334\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.373461\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.369726\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.366029\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.372368\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.368645\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.364958\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.371309\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.357596\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.344020\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.330580\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.317274\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.324101\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.330860\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.327551\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.324276\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.331033\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.337723\n",
            "resetting env. episode 470.000000, reward total was -18.000000. running mean: -20.314346\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.311202\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.318090\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.324909\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.331660\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.338343\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.344960\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.341510\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.338095\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.334714\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.341367\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.337954\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.344574\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.351128\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.357617\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.364041\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.360400\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.346796\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.353328\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.339795\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.346397\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.352933\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.349404\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.355910\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.362351\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.368727\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.365040\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.361390\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.367776\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.374098\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.370357\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.366653\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.372987\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.369257\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.375564\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.381809\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.377991\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.384211\n",
            "resetting env. episode 508.000000, reward total was -19.000000. running mean: -20.370369\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.376665\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.372898\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.369169\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.365478\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.371823\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.378105\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -20.374324\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.370580\n",
            "resetting env. episode 517.000000, reward total was -18.000000. running mean: -20.346875\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.343406\n",
            "resetting env. episode 519.000000, reward total was -19.000000. running mean: -20.329972\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.326672\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.323405\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.330171\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.336870\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.343501\n",
            "resetting env. episode 525.000000, reward total was -18.000000. running mean: -20.320066\n",
            "resetting env. episode 526.000000, reward total was -18.000000. running mean: -20.296865\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.303897\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.300858\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.297849\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.304871\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.311822\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.318704\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.325517\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.332261\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.338939\n",
            "resetting env. episode 536.000000, reward total was -18.000000. running mean: -20.315549\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.322394\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.329170\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.335878\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.332519\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.329194\n",
            "resetting env. episode 542.000000, reward total was -19.000000. running mean: -20.315902\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.322743\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.329516\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.326221\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.332959\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.329629\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.326333\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.333069\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.339739\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.336341\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.342978\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.329548\n",
            "resetting env. episode 554.000000, reward total was -18.000000. running mean: -20.306253\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.303190\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.300158\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.307157\n",
            "resetting env. episode 558.000000, reward total was -19.000000. running mean: -20.294085\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.301144\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.308133\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.315051\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.311901\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.318782\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.325594\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.332338\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.339015\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.345625\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.352168\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.348647\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.355160\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.361609\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.367992\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.374313\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.380569\n",
            "resetting env. episode 575.000000, reward total was -19.000000. running mean: -20.366764\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.373096\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.379365\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.385571\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.391716\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.397799\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.393821\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.399882\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.395884\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.401925\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.407906\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.413826\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.419688\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.425491\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -20.411236\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.417124\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.422953\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.418723\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.424536\n",
            "resetting env. episode 594.000000, reward total was -19.000000. running mean: -20.410291\n",
            "resetting env. episode 595.000000, reward total was -18.000000. running mean: -20.386188\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.382326\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.378503\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.384718\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -20.380870\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.387062\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -20.373191\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -20.359459\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.365865\n",
            "resetting env. episode 604.000000, reward total was -15.000000. running mean: -20.312206\n",
            "resetting env. episode 605.000000, reward total was -18.000000. running mean: -20.289084\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.286193\n",
            "resetting env. episode 607.000000, reward total was -19.000000. running mean: -20.273331\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.270598\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.267892\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.275213\n",
            "resetting env. episode 611.000000, reward total was -19.000000. running mean: -20.262461\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.269836\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.277138\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.284366\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.281523\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.268708\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.276020\n",
            "resetting env. episode 618.000000, reward total was -19.000000. running mean: -20.263260\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.270628\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.277921\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.275142\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.282391\n",
            "resetting env. episode 623.000000, reward total was -20.000000. running mean: -20.279567\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.286771\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.293903\n",
            "resetting env. episode 626.000000, reward total was -20.000000. running mean: -20.290964\n",
            "resetting env. episode 627.000000, reward total was -18.000000. running mean: -20.268055\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.275374\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -20.272620\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.279894\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.287095\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -20.284224\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.291382\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.288468\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.295584\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.302628\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.299602\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.306606\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.313539\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.320404\n",
            "resetting env. episode 641.000000, reward total was -19.000000. running mean: -20.307200\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.314128\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.320987\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.317777\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.324599\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.331353\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.338040\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -20.334659\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.331313\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.337999\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.334619\n",
            "resetting env. episode 652.000000, reward total was -18.000000. running mean: -20.311273\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.318161\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.324979\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.331729\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.328412\n",
            "resetting env. episode 657.000000, reward total was -18.000000. running mean: -20.305128\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.302076\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.309056\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.315965\n",
            "resetting env. episode 661.000000, reward total was -17.000000. running mean: -20.282805\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.279977\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.287178\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.284306\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.291463\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.288548\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.275663\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.282906\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.290077\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.297176\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -20.284205\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.281362\n",
            "resetting env. episode 673.000000, reward total was -19.000000. running mean: -20.268549\n",
            "resetting env. episode 674.000000, reward total was -18.000000. running mean: -20.245863\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.253405\n",
            "resetting env. episode 676.000000, reward total was -18.000000. running mean: -20.230871\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.228562\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.236276\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.233914\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.231574\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.229259\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.226966\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.234696\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.242349\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.239926\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.237527\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.245151\n",
            "resetting env. episode 688.000000, reward total was -19.000000. running mean: -20.232700\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.240373\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.237969\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.245590\n",
            "resetting env. episode 692.000000, reward total was -19.000000. running mean: -20.233134\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.240802\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.248394\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -20.235910\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.233551\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.241216\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.248804\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.256316\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.253752\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.261215\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.268603\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.275917\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.273157\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.270426\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -20.267722\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.265044\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.262394\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.269770\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.277072\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.284302\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.281459\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.288644\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.295758\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.302800\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.309772\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.316674\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.313508\n",
            "resetting env. episode 719.000000, reward total was -19.000000. running mean: -20.300372\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.307369\n",
            "resetting env. episode 721.000000, reward total was -19.000000. running mean: -20.294295\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.301352\n",
            "resetting env. episode 723.000000, reward total was -18.000000. running mean: -20.278339\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.285555\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.282700\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.289873\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -20.286974\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.294104\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.301163\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.308152\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.295070\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.302119\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.309098\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.306007\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.312947\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.309818\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -20.306719\n",
            "resetting env. episode 738.000000, reward total was -18.000000. running mean: -20.283652\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.290816\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.297908\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.304928\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.311879\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.308760\n",
            "resetting env. episode 744.000000, reward total was -17.000000. running mean: -20.275673\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.282916\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.290087\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.297186\n",
            "resetting env. episode 748.000000, reward total was -19.000000. running mean: -20.284214\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.291372\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.298458\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.305474\n",
            "resetting env. episode 752.000000, reward total was -18.000000. running mean: -20.282419\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.289595\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.296699\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.283732\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.290895\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.287986\n",
            "resetting env. episode 758.000000, reward total was -19.000000. running mean: -20.275106\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.272355\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -20.259631\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.267035\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.274364\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.281621\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.278805\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.286017\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.293156\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -20.280225\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.287423\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.294548\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.301603\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.298587\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.305601\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.312545\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.319420\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.316225\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.313063\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.319932\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.326733\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.333466\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.340131\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -20.326730\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.333463\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.330128\n",
            "resetting env. episode 784.000000, reward total was -19.000000. running mean: -20.316827\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.323658\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.330422\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.337118\n",
            "resetting env. episode 788.000000, reward total was -19.000000. running mean: -20.323746\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -20.310509\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -20.297404\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -20.284430\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.291585\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -20.278670\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.285883\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.293024\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.300094\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.307093\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.314022\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -20.310882\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -20.297773\n",
            "resetting env. episode 801.000000, reward total was -19.000000. running mean: -20.284795\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.291947\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.299028\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.306038\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.312977\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -20.309847\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.306749\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.313681\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.310545\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.307439\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.304365\n",
            "resetting env. episode 812.000000, reward total was -19.000000. running mean: -20.291321\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.298408\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.305424\n",
            "resetting env. episode 815.000000, reward total was -19.000000. running mean: -20.292370\n",
            "resetting env. episode 816.000000, reward total was -19.000000. running mean: -20.279446\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.286651\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.293785\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.300847\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.307839\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -20.294760\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.301813\n",
            "resetting env. episode 823.000000, reward total was -19.000000. running mean: -20.288794\n",
            "resetting env. episode 824.000000, reward total was -18.000000. running mean: -20.265907\n",
            "resetting env. episode 825.000000, reward total was -19.000000. running mean: -20.253247\n",
            "resetting env. episode 826.000000, reward total was -18.000000. running mean: -20.230715\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.228408\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.236124\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.243763\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.241325\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.248912\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.256423\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.253858\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.251320\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.258807\n",
            "resetting env. episode 836.000000, reward total was -19.000000. running mean: -20.246218\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.233756\n",
            "resetting env. episode 838.000000, reward total was -20.000000. running mean: -20.231419\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.239105\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.236713\n",
            "resetting env. episode 841.000000, reward total was -19.000000. running mean: -20.224346\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.232103\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.229782\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.237484\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.235109\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.232758\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.230431\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.238126\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.245745\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.243288\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.240855\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.238446\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.246062\n",
            "resetting env. episode 854.000000, reward total was -17.000000. running mean: -20.213601\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.221465\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.229250\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.236958\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.244588\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.252142\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.259621\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.267025\n",
            "resetting env. episode 862.000000, reward total was -17.000000. running mean: -20.234355\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.232011\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -20.219691\n",
            "resetting env. episode 865.000000, reward total was -18.000000. running mean: -20.197494\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.205519\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -20.203464\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.201429\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.199415\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.207421\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.215347\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.223193\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.230961\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.238652\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.236265\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.243902\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.251463\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.248949\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.246459\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.243995\n",
            "resetting env. episode 881.000000, reward total was -19.000000. running mean: -20.231555\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.229239\n",
            "resetting env. episode 883.000000, reward total was -18.000000. running mean: -20.206947\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.204877\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -20.192828\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.200900\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.198891\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.206902\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.214833\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.222685\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.220458\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.228254\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.225971\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.233711\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.231374\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.239060\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.246670\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.254203\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.261661\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.269044\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.266354\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.273690\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.280954\n",
            "resetting env. episode 904.000000, reward total was -19.000000. running mean: -20.268144\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.275463\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -20.262708\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.270081\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -20.267380\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.264706\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.262059\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.269439\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.276744\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.283977\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.291137\n",
            "resetting env. episode 915.000000, reward total was -17.000000. running mean: -20.258226\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.255643\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -20.253087\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.250556\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -20.238051\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.235670\n",
            "resetting env. episode 921.000000, reward total was -17.000000. running mean: -20.203313\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.211280\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.209167\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.217076\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.224905\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.232656\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.230329\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.238026\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.245646\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -20.243189\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.250757\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.258250\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.265667\n",
            "resetting env. episode 934.000000, reward total was -20.000000. running mean: -20.263011\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.270381\n",
            "resetting env. episode 936.000000, reward total was -19.000000. running mean: -20.257677\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.255100\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.252549\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.260024\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -20.247423\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.244949\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.242500\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.250075\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.247574\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -20.245098\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.252647\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.260121\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.267519\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.274844\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.282096\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.279275\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.286482\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.293617\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.300681\n",
            "resetting env. episode 955.000000, reward total was -19.000000. running mean: -20.287674\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.294798\n",
            "resetting env. episode 957.000000, reward total was -19.000000. running mean: -20.281850\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.289031\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.286141\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.293279\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.300347\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.297343\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.294370\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -20.281426\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.288612\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.285726\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.292868\n",
            "resetting env. episode 968.000000, reward total was -19.000000. running mean: -20.279940\n",
            "resetting env. episode 969.000000, reward total was -19.000000. running mean: -20.267140\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.274469\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.281724\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.288907\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.296018\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.293058\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.300127\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.297126\n",
            "resetting env. episode 977.000000, reward total was -19.000000. running mean: -20.284155\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.291313\n",
            "resetting env. episode 979.000000, reward total was -19.000000. running mean: -20.278400\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.285616\n",
            "resetting env. episode 981.000000, reward total was -19.000000. running mean: -20.272760\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.280032\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.287232\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -20.284360\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.291516\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -20.278601\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.285815\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.282957\n",
            "resetting env. episode 989.000000, reward total was -18.000000. running mean: -20.260127\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.267526\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.274850\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.282102\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.279281\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -20.276488\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.283723\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.280886\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.288077\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.285196\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.292344\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.299421\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.306427\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.303363\n",
            "resetting env. episode 1003.000000, reward total was -18.000000. running mean: -20.280329\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.267526\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.274850\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.282102\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.279281\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.276488\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.283723\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.290886\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.297977\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.294997\n",
            "resetting env. episode 1013.000000, reward total was -20.000000. running mean: -20.292047\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.299127\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.306136\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.303074\n",
            "resetting env. episode 1017.000000, reward total was -19.000000. running mean: -20.290043\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.297143\n",
            "resetting env. episode 1019.000000, reward total was -18.000000. running mean: -20.274172\n",
            "resetting env. episode 1020.000000, reward total was -19.000000. running mean: -20.261430\n",
            "resetting env. episode 1021.000000, reward total was -19.000000. running mean: -20.248816\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.256327\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.253764\n",
            "resetting env. episode 1024.000000, reward total was -20.000000. running mean: -20.251227\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.248714\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.256227\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.263665\n",
            "resetting env. episode 1028.000000, reward total was -20.000000. running mean: -20.261028\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.268418\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.265734\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.273076\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.270346\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.267642\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.274966\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.282216\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -20.279394\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.286600\n",
            "resetting env. episode 1038.000000, reward total was -18.000000. running mean: -20.263734\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.271097\n",
            "resetting env. episode 1040.000000, reward total was -19.000000. running mean: -20.258386\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -20.255802\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.263244\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.270611\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.277905\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.275126\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.282375\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.289551\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.296656\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.303689\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.310652\n",
            "resetting env. episode 1051.000000, reward total was -19.000000. running mean: -20.297546\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.294570\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.301625\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -20.288608\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.295722\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.302765\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -20.289737\n",
            "resetting env. episode 1058.000000, reward total was -19.000000. running mean: -20.276840\n",
            "resetting env. episode 1059.000000, reward total was -19.000000. running mean: -20.264072\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.271431\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.278717\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.275929\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.273170\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.260438\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.267834\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.265156\n",
            "resetting env. episode 1067.000000, reward total was -18.000000. running mean: -20.242504\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.240079\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.247678\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.255202\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.262649\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.260023\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.257423\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.264849\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.272200\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.279478\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.286683\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.283816\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.290978\n",
            "resetting env. episode 1080.000000, reward total was -18.000000. running mean: -20.268068\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -20.265388\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -20.262734\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -20.260107\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.267506\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.274830\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.282082\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.289261\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -20.286369\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.283505\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -20.270670\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.277963\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -20.285184\n",
            "resetting env. episode 1093.000000, reward total was -19.000000. running mean: -20.272332\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -20.259609\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.257012\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.264442\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.271798\n",
            "resetting env. episode 1098.000000, reward total was -19.000000. running mean: -20.259080\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -20.256489\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.253924\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.251385\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.258871\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -20.246282\n",
            "resetting env. episode 1104.000000, reward total was -18.000000. running mean: -20.223820\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -20.221581\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.229366\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.237072\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.244701\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.242254\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.249832\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.247333\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.254860\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.252311\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.249788\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.257290\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -20.254717\n",
            "resetting env. episode 1117.000000, reward total was -19.000000. running mean: -20.242170\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.239749\n",
            "resetting env. episode 1119.000000, reward total was -19.000000. running mean: -20.227351\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.225078\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -20.222827\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.230599\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.238293\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.245910\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.243451\n",
            "resetting env. episode 1126.000000, reward total was -19.000000. running mean: -20.231016\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -20.218706\n",
            "resetting env. episode 1128.000000, reward total was -18.000000. running mean: -20.196519\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -20.194554\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.202608\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.210582\n",
            "resetting env. episode 1132.000000, reward total was -19.000000. running mean: -20.198476\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -20.196491\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.194527\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.192581\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -20.180655\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.188849\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.186960\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.185091\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.193240\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.191307\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.199394\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.207400\n",
            "resetting env. episode 1144.000000, reward total was -19.000000. running mean: -20.195326\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.203373\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.211339\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -20.209226\n",
            "resetting env. episode 1148.000000, reward total was -17.000000. running mean: -20.177134\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.185362\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.193509\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -20.191574\n",
            "resetting env. episode 1152.000000, reward total was -19.000000. running mean: -20.179658\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.177861\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.186083\n",
            "resetting env. episode 1155.000000, reward total was -18.000000. running mean: -20.164222\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -20.162580\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.170954\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.179244\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.177452\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.185677\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -20.173821\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -20.162083\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.160462\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.158857\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.167268\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.165596\n",
            "resetting env. episode 1167.000000, reward total was -20.000000. running mean: -20.163940\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.162300\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.160677\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.169071\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -20.167380\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.165706\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.174049\n",
            "resetting env. episode 1174.000000, reward total was -19.000000. running mean: -20.162309\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -20.160686\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.169079\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -20.157388\n",
            "resetting env. episode 1178.000000, reward total was -19.000000. running mean: -20.145814\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.144356\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -20.142912\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -20.141483\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -20.130068\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.138768\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -20.127380\n",
            "resetting env. episode 1185.000000, reward total was -19.000000. running mean: -20.116106\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -20.114945\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -20.113796\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.122658\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.131431\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.130117\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.128816\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.137528\n",
            "resetting env. episode 1193.000000, reward total was -18.000000. running mean: -20.116152\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.114991\n",
            "resetting env. episode 1195.000000, reward total was -20.000000. running mean: -20.113841\n",
            "resetting env. episode 1196.000000, reward total was -18.000000. running mean: -20.092702\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.101775\n",
            "resetting env. episode 1198.000000, reward total was -18.000000. running mean: -20.080758\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.089950\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.099051\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.088060\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.097179\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -20.086208\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -20.075346\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -20.074592\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.083846\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.073008\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.082278\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -20.071455\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -20.070740\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.080033\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -20.079233\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.088440\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.087556\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.096680\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.095714\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.104756\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -20.103709\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -20.092672\n",
            "resetting env. episode 1220.000000, reward total was -18.000000. running mean: -20.071745\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.081028\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -20.080217\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.089415\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -20.088521\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.087636\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.096759\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.105792\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -20.094734\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.103787\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.102749\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.101721\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.100704\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.109697\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.118600\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -20.107414\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -20.106340\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.115276\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.124124\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -20.112882\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.111754\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.120636\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.129430\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.138135\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.136754\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.145387\n",
            "resetting env. episode 1246.000000, reward total was -19.000000. running mean: -20.133933\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.132593\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.131267\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.139955\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.138555\n",
            "resetting env. episode 1251.000000, reward total was -19.000000. running mean: -20.127170\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.125898\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.124639\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.123393\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.132159\n",
            "resetting env. episode 1256.000000, reward total was -19.000000. running mean: -20.120837\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.119629\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.128432\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.137148\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.135777\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.144419\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -20.132975\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.141645\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.150228\n",
            "resetting env. episode 1265.000000, reward total was -18.000000. running mean: -20.128726\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.137439\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.146064\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.154604\n",
            "resetting env. episode 1269.000000, reward total was -19.000000. running mean: -20.143058\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.141627\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.140211\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.138809\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.147421\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.155947\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.164387\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.162743\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.161116\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.169505\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.167810\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.176131\n",
            "resetting env. episode 1281.000000, reward total was -20.000000. running mean: -20.174370\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -20.172626\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.180900\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.179091\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.187300\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.195427\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -20.193473\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.191538\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.189623\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.187727\n",
            "resetting env. episode 1291.000000, reward total was -19.000000. running mean: -20.175849\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.184091\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.192250\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.200327\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.208324\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -20.206241\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.214179\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.222037\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -20.219816\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.227618\n",
            "resetting env. episode 1301.000000, reward total was -18.000000. running mean: -20.205342\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.213289\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -20.201156\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.209144\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.217053\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.224882\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.232633\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.240307\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.247904\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.245425\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -20.242971\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -20.230541\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -20.218236\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.226053\n",
            "resetting env. episode 1315.000000, reward total was -19.000000. running mean: -20.213793\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.221655\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.229438\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -20.227144\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.224872\n",
            "resetting env. episode 1320.000000, reward total was -18.000000. running mean: -20.202624\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.210597\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.208491\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.216407\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.224243\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.232000\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.229680\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.237383\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.245009\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -20.242559\n",
            "resetting env. episode 1330.000000, reward total was -19.000000. running mean: -20.230134\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.237832\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -20.235454\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.243100\n",
            "resetting env. episode 1334.000000, reward total was -19.000000. running mean: -20.230669\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.238362\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.245978\n",
            "resetting env. episode 1337.000000, reward total was -18.000000. running mean: -20.223518\n",
            "resetting env. episode 1338.000000, reward total was -19.000000. running mean: -20.211283\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.219170\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -20.216979\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -20.214809\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.222661\n",
            "resetting env. episode 1343.000000, reward total was -19.000000. running mean: -20.210434\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.218330\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.226147\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.233885\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.241546\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.239131\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.236740\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -20.234372\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.232028\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.229708\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.237411\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.245037\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.252587\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -20.250061\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.247560\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -20.235084\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.242734\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -20.240306\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -20.227903\n",
            "resetting env. episode 1362.000000, reward total was -19.000000. running mean: -20.215624\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.223468\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.231233\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.238921\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.236532\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -20.224166\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.231925\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -20.229606\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.227309\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.235036\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.242686\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.250259\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.257757\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.265179\n",
            "resetting env. episode 1376.000000, reward total was -18.000000. running mean: -20.242527\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.250102\n",
            "resetting env. episode 1378.000000, reward total was -19.000000. running mean: -20.237601\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.245225\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -20.242773\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -20.250345\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.257841\n",
            "resetting env. episode 1383.000000, reward total was -18.000000. running mean: -20.235263\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.232910\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.230581\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.228276\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.235993\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.243633\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -20.241197\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.248785\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.256297\n",
            "resetting env. episode 1392.000000, reward total was -19.000000. running mean: -20.243734\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.241296\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.238883\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.236495\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.244130\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.251688\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.249171\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -20.246680\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.244213\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.251771\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.259253\n",
            "resetting env. episode 1403.000000, reward total was -19.000000. running mean: -20.246661\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.254194\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -20.251652\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.259136\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.256544\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.253979\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.261439\n",
            "resetting env. episode 1410.000000, reward total was -19.000000. running mean: -20.248825\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.256336\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -20.253773\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -20.251235\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.248723\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -20.246236\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.253773\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.261236\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.268623\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.275937\n",
            "resetting env. episode 1420.000000, reward total was -18.000000. running mean: -20.253178\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.260646\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.258039\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.265459\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -20.262804\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.270176\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -20.257475\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.254900\n",
            "resetting env. episode 1428.000000, reward total was -19.000000. running mean: -20.242351\n",
            "resetting env. episode 1429.000000, reward total was -18.000000. running mean: -20.219927\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.227728\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.235451\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -20.233096\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.240765\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.248358\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.255874\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.263315\n",
            "resetting env. episode 1437.000000, reward total was -19.000000. running mean: -20.250682\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.258175\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.255594\n",
            "resetting env. episode 1440.000000, reward total was -19.000000. running mean: -20.243038\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.240607\n",
            "resetting env. episode 1442.000000, reward total was -19.000000. running mean: -20.228201\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.235919\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -20.223560\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.231324\n",
            "resetting env. episode 1446.000000, reward total was -18.000000. running mean: -20.209011\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.216921\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.224752\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.222504\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.220279\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.228076\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.235796\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.243438\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -20.241003\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.248593\n",
            "resetting env. episode 1456.000000, reward total was -18.000000. running mean: -20.226107\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.233846\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.241508\n",
            "resetting env. episode 1459.000000, reward total was -19.000000. running mean: -20.229093\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -20.226802\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -20.224534\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -20.212289\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.220166\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.227964\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.235684\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.243327\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -20.240894\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.248485\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -20.236000\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.243640\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -20.241204\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.248792\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.246304\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.253841\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.261303\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.258690\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.266103\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.273442\n",
            "resetting env. episode 1479.000000, reward total was -20.000000. running mean: -20.270707\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -20.268000\n",
            "resetting env. episode 1481.000000, reward total was -19.000000. running mean: -20.255320\n",
            "resetting env. episode 1482.000000, reward total was -18.000000. running mean: -20.232767\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -20.220439\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -20.218235\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -20.216053\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.223892\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.231653\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -20.229337\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.227043\n",
            "resetting env. episode 1490.000000, reward total was -19.000000. running mean: -20.214773\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.212625\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -20.200499\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -20.198494\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.206509\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.204444\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.212399\n",
            "resetting env. episode 1497.000000, reward total was -17.000000. running mean: -20.180275\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.188473\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.196588\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.204622\n",
            "CPU times: user 1h 12min 49s, sys: 33min 35s, total: 1h 46min 24s\n",
            "Wall time: 55min 9s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "f3dc32bf-ab84-418d-b830-407510690d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}