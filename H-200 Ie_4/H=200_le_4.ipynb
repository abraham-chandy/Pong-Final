{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1eICmP8sgLa",
        "outputId": "36c1d643-3ff9-4e3b-a443-1450c03e10db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "2893a411-8109-4f73-dca8-3c5544540d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=38855d1879cd0d054d7a39348b063dad631bf1be47e6d50a2f35e884a054bb7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "45c301a4-ee5d-4187-9f33-664f7ff41c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "e5b4f556-62e9-42ec-c142-547f609dd1b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "4f4f2ea8-b174-42ef-cde2-535b6bcaea54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "2f4c4d07-8dca-4dbd-80f1-34a3c657f0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "90772b9a-f3fa-4a1c-bdd0-f05d4b9bf9bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.029800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049502\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -19.059007\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.078417\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.097633\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.106656\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.125590\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.134334\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.152991\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -19.161461\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.179846\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.188048\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.206167\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.224105\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.241864\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.259446\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.276851\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.284083\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.301242\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -19.308230\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.325147\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.341896\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.358477\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.374892\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.381143\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.397332\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.413358\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.419225\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.425033\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.440782\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -19.446374\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.461911\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.477292\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -19.482519\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.497693\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.512717\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.517589\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.532413\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.537089\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.541718\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.546301\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.560838\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.575230\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.589478\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.593583\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -19.587647\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.601770\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -19.595753\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.599795\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.613797\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.627659\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.641383\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.654969\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.668419\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.671735\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.685018\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.698168\n",
            "resetting env. episode 60.000000, reward total was -18.000000. running mean: -19.681186\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.694374\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -19.687430\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.700556\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -19.703550\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.716515\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.729350\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.742056\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.754636\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.757089\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.769518\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.781823\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -19.774005\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -19.766265\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.778602\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.790816\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.802908\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.814879\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.826730\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.838463\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.840078\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -19.831678\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.843361\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.854927\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -19.846378\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.857914\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.869335\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.870642\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.881935\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.893116\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -19.884185\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.885343\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.886489\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -19.877625\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.888848\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.889960\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.901060\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.912050\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.922929\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -19.913700\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.924563\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.935317\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.935964\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.946604\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -19.937138\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.947767\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.958289\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.958706\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.959119\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.969528\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.979833\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.980035\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.980234\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.980432\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.990627\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.000721\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -19.990714\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.000807\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.000799\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.010791\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.010683\n",
            "resetting env. episode 121.000000, reward total was -18.000000. running mean: -19.990576\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.000670\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.000664\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.010657\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.020550\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.020345\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.020141\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.019940\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.019741\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.029543\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.029248\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.038955\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.048566\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.058080\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.067499\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.066824\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.066156\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.075495\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.074740\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.083992\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.093152\n",
            "resetting env. episode 142.000000, reward total was -16.000000. running mean: -20.052221\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.061699\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.071082\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.070371\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.079667\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.078870\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.088082\n",
            "resetting env. episode 149.000000, reward total was -18.000000. running mean: -20.067201\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.066529\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.075864\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.075105\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.084354\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.093510\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.102575\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.101549\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.110534\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.109429\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.098334\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.097351\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.096377\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.105414\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.114360\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.113216\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.122084\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.130863\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.119554\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.128359\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.137075\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.145704\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.154247\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.162705\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.171078\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.179367\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.177573\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.185798\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.183940\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.182100\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.180279\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.188477\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.196592\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.204626\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.212580\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.220454\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.228249\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.235967\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.243607\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.251171\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.248659\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.246173\n",
            "resetting env. episode 191.000000, reward total was -18.000000. running mean: -20.223711\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.231474\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.239159\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.246768\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.244300\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.251857\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.249338\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.256845\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.254276\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.241734\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.249316\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.236823\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.234455\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.242110\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.249689\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.257192\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.264621\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.271974\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.269255\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.266562\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.273896\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.281157\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.278346\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.275562\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.272807\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.280079\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.287278\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.284405\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.281561\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.278745\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.285958\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.293098\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.290167\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.287266\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.284393\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.291549\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.288634\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.285747\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.292890\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.299961\n",
            "resetting env. episode 231.000000, reward total was -18.000000. running mean: -20.276961\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.284192\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.291350\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.288436\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.295552\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.292596\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.279671\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.286874\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.294005\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.301065\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.288054\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.295174\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.302222\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.289200\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.286308\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.283445\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.290610\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.297704\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.304727\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.311680\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.318563\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.325377\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.332124\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.328802\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.335514\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.342159\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.348738\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.355250\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.361698\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.368081\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.374400\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.380656\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.386849\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.392981\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.389051\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.385161\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.391309\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.387396\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.373522\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.379787\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.385989\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.392129\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.388208\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.394326\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.400382\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.396379\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.382415\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.388591\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.374705\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.380958\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.387148\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.393277\n",
            "resetting env. episode 283.000000, reward total was -19.000000. running mean: -20.379344\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.385550\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.381695\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.387878\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.383999\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.380159\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.386358\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.382494\n",
            "resetting env. episode 291.000000, reward total was -18.000000. running mean: -20.358669\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.345082\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.341632\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.348215\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.354733\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.361186\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.347574\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.354098\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.350557\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.357052\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.353481\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.359946\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.366347\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.372683\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.378957\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.385167\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.371315\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.377602\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.383826\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.389988\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.386088\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.392227\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.398305\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.404322\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.410279\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.416176\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.422014\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.427794\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.433516\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.439181\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.444789\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.440341\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.445938\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.441478\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.447064\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.432593\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.438267\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.433884\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.439545\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.445150\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.450698\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.456192\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.461630\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.447013\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.442543\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.448118\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.443637\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.449200\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.454708\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.460161\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.465559\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.470904\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.466195\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.471533\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.476818\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.472049\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.477329\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.482556\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.487730\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.492853\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.487924\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.493045\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.498115\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.503133\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.488102\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.493221\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.498289\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.493306\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.498373\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.493389\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.498455\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.503471\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.488436\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.483552\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.468716\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.474029\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.479289\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.484496\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.489651\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.484754\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.479907\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.485108\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.490257\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.495354\n",
            "resetting env. episode 375.000000, reward total was -18.000000. running mean: -20.470401\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.475697\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.480940\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.486130\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.481269\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.486456\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.491592\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.496676\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.501709\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.506692\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.501625\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.506609\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.511543\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.516427\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.511263\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.516150\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.520989\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.525779\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.520521\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.525316\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.530063\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.534762\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.539414\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.544020\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.548580\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.543094\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.547663\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.552187\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.556665\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.551098\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.555587\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.560031\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.564431\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.548787\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.543299\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.547866\n",
            "resetting env. episode 411.000000, reward total was -17.000000. running mean: -20.512387\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.517263\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.512091\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.516970\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.511800\n",
            "resetting env. episode 416.000000, reward total was -18.000000. running mean: -20.486682\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.491815\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.486897\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.492028\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.487108\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.492237\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.497314\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.502341\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.507318\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.512245\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.517122\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.521951\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.526732\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.531464\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.526150\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.530888\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.535579\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.540223\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.544821\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.549373\n",
            "resetting env. episode 436.000000, reward total was -18.000000. running mean: -20.523879\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.528640\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.533354\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.538020\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.542640\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.547214\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.541742\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.546324\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.550861\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.555352\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.559799\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.564201\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.568559\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.572873\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.577145\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.581373\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.585559\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.589704\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.593807\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.597869\n",
            "resetting env. episode 456.000000, reward total was -18.000000. running mean: -20.571890\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.576171\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.570409\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.554705\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.559158\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.553567\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.558031\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.542451\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.547026\n",
            "resetting env. episode 465.000000, reward total was -16.000000. running mean: -20.501556\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.506540\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.511475\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.516360\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.521197\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.525985\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.530725\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.535418\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.540063\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.544663\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.529216\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.523924\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.518685\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.523498\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.528263\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.522980\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.517750\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.522573\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.527347\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.522074\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.526853\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.511585\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.516469\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.521304\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.506091\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.491030\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.486120\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.481259\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.486446\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.491581\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.476666\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.481899\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.487080\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.492209\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.497287\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.502314\n",
            "CPU times: user 22min 44s, sys: 10min 21s, total: 33min 5s\n",
            "Wall time: 17min 14s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "25d9f7c4-51b8-4a38-81fa-20fce0bc27f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.970200\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -20.940498\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.941093\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.941682\n",
            "resetting env. episode 8.000000, reward total was -19.000000. running mean: -20.922265\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.913043\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.913912\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.914773\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.915625\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.906469\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.897404\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.898430\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.889446\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.880552\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.871746\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.873029\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.864298\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.845655\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.847199\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.848727\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.840240\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.821837\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.823619\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.825383\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.817129\n",
            "resetting env. episode 29.000000, reward total was -18.000000. running mean: -20.788957\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.791068\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.783157\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.765326\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.767672\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.759996\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.762396\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.764772\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.767124\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.759453\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.751858\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.754340\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.756796\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.759228\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.761636\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.764020\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.756379\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.748816\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.751328\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.733814\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.726476\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.709211\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.702119\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -20.685098\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.688247\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.691365\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.694451\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.697506\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.700531\n",
            "resetting env. episode 58.000000, reward total was -17.000000. running mean: -20.663526\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.666891\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.670222\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.663520\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.656884\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.640316\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.623912\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.627673\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -20.611397\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.595283\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.599330\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.603337\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.597303\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.591330\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -20.575417\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.559663\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.554066\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.558525\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.552940\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.537411\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.542037\n",
            "resetting env. episode 79.000000, reward total was -18.000000. running mean: -20.516616\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.521450\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.506236\n",
            "resetting env. episode 82.000000, reward total was -18.000000. running mean: -20.481173\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.476361\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.481598\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.476782\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.482014\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.487194\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.492322\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.487399\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.472525\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.477800\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.473022\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.478291\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.473508\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.478773\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.473986\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.479246\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.484453\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.489609\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.494713\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.499766\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.504768\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.499720\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.504723\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.509676\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.514579\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.519433\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.524239\n",
            "resetting env. episode 109.000000, reward total was -18.000000. running mean: -20.498997\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.504007\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.508966\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.493877\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.488938\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.494049\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.499108\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.504117\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.509076\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.493985\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.499045\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.484055\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.489214\n",
            "resetting env. episode 122.000000, reward total was -18.000000. running mean: -20.464322\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.469679\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.464982\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.470332\n",
            "resetting env. episode 126.000000, reward total was -18.000000. running mean: -20.445629\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.451173\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.456661\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.452094\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.457573\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.452998\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.438468\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.444083\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.439642\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.445246\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.440793\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.436385\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.442022\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.447601\n",
            "resetting env. episode 140.000000, reward total was -18.000000. running mean: -20.423125\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.428894\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.424605\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.430359\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.426055\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.421795\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.427577\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.433301\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.428968\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.424679\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.430432\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.426127\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.431866\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.417547\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.423372\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.429138\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.434847\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.430498\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.436193\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.441832\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.447413\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.442939\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.448510\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.454025\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.459484\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.464889\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.450241\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.455738\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.461181\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.456569\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.462003\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.457383\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.462809\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.448181\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.433700\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.439363\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.424969\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.420719\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.426512\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.432247\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.437924\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.443545\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.449110\n",
            "resetting env. episode 183.000000, reward total was -18.000000. running mean: -20.424619\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.420372\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.426169\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.421907\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.417688\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.413511\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.399376\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.405382\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.401328\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.407315\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.413242\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.419110\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.414918\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.420769\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.416562\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.422396\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.428172\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.433890\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.439551\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.435156\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.440804\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.426396\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.432132\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.437811\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.423433\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.429199\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.414907\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.420757\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.426550\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.422284\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.428062\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.423781\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.429543\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.435248\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.440895\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.436486\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.432121\n",
            "resetting env. episode 220.000000, reward total was -18.000000. running mean: -20.407800\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.413722\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.399585\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.395589\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.401633\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.397617\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.393641\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.399704\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.405707\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.411650\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.417534\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.413358\n",
            "resetting env. episode 232.000000, reward total was -18.000000. running mean: -20.389225\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.395333\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.401379\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.397365\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.403392\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.409358\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -20.395264\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.391312\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.397399\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.403425\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.399390\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.395396\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.391442\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.397528\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.383553\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.379717\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.375920\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.382161\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.378339\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.374556\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.370810\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.367102\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.373431\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.379697\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.385900\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.382041\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.378220\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.374438\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.380694\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.386887\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.393018\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.379088\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.375297\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.381544\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.387729\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.383851\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.390013\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.396113\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.392152\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.398230\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.404248\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.410205\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.396103\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.402142\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.388121\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.394240\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.390297\n",
            "resetting env. episode 279.000000, reward total was -18.000000. running mean: -20.366394\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.362730\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.369103\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.375412\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.371658\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.367941\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.374262\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.360519\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.366914\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.363245\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.359612\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.366016\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.362356\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.368733\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.365045\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.361395\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.367781\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.374103\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.380362\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.376558\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.362793\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.369165\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.375473\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.381718\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.387901\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.374022\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.380282\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.386479\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.372614\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.368888\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.375199\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.371447\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.367733\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.374056\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.380315\n",
            "resetting env. episode 314.000000, reward total was -18.000000. running mean: -20.356512\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.362947\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.369317\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.375624\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.371868\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.368149\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.364468\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.350823\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.357315\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.363742\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.370104\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.376403\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.382639\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.388813\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.384925\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.371075\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.367365\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.363691\n",
            "resetting env. episode 332.000000, reward total was -18.000000. running mean: -20.340054\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.346654\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.353187\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.349655\n",
            "resetting env. episode 336.000000, reward total was -18.000000. running mean: -20.326159\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.312897\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.319768\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.306570\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.313505\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.320370\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.327166\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.333894\n",
            "resetting env. episode 344.000000, reward total was -18.000000. running mean: -20.310555\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.317450\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.324275\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.331033\n",
            "resetting env. episode 348.000000, reward total was -18.000000. running mean: -20.307722\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.314645\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.321499\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.328284\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.335001\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.341651\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.348234\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.354752\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.361204\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.367592\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.373916\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.380177\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.366375\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.372712\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.378985\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.365195\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.371543\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.377827\n",
            "resetting env. episode 366.000000, reward total was -18.000000. running mean: -20.354049\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.360509\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.366903\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.373234\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.379502\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.375707\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.371950\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.368231\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.374548\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.380803\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.386995\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.393125\n",
            "resetting env. episode 378.000000, reward total was -18.000000. running mean: -20.369193\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.365502\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.361847\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.358228\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.364646\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.350999\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.357489\n",
            "resetting env. episode 385.000000, reward total was -18.000000. running mean: -20.333914\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.320575\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.327370\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.334096\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.330755\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.337447\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.324073\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.320832\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.317624\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.304448\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.311403\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.308289\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.315206\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.322054\n",
            "resetting env. episode 399.000000, reward total was -18.000000. running mean: -20.298834\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.295845\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.302887\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.299858\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.306859\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.313791\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.320653\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.327446\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.334172\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.340830\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.347422\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.353948\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.350408\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.356904\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.353335\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.339802\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.346404\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.342940\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.349510\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.346015\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.332555\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.339229\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.335837\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.322479\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.329254\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.315961\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.322802\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.329574\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.326278\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.313015\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.319885\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.326686\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.323419\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.310185\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.317083\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.313913\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.320773\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.327566\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.324290\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.321047\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.327837\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.324558\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.321313\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.328100\n",
            "resetting env. episode 443.000000, reward total was -18.000000. running mean: -20.304819\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.311770\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.318653\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.305466\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.312411\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.319287\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.326095\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.312834\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.319705\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.326508\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.333243\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.319911\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.316712\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.323544\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.310309\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.297206\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.304234\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.311192\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.318080\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.324899\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.311650\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.308533\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.305448\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -20.292393\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.289470\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.296575\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.293609\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.280673\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.277866\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.265088\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.272437\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.259712\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.257115\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.264544\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.271899\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.279180\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.276388\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.273624\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.280888\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.288079\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.295198\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.302246\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.279224\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.276431\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.283667\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.280830\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.278022\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.285242\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.292389\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.299466\n",
            "resetting env. episode 493.000000, reward total was -17.000000. running mean: -20.266471\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.263806\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.271168\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.278456\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.275672\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.262915\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.250286\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.257783\n",
            "CPU times: user 23min 34s, sys: 10min 35s, total: 34min 10s\n",
            "Wall time: 17min 41s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "46666436-db10-474c-f757-b0ef4ae23673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "a1c9e85f-4152-4ef0-d94a-e8999de032be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -19.029800\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.049502\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.069007\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -19.068317\n",
            "resetting env. episode 8.000000, reward total was -18.000000. running mean: -19.057634\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -19.067057\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.076387\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -19.075623\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -19.084867\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.094018\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -19.093078\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -19.102147\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.111126\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.120014\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -19.118814\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.127626\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.146350\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.154886\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.173337\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.191604\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.199688\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -19.207691\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.225614\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.243358\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.260925\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.278315\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.285532\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.292677\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.309750\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.326653\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.333386\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.340052\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -19.346652\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.363185\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -19.369553\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -19.355858\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.372299\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.388576\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -19.384690\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.400844\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.416835\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.432667\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.438340\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.453957\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -19.459417\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -19.464823\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.480175\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.495373\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.510419\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.525315\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.530062\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -19.524761\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -19.519514\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.534319\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.548975\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -19.553486\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -19.547951\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.552471\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.566946\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.581277\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -19.575464\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -19.569710\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.584013\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.588172\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.602291\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -19.596268\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.610305\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.624202\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.637960\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.651580\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.665065\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.678414\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.691630\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.704714\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -19.697666\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.710690\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.713583\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -19.706447\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.709383\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -19.712289\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.725166\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.737914\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.750535\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.753030\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.765499\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.777844\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.780066\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.782265\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -19.774443\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.786698\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.798831\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.800843\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.802834\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.814806\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.806658\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.818591\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.830406\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.842102\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.853680\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.855144\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.866592\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.877926\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.889147\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.890256\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.891353\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.892440\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.893515\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.894580\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.895634\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.906678\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.907611\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -19.908535\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.909450\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.920355\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -19.911152\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.912040\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.912920\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.913790\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.914653\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.925506\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.936251\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.936888\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.937520\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.948144\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.958663\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -19.949076\n",
            "resetting env. episode 130.000000, reward total was -18.000000. running mean: -19.929586\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -19.930290\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.940987\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.941577\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -19.942161\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.952740\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -19.953212\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.963680\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -19.954043\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.964503\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.974858\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.985109\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -19.975258\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.985505\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -19.975650\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -19.975894\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.986135\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -19.996274\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.006311\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.006248\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -19.996185\n",
            "resetting env. episode 151.000000, reward total was -18.000000. running mean: -19.976223\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.986461\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -19.986597\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -19.996731\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.006763\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.006696\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.016629\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.016462\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.026298\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.026035\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.035775\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.035417\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.045063\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.054612\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.064066\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.053425\n",
            "resetting env. episode 167.000000, reward total was -18.000000. running mean: -20.032891\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.022562\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.022336\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.032113\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.041792\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.051374\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.060860\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.070252\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.079549\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.088754\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.087866\n",
            "resetting env. episode 178.000000, reward total was -18.000000. running mean: -20.066987\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.076318\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.075554\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.084799\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.093951\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.103011\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.111981\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.110861\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.099753\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.098755\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.087768\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.076890\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.076121\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.075360\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.084606\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.093760\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.092823\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.101894\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.100876\n",
            "resetting env. episode 197.000000, reward total was -18.000000. running mean: -20.079867\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.089068\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.088177\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.077296\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.086523\n",
            "resetting env. episode 202.000000, reward total was -18.000000. running mean: -20.065657\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.055001\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.064451\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.053806\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.063268\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.072636\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.071909\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.071190\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.060478\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.059873\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.059275\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.038682\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.048295\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.047812\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.047334\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.056861\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.056292\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.065729\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.065072\n",
            "resetting env. episode 221.000000, reward total was -17.000000. running mean: -20.034421\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.024077\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.033836\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.043498\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.053063\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.062532\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.071907\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.081188\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.070376\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.069672\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.078976\n",
            "resetting env. episode 232.000000, reward total was -18.000000. running mean: -20.058186\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.067604\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.066928\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.076259\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.085496\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.094641\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.103695\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.112658\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.111531\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.120416\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.129212\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.137920\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.136540\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.135175\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.133823\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.142485\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.151060\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.159549\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.157954\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.166374\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.164711\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.163064\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.161433\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.169819\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.168120\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.176439\n",
            "resetting env. episode 258.000000, reward total was -18.000000. running mean: -20.154675\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.143128\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.151697\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.160180\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.158578\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.166992\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.165322\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.173669\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.181932\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.190113\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.188212\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.196330\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.204367\n",
            "resetting env. episode 271.000000, reward total was -18.000000. running mean: -20.182323\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.190500\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.198595\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.196609\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.194643\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.202696\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.210669\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.208563\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.216477\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.224312\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.212069\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.219948\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.227749\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.235471\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.243117\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.240686\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.248279\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.245796\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.233338\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.241005\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.248594\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.256109\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.263547\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.270912\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.268203\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.255521\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.252966\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.250436\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.257932\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.265352\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.272699\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.279972\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.287172\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.284300\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.291457\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.288543\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.285657\n",
            "resetting env. episode 308.000000, reward total was -18.000000. running mean: -20.262801\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.260173\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.267571\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.254895\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.262346\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.269723\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.277026\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.284255\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.281413\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.278599\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.285813\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.292955\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.280025\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.287225\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.274353\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.271609\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.268893\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.256204\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.263642\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.271006\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.278296\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.285513\n",
            "resetting env. episode 330.000000, reward total was -18.000000. running mean: -20.262657\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.270031\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.257331\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.254757\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.262210\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.259588\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.256992\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.254422\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.261878\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.259259\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.266666\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.264000\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.261360\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.268746\n",
            "resetting env. episode 344.000000, reward total was -18.000000. running mean: -20.246058\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.253598\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.261062\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.258451\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.265867\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.263208\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.270576\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.267870\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.265192\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.272540\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.269814\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.277116\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.274345\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.281602\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.278785\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.275998\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.283238\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.270405\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.277701\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.284924\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.292075\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.289154\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.276263\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.273500\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.270765\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.278057\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.285277\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.282424\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.289600\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.286704\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.283837\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.290998\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.298088\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.305108\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.312056\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.308936\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.305847\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.302788\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.299760\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.306763\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.303695\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.310658\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.317551\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.314376\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.321232\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.328020\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.334740\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.331392\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.338078\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.344698\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.341251\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.347838\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.354360\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.360816\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.367208\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.373536\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.369800\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.356102\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.342541\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.349116\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.355625\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.362069\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.358448\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.364863\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.371215\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.367503\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.373828\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.370089\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.366388\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.352725\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.359197\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.355605\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.362049\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.358429\n",
            "resetting env. episode 418.000000, reward total was -18.000000. running mean: -20.334845\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.331496\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.338181\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.334799\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.331451\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.338137\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.344755\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.351308\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.337795\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.334417\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.341073\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.347662\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.354185\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.360644\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.367037\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.363367\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.349733\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.346236\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.352773\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.359246\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.365653\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.361997\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.368377\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.374693\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.360946\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.357337\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.363763\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.370126\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.366424\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.362760\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.369132\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.365441\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.361787\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.358169\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.364587\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.360941\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.367332\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.373659\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.379922\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.366123\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.362461\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.358837\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.365249\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.361596\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.347980\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.344500\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.351055\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.357545\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.353969\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.330430\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.337125\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.343754\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.340316\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.336913\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.333544\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.330209\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.326907\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.333638\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.340301\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.336898\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -20.323529\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.320294\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.327091\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.323820\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.330582\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.337276\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.333903\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.340564\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.327159\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.333887\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.340548\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.327143\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.323871\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.330633\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.337326\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.343953\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.330513\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.337208\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.343836\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.340398\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.336994\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.323624\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.320388\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.317184\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.324012\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.320772\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.317564\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.324388\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.331145\n",
            "resetting env. episode 507.000000, reward total was -19.000000. running mean: -20.317833\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.324655\n",
            "resetting env. episode 509.000000, reward total was -20.000000. running mean: -20.321408\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.328194\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.324912\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.321663\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.318447\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.325262\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.332009\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -20.318689\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.325502\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.332247\n",
            "resetting env. episode 519.000000, reward total was -19.000000. running mean: -20.318925\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.325736\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.332478\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.329154\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.325862\n",
            "resetting env. episode 524.000000, reward total was -18.000000. running mean: -20.302603\n",
            "resetting env. episode 525.000000, reward total was -19.000000. running mean: -20.289577\n",
            "resetting env. episode 526.000000, reward total was -19.000000. running mean: -20.276682\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.273915\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.281176\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.288364\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.285480\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.292625\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.299699\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.306702\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.313635\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.310499\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.317394\n",
            "resetting env. episode 537.000000, reward total was -20.000000. running mean: -20.314220\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -20.301078\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.298067\n",
            "resetting env. episode 540.000000, reward total was -18.000000. running mean: -20.275086\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.272335\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.279612\n",
            "resetting env. episode 543.000000, reward total was -18.000000. running mean: -20.256816\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.264248\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.271605\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.268889\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.276200\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.283438\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.290604\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.297698\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.304721\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.311674\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.318557\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.325371\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.322118\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.328897\n",
            "resetting env. episode 557.000000, reward total was -19.000000. running mean: -20.315608\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.312451\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.319327\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.326134\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.332872\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.339544\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.346148\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.352687\n",
            "resetting env. episode 565.000000, reward total was -19.000000. running mean: -20.339160\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.335768\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -20.332411\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.329086\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.335796\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.342438\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.349013\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.355523\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.361968\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.358348\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.364765\n",
            "resetting env. episode 576.000000, reward total was -18.000000. running mean: -20.341117\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.347706\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.354229\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.360687\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.367080\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -20.353409\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.359875\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.356276\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.362713\n",
            "resetting env. episode 585.000000, reward total was -19.000000. running mean: -20.349086\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.355595\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.362039\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.348419\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.344935\n",
            "resetting env. episode 590.000000, reward total was -18.000000. running mean: -20.321485\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -20.318271\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.325088\n",
            "resetting env. episode 593.000000, reward total was -19.000000. running mean: -20.311837\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.318719\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.325531\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.322276\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -20.309053\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.305963\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.312903\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -20.309774\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.306676\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.303610\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.310574\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.307468\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.314393\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.321249\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.328037\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.334756\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.331409\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.338095\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.334714\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.341367\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.337953\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.344573\n",
            "resetting env. episode 615.000000, reward total was -19.000000. running mean: -20.331128\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.317816\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.314638\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.321492\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.318277\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.325094\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.321843\n",
            "resetting env. episode 622.000000, reward total was -18.000000. running mean: -20.298625\n",
            "resetting env. episode 623.000000, reward total was -20.000000. running mean: -20.295639\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.302682\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.299655\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.306659\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.313592\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.320456\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.327252\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -20.313979\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.310839\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -20.307731\n",
            "resetting env. episode 633.000000, reward total was -19.000000. running mean: -20.294654\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.291707\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.298790\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.295802\n",
            "resetting env. episode 637.000000, reward total was -19.000000. running mean: -20.282844\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.280016\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.287216\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.294343\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.301400\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.298386\n",
            "resetting env. episode 643.000000, reward total was -19.000000. running mean: -20.285402\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.292548\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.299623\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.296626\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.283660\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -20.280824\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.288015\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.285135\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.292284\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.289361\n",
            "resetting env. episode 653.000000, reward total was -18.000000. running mean: -20.266467\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.263803\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.261165\n",
            "resetting env. episode 656.000000, reward total was -18.000000. running mean: -20.238553\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.246167\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -20.233706\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.231369\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.239055\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.236664\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.234298\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.231955\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.239635\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.247239\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.244767\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.252319\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.259796\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.267198\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.274526\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.281781\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.288963\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.296073\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.303112\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.300081\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.297080\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.304110\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.311069\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.307958\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.304878\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.311829\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.308711\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.305624\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.302568\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -20.289542\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.296647\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.293680\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.290743\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.297836\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.304858\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.301809\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.308791\n",
            "resetting env. episode 693.000000, reward total was -19.000000. running mean: -20.295703\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.302746\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.309719\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.306621\n",
            "resetting env. episode 697.000000, reward total was -17.000000. running mean: -20.273555\n",
            "resetting env. episode 698.000000, reward total was -19.000000. running mean: -20.260820\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.268211\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.265529\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.272874\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.280145\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.287344\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.284470\n",
            "resetting env. episode 705.000000, reward total was -19.000000. running mean: -20.271626\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.278909\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.286120\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.283259\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.290427\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.287522\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -20.274647\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.281901\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.279082\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.276291\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.283528\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -20.280693\n",
            "resetting env. episode 717.000000, reward total was -19.000000. running mean: -20.267886\n",
            "resetting env. episode 718.000000, reward total was -19.000000. running mean: -20.255207\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.262655\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.270028\n",
            "resetting env. episode 721.000000, reward total was -19.000000. running mean: -20.257328\n",
            "resetting env. episode 722.000000, reward total was -19.000000. running mean: -20.244755\n",
            "resetting env. episode 723.000000, reward total was -20.000000. running mean: -20.242307\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.239884\n",
            "resetting env. episode 725.000000, reward total was -18.000000. running mean: -20.217485\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.215310\n",
            "resetting env. episode 727.000000, reward total was -18.000000. running mean: -20.193157\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.191226\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.199313\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.197320\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.185347\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.183494\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.191659\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.199742\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.207745\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.205667\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.213611\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.221474\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.219260\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.217067\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.224896\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.232647\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -20.220321\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.228118\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.235837\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.233478\n",
            "resetting env. episode 747.000000, reward total was -18.000000. running mean: -20.211143\n",
            "resetting env. episode 748.000000, reward total was -19.000000. running mean: -20.199032\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.207042\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.214971\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -20.202822\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.210793\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.218685\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.216499\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.204334\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.212290\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.220167\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.227966\n",
            "resetting env. episode 759.000000, reward total was -19.000000. running mean: -20.215686\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.213529\n",
            "resetting env. episode 761.000000, reward total was -19.000000. running mean: -20.201394\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.199380\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.187386\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.195512\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.203557\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -20.201522\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.209506\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.217411\n",
            "resetting env. episode 769.000000, reward total was -20.000000. running mean: -20.215237\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.213085\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.210954\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.208844\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.216756\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.224588\n",
            "resetting env. episode 775.000000, reward total was -19.000000. running mean: -20.212343\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.210219\n",
            "resetting env. episode 777.000000, reward total was -19.000000. running mean: -20.198117\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.206136\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -20.194074\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.202134\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.200112\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.208111\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.216030\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.223870\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.231631\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.239315\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.246922\n",
            "resetting env. episode 788.000000, reward total was -19.000000. running mean: -20.234452\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -20.232108\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.239787\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.237389\n",
            "resetting env. episode 792.000000, reward total was -19.000000. running mean: -20.225015\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.232765\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.230437\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.238133\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.235752\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.243394\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.240960\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.248550\n",
            "resetting env. episode 800.000000, reward total was -20.000000. running mean: -20.246065\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.253604\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.261068\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.268458\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.265773\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.273115\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -20.260384\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.257780\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.265202\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.272550\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.279825\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.277027\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.284256\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.281414\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.288600\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.285714\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.292857\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.299928\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.296929\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.303959\n",
            "resetting env. episode 820.000000, reward total was -18.000000. running mean: -20.280920\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.288111\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.295230\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.302277\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.309254\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.316162\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.323000\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.329770\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -20.326473\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.333208\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.329876\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.326577\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.323311\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.310078\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.306977\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.313908\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.310769\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.297661\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.304684\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.311637\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.308521\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.305436\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.312381\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.309258\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.316165\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.323003\n",
            "resetting env. episode 846.000000, reward total was -16.000000. running mean: -20.279773\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.286976\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.294106\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.301165\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.308153\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.315072\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.321921\n",
            "resetting env. episode 853.000000, reward total was -19.000000. running mean: -20.308702\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.315615\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -20.312459\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.309334\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.316241\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.313078\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.309947\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.316848\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.323680\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.330443\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.337138\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.343767\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.340329\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.336926\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -20.333557\n",
            "resetting env. episode 868.000000, reward total was -19.000000. running mean: -20.320221\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.327019\n",
            "resetting env. episode 870.000000, reward total was -19.000000. running mean: -20.313749\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.310611\n",
            "resetting env. episode 872.000000, reward total was -19.000000. running mean: -20.297505\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.304530\n",
            "resetting env. episode 874.000000, reward total was -19.000000. running mean: -20.291485\n",
            "resetting env. episode 875.000000, reward total was -19.000000. running mean: -20.278570\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.285784\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -20.282926\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.280097\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.287296\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.294423\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.291479\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.288564\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.295679\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.302722\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -20.289695\n",
            "resetting env. episode 886.000000, reward total was -18.000000. running mean: -20.266798\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.264130\n",
            "resetting env. episode 888.000000, reward total was -19.000000. running mean: -20.251488\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.258973\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.266384\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -20.253720\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.261183\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.268571\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.275885\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.273126\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -20.270395\n",
            "resetting env. episode 897.000000, reward total was -20.000000. running mean: -20.267691\n",
            "resetting env. episode 898.000000, reward total was -19.000000. running mean: -20.255014\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.262464\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -20.259839\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.267241\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.274569\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.281823\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.289005\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.296115\n",
            "resetting env. episode 906.000000, reward total was -20.000000. running mean: -20.293153\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.300222\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -20.297220\n",
            "resetting env. episode 909.000000, reward total was -19.000000. running mean: -20.284247\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.291405\n",
            "resetting env. episode 911.000000, reward total was -19.000000. running mean: -20.278491\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.285706\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.292849\n",
            "resetting env. episode 914.000000, reward total was -18.000000. running mean: -20.269921\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.277221\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.274449\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.281705\n",
            "resetting env. episode 918.000000, reward total was -18.000000. running mean: -20.258888\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.266299\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.273636\n",
            "resetting env. episode 921.000000, reward total was -19.000000. running mean: -20.260899\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.258290\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.255707\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.263150\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.260519\n",
            "resetting env. episode 926.000000, reward total was -19.000000. running mean: -20.247914\n",
            "resetting env. episode 927.000000, reward total was -18.000000. running mean: -20.225435\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.233180\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.230848\n",
            "resetting env. episode 930.000000, reward total was -19.000000. running mean: -20.218540\n",
            "resetting env. episode 931.000000, reward total was -19.000000. running mean: -20.206355\n",
            "resetting env. episode 932.000000, reward total was -17.000000. running mean: -20.174291\n",
            "resetting env. episode 933.000000, reward total was -20.000000. running mean: -20.172548\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.160823\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.159214\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.157622\n",
            "resetting env. episode 937.000000, reward total was -19.000000. running mean: -20.146046\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.154586\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -20.143040\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.141609\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.150193\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.148691\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.157204\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.155632\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.164076\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -20.152435\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.160911\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.159302\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.167709\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -20.166032\n",
            "resetting env. episode 951.000000, reward total was -19.000000. running mean: -20.154371\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.152828\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.161299\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.159686\n",
            "resetting env. episode 955.000000, reward total was -18.000000. running mean: -20.138089\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.136709\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.135341\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.143988\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.142548\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.141123\n",
            "resetting env. episode 961.000000, reward total was -17.000000. running mean: -20.109711\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.108614\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.117528\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.116353\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -20.115189\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.114038\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.112897\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.121768\n",
            "resetting env. episode 969.000000, reward total was -18.000000. running mean: -20.100550\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.109545\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -20.098450\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.107465\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.106390\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.105326\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -20.094273\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.103330\n",
            "resetting env. episode 977.000000, reward total was -19.000000. running mean: -20.092297\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -20.091374\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.100460\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.109456\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -20.108361\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.117278\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -20.116105\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -20.114944\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -20.113794\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.122656\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.121430\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.120216\n",
            "resetting env. episode 989.000000, reward total was -17.000000. running mean: -20.089013\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.098123\n",
            "resetting env. episode 991.000000, reward total was -18.000000. running mean: -20.077142\n",
            "resetting env. episode 992.000000, reward total was -19.000000. running mean: -20.066371\n",
            "resetting env. episode 993.000000, reward total was -19.000000. running mean: -20.055707\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.065150\n",
            "resetting env. episode 995.000000, reward total was -19.000000. running mean: -20.054498\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.053953\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -20.043414\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.042980\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -20.042550\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.052124\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.051603\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.051087\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.060576\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.049971\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.059471\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.068876\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.078187\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.077405\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.086631\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.085765\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.084907\n",
            "resetting env. episode 1012.000000, reward total was -19.000000. running mean: -20.074058\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.083318\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.092485\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.091560\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -20.080644\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.089838\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.088939\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -20.078050\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.087269\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.086397\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -20.075533\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.074777\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.084030\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.083189\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.082358\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.081534\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.090719\n",
            "resetting env. episode 1029.000000, reward total was -19.000000. running mean: -20.079811\n",
            "resetting env. episode 1030.000000, reward total was -19.000000. running mean: -20.069013\n",
            "resetting env. episode 1031.000000, reward total was -20.000000. running mean: -20.068323\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.077640\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.076864\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.086095\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.095234\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -20.094282\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.103339\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.102305\n",
            "resetting env. episode 1039.000000, reward total was -20.000000. running mean: -20.101282\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.110270\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -20.109167\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.118075\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.126894\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.125625\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.134369\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.143026\n",
            "resetting env. episode 1047.000000, reward total was -19.000000. running mean: -20.131595\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.140279\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.148877\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.147388\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.155914\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.164355\n",
            "resetting env. episode 1053.000000, reward total was -19.000000. running mean: -20.152711\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -20.141184\n",
            "resetting env. episode 1055.000000, reward total was -18.000000. running mean: -20.119772\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.118575\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.127389\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.136115\n",
            "resetting env. episode 1059.000000, reward total was -19.000000. running mean: -20.124754\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -20.123506\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -20.122271\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.131048\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.139738\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.128341\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.137057\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.145687\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.154230\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.162687\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.161061\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.169450\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -20.167755\n",
            "resetting env. episode 1072.000000, reward total was -18.000000. running mean: -20.146078\n",
            "resetting env. episode 1073.000000, reward total was -19.000000. running mean: -20.134617\n",
            "resetting env. episode 1074.000000, reward total was -19.000000. running mean: -20.123271\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.132038\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -20.120718\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.129511\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.128216\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.136933\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.145564\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.154108\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.162567\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -20.160942\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.169332\n",
            "resetting env. episode 1085.000000, reward total was -19.000000. running mean: -20.157639\n",
            "resetting env. episode 1086.000000, reward total was -20.000000. running mean: -20.156063\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.154502\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.162957\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.161327\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.169714\n",
            "resetting env. episode 1091.000000, reward total was -19.000000. running mean: -20.158017\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.146437\n",
            "resetting env. episode 1093.000000, reward total was -16.000000. running mean: -20.104972\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -20.093923\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.102983\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -20.101954\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.110934\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.119825\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.128626\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -20.117340\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.116167\n",
            "resetting env. episode 1102.000000, reward total was -20.000000. running mean: -20.115005\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.123855\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.132617\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.141290\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.139877\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.148479\n",
            "resetting env. episode 1108.000000, reward total was -19.000000. running mean: -20.136994\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.145624\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.144168\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.152726\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.161199\n",
            "resetting env. episode 1113.000000, reward total was -19.000000. running mean: -20.149587\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.148091\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.156610\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.165044\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.173393\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.181660\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.189843\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.197945\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -20.195965\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -20.194005\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.202065\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.210045\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.207944\n",
            "resetting env. episode 1126.000000, reward total was -19.000000. running mean: -20.195865\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -20.183906\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -20.182067\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.190246\n",
            "resetting env. episode 1130.000000, reward total was -18.000000. running mean: -20.168344\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -20.156661\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -20.155094\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -20.153543\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -20.142008\n",
            "resetting env. episode 1135.000000, reward total was -19.000000. running mean: -20.130587\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -20.119282\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.128089\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.136808\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.135440\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -20.124085\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.122845\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.131616\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.140300\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.148897\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.147408\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.145934\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.154475\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.162930\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.171301\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.179588\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.187792\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.195914\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.203955\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.211915\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -20.209796\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.217698\n",
            "resetting env. episode 1157.000000, reward total was -19.000000. running mean: -20.205521\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.213466\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.211331\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.219218\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -20.207026\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -20.204955\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.202906\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.210877\n",
            "resetting env. episode 1165.000000, reward total was -19.000000. running mean: -20.198768\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.196780\n",
            "resetting env. episode 1167.000000, reward total was -20.000000. running mean: -20.194812\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.192864\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.200936\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.208926\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -20.196837\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.194869\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.192920\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.190991\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.199081\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.197090\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.205119\n",
            "resetting env. episode 1178.000000, reward total was -19.000000. running mean: -20.193068\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.201137\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -20.189126\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.197235\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -20.185262\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.193410\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.201476\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.199461\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.207466\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -20.195392\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.203438\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.211403\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.219289\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.217096\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.214925\n",
            "resetting env. episode 1193.000000, reward total was -19.000000. running mean: -20.202776\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.210748\n",
            "resetting env. episode 1195.000000, reward total was -19.000000. running mean: -20.198641\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.206655\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -20.204588\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.212542\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.210417\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.218313\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.206129\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.214068\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.211927\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.219808\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.227610\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.235334\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.222981\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -20.220751\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.228543\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.236258\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -20.223895\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.231656\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -20.229340\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.237046\n",
            "resetting env. episode 1215.000000, reward total was -19.000000. running mean: -20.224676\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.222429\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.220205\n",
            "resetting env. episode 1218.000000, reward total was -19.000000. running mean: -20.208003\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.215923\n",
            "resetting env. episode 1220.000000, reward total was -18.000000. running mean: -20.193764\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -20.191826\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.199908\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -20.197909\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -20.195930\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.193970\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.192031\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -20.190110\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.198209\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -20.196227\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.194265\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -20.182322\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.190499\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.188594\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -20.186708\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -20.184841\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -20.182992\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.191163\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -20.189251\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -20.177358\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.175585\n",
            "resetting env. episode 1241.000000, reward total was -19.000000. running mean: -20.163829\n",
            "resetting env. episode 1242.000000, reward total was -20.000000. running mean: -20.162191\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -20.150569\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.149063\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -20.137572\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -20.136197\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.144835\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.153386\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.161853\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.160234\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.158632\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.167045\n",
            "resetting env. episode 1253.000000, reward total was -19.000000. running mean: -20.155375\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.153821\n",
            "resetting env. episode 1255.000000, reward total was -15.000000. running mean: -20.102283\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.101260\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.110248\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.119145\n",
            "resetting env. episode 1259.000000, reward total was -19.000000. running mean: -20.107954\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.116874\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.125705\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.134448\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.143104\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.151673\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.160156\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.168554\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.166869\n",
            "resetting env. episode 1268.000000, reward total was -19.000000. running mean: -20.155200\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.163648\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.162012\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.160392\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.158788\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.167200\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -20.165528\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -20.153873\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.162334\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.160710\n",
            "resetting env. episode 1278.000000, reward total was -19.000000. running mean: -20.149103\n",
            "resetting env. episode 1279.000000, reward total was -19.000000. running mean: -20.137612\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.136236\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -20.124874\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -20.123625\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -20.112389\n",
            "resetting env. episode 1284.000000, reward total was -18.000000. running mean: -20.091265\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.100352\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -20.099349\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.108355\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -20.097272\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.106299\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.105236\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -20.104184\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -20.103142\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.112110\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.120989\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.119779\n",
            "resetting env. episode 1296.000000, reward total was -18.000000. running mean: -20.098582\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -20.097596\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -20.096620\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.105654\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.114597\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.113451\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.122317\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.121093\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.119883\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.128684\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.127397\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -20.126123\n",
            "resetting env. episode 1308.000000, reward total was -17.000000. running mean: -20.094862\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.093913\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.092974\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -20.092044\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.101124\n",
            "resetting env. episode 1313.000000, reward total was -18.000000. running mean: -20.080113\n",
            "resetting env. episode 1314.000000, reward total was -19.000000. running mean: -20.069311\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.078618\n",
            "resetting env. episode 1316.000000, reward total was -18.000000. running mean: -20.057832\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.057254\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -20.056681\n",
            "resetting env. episode 1319.000000, reward total was -19.000000. running mean: -20.046114\n",
            "resetting env. episode 1320.000000, reward total was -18.000000. running mean: -20.025653\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.035397\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.035043\n",
            "resetting env. episode 1323.000000, reward total was -19.000000. running mean: -20.024692\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.024445\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.034201\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.043859\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -20.043420\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.052986\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.062456\n",
            "resetting env. episode 1330.000000, reward total was -18.000000. running mean: -20.041832\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -20.041413\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -20.040999\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.040589\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.050183\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.059682\n",
            "resetting env. episode 1336.000000, reward total was -17.000000. running mean: -20.029085\n",
            "resetting env. episode 1337.000000, reward total was -19.000000. running mean: -20.018794\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.028606\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.038320\n",
            "resetting env. episode 1340.000000, reward total was -19.000000. running mean: -20.027937\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -20.017657\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.017481\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.027306\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.037033\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -20.026663\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.036396\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.046032\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.055572\n",
            "resetting env. episode 1349.000000, reward total was -18.000000. running mean: -20.035016\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -20.024666\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.024419\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.024175\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -20.013933\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.013794\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.013656\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.023519\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.023284\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -20.013051\n",
            "resetting env. episode 1359.000000, reward total was -18.000000. running mean: -19.992921\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -19.992992\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -19.993062\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.003131\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.003100\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -19.993069\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.003138\n",
            "resetting env. episode 1366.000000, reward total was -19.000000. running mean: -19.993107\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.003176\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -20.003144\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.013112\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.012981\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.022852\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.022623\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.022397\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.032173\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -20.031851\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.041533\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.051117\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -20.050606\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -20.040100\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.049699\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.049202\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -20.038710\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -20.028323\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.028040\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.037759\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.047382\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.046908\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.056439\n",
            "resetting env. episode 1389.000000, reward total was -19.000000. running mean: -20.045874\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -20.035416\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.045061\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.054611\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.054065\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.063524\n",
            "resetting env. episode 1395.000000, reward total was -18.000000. running mean: -20.042889\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.052460\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.061935\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.061316\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.070703\n",
            "resetting env. episode 1400.000000, reward total was -19.000000. running mean: -20.059996\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.059396\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.068802\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.078114\n",
            "resetting env. episode 1404.000000, reward total was -19.000000. running mean: -20.067333\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.076659\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -20.075893\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.075134\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.074383\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.083639\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -20.082802\n",
            "resetting env. episode 1411.000000, reward total was -19.000000. running mean: -20.071974\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.081255\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.090442\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -20.079538\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.088742\n",
            "resetting env. episode 1416.000000, reward total was -20.000000. running mean: -20.087855\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.086976\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.096107\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.105145\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.104094\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.103053\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.102023\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -20.101002\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.109992\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -20.108892\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -20.107803\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.116725\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.125558\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.124303\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.133060\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.131729\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -20.130412\n",
            "resetting env. episode 1433.000000, reward total was -18.000000. running mean: -20.109108\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -20.108016\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.116936\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.115767\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.114609\n",
            "resetting env. episode 1438.000000, reward total was -20.000000. running mean: -20.113463\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.122329\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.131105\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.139794\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -20.138396\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -20.137012\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.135642\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.144286\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.152843\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -20.141314\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -20.139901\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.138502\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.137117\n",
            "resetting env. episode 1451.000000, reward total was -18.000000. running mean: -20.115746\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.124589\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -20.123343\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.132109\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.140788\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -20.139380\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.147987\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.156507\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.154942\n",
            "resetting env. episode 1460.000000, reward total was -18.000000. running mean: -20.133392\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.142058\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -20.130638\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -20.119331\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.128138\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.126857\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.125588\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -20.124332\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.133089\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -20.141758\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.150340\n",
            "resetting env. episode 1471.000000, reward total was -17.000000. running mean: -20.118837\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.117649\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -20.106472\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.105407\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.114353\n",
            "resetting env. episode 1476.000000, reward total was -19.000000. running mean: -20.103210\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -20.092178\n",
            "resetting env. episode 1478.000000, reward total was -20.000000. running mean: -20.091256\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.100343\n",
            "resetting env. episode 1480.000000, reward total was -19.000000. running mean: -20.089340\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.098447\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -20.097462\n",
            "resetting env. episode 1483.000000, reward total was -18.000000. running mean: -20.076487\n",
            "resetting env. episode 1484.000000, reward total was -19.000000. running mean: -20.065723\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.075065\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.084315\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.093472\n",
            "resetting env. episode 1488.000000, reward total was -19.000000. running mean: -20.082537\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -20.071711\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -20.070994\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.080284\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.089482\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.098587\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.107601\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.116525\n",
            "resetting env. episode 1496.000000, reward total was -19.000000. running mean: -20.105360\n",
            "resetting env. episode 1497.000000, reward total was -18.000000. running mean: -20.084306\n",
            "resetting env. episode 1498.000000, reward total was -19.000000. running mean: -20.073463\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -20.072728\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.082001\n",
            "CPU times: user 1h 12min 38s, sys: 32min 37s, total: 1h 45min 16s\n",
            "Wall time: 54min 14s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "w2NblmwDsL3y",
        "outputId": "7651b015-fc32-43eb-bf28-62a93ccae311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHKklEQVR4nO3dTW9cZxmA4dd1qnw4ifPptKZqaPgoUkFCojvaFSzoBomfwQL1V7BFgh/Bgi2LLhBrWLCoQAIEKSEikKbETRwncVISDQtUiXYo8j12MuP4upavdI6ekTy3znmtc2ZpMpkMgOK5eQ8A7D/CAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWTCAWSHZj3wO188uuPHap9bGuPNi4fHseefXKdeOHd2HDtydGr9xsbGuLe9vePznD21OlaPn9j1PHfu3R03b93e9XnYe5sXz417L57e9XmO3dgcp658sAcTzc/b73y4NMtxM4fjrS9Nf0nn6YXz58f509N/DPe2t2M4To2L6+u7nufa+zeEY0Ftfn5tfPCNV3Z9nnO/vbrvwzErtypAJhxAJhxAJhxANvPm6EFze2tr3Nm6O7V+4vjKOH3y5BwmYq+tXL81Vq5Pb2jfv7A67n7uzBwmWlzCsUMbt26Pv1y7NrV+cX1dOJ4Rq1f+OdZ//eep9fdfvyQcn+JWBciEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8iEA8i8yGeHjh45PM6srk6tHztyZA7T8CQ8XD027rx8dmr9wamVOUyz2IRjh9bX1sb62tq8x+AJ2njtpbHx2kvzHmNfcKsCZMIBZMIBZMIBZM/M5uj97e2xeWj64/zr0aN0ngcPPxqbW1u7nmf74YNdn4Mn4/DW9v/8/ZR8ns2d/5j5s2ZpMpnMdOCP3zoz24EwZ3v5h7u0h+eah7ff+XCmj/DMXHHATu33L/sisMcBZMIBZDPfqrz5g5/s5RzAPjLz5ujGxobNUdjnzp49O9OWj1sVIBMOIBMOIBMOIBMOIBMOIBMOIBMOIBMOIBMOIBMOIBMOIBMOIJv5sfp3f/ajvZwDmINvff+HMx3nnaNwgM36zlG3KkAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEAmHEB2aN4DfJbl5eWxtLQ0tf748eMxmUzmMBHwsYUNx9e/8uo4sbIytf7uH/44bm9tzWEi4GMLG45Dy8vj+UOfHG8ymXziKuSVb353HD//8hhjjL/+6udj68bVpzojHFQLG46duPTG98aLX3tjjDHGxpXfCQc8JTZHgUw4gEw4gEw4gGxfb47+7Te/GJv/eG+MMca9m3+f8zRwcOzrcPzplz+d9whwILlVAbJ9fcUBz4KPVg6PzUsXptYPbT8cpy7fGNMPXsyfcMCcPThzfFz99lfH+NSzWSvXb49Tl2/Maar/z60KkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkC3ssyqTMfx+CiyohQ3H7y+/N5aXpy+I7t7fnsM0wH9b2HDcvX9/3iMAn8EeB5AJB5DNfKty/suv7+UccGCtXDg5Hh3/wtT6kTN3x9qrD//zn4IFszTrfy5u3ry5gB8HKM6dOzfTmwlnvuJYWlrENyECT4M9DiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiCb+XdVgIPLFQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQ/RsuYMoBR1um/AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=200_le_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}