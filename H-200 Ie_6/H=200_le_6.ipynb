{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cWACPRL869I4","executionInfo":{"status":"ok","timestamp":1660933055471,"user_tz":420,"elapsed":3278,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[],"source":["!pip install gym >/dev/null"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2Os6feRY6ec_","executionInfo":{"status":"ok","timestamp":1660933060169,"user_tz":420,"elapsed":4716,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[],"source":["!pip install JSAnimation >/dev/null"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wotUOa_e6edP","executionInfo":{"status":"ok","timestamp":1660933060170,"user_tz":420,"elapsed":23,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[],"source":["%matplotlib inline\n","from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"R66_INeZ9nYX"},"source":["## Step 2: Playing Pong"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ngMhg3fB9aA","outputId":"62e5d652-2d7f-482f-c551-c805ab70b523","executionInfo":{"status":"ok","timestamp":1660933086221,"user_tz":420,"elapsed":26072,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n","Collecting ale-py~=0.7.5\n","  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 29.0 MB/s \n","\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=f4ae19082406dd5ac3bde5332c221e4906d99fd516c123baf0aaffadd5d5e6b4\n","  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"]}],"source":["%pip install -U gym>=0.21.0\n","%pip install -U gym[atari,accept-rom-license]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtT2GyK_6edc","outputId":"c225e262-a41a-4668-8531-890e5d2e082d","executionInfo":{"status":"ok","timestamp":1660933087685,"user_tz":420,"elapsed":1482,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  f\"The environment {id} is out of date. You should consider \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"]}],"source":["import gym\n","env = gym.make('Pong-v0')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRE6WmXQJ1Z0","outputId":"fcc8c620-dbeb-4221-f8e7-4fc334307db3","executionInfo":{"status":"ok","timestamp":1660933087686,"user_tz":420,"elapsed":23,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(6)"]},"metadata":{},"execution_count":6}],"source":["env.action_space"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yl_9d4HFJ31W","outputId":"6ad69cf9-7a19-4145-bf55-882943feb4c6","executionInfo":{"status":"ok","timestamp":1660933087687,"user_tz":420,"elapsed":20,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0, 255, (210, 160, 3), uint8)"]},"metadata":{},"execution_count":7}],"source":["env.observation_space"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trwRXI-h6eeI","outputId":"93bfad36-d55d-4671-fb7f-6e3550a2e133","executionInfo":{"status":"ok","timestamp":1660933087688,"user_tz":420,"elapsed":18,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  \"The argument mode in render method is deprecated; \"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n","  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  \"Core environment is written in old step API which returns one bool instead of two. \"\n"]},{"output_type":"stream","name":"stdout","text":["Episode finished without success, accumulated reward = -18.0\n"]}],"source":["# Run a demo of the environment\n","observation = env.reset()\n","cumulated_reward = 0\n","\n","frames = []\n","for t in range(1000):\n","#     print(observation)\n","    frames.append(env.render(mode = 'rgb_array'))\n","    # very stupid agent, just makes a random action within the allowd action space\n","    action = env.action_space.sample()\n","#     print(\"Action: {}\".format(t+1))    \n","    observation, reward, done, info = env.step(action)\n","#     print(reward)\n","    cumulated_reward += reward\n","    if done:\n","        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","        break\n","print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","\n","env.close()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"3zZTecVWLLes","executionInfo":{"status":"ok","timestamp":1660933087689,"user_tz":420,"elapsed":13,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[],"source":["def sigmoid(x): \n","  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n","\n","def prepro(I):\n","  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n","  I = I[35:195] # crop\n","  I = I[::2,::2,0] # downsample by factor of 2\n","  I[I == 144] = 0 # erase background (background type 1)\n","  I[I == 109] = 0 # erase background (background type 2)\n","  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n","  return I.astype(np.float).ravel()\n","\n","def policy_forward(x):\n","  h = np.dot(model['W1'], x)\n","  h[h<0] = 0 # ReLU nonlinearity\n","  logp = np.dot(model['W2'], h)\n","  p = sigmoid(logp)\n","  return p, h # return probability of taking action 2, and hidden state\n","\n","def model_step(model, observation, prev_x):\n","  # preprocess the observation, set input to network to be difference image\n","  cur_x = prepro(observation)\n","  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","  prev_x = cur_x\n","  \n","  # forward the policy network and sample an action from the returned probability\n","  aprob, _ = policy_forward(x)\n","  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n","  \n","  return action, prev_x\n","\n","def play_game(env, model):\n","  observation = env.reset()\n","\n","  frames = []\n","  cumulated_reward = 0\n","\n","  prev_x = None # used in computing the difference frame\n","\n","  for t in range(1000):\n","      frames.append(env.render(mode = 'rgb_array'))\n","      action, prev_x = model_step(model, observation, prev_x)\n","      observation, reward, done, info = env.step(action)\n","      cumulated_reward += reward\n","      if done:\n","          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","          break\n","  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","  display_frames_as_gif(frames)\n","  env.close()"]},{"cell_type":"markdown","metadata":{"id":"6gWvZQ7AQLQt"},"source":["## Step 3: Policy Gradient from Scratch"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"eqFm7hqcItWl","executionInfo":{"status":"ok","timestamp":1660933087690,"user_tz":420,"elapsed":14,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[],"source":["import numpy as np\n","\n","# model initialization\n","H = 200 # number of hidden layer neurons\n","D = 80 * 80 # input dimensionality: 80x80 grid\n","model = {}\n","model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n","model['W2'] = np.random.randn(H) / np.sqrt(H)\n","\n","# import pickle\n","# model = pickle.load(open('model.pkl', 'rb'))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"TwjiwKisQM19","executionInfo":{"status":"ok","timestamp":1660933087691,"user_tz":420,"elapsed":14,"user":{"displayName":"Abraham Chandy","userId":"14685478709044287314"}}},"outputs":[],"source":["# hyperparameters\n","batch_size = 10 # every how many episodes to do a param update?\n","# learning_rate = 1e-4\n","learning_rate = 1e-6\n"," \n","gamma = 0.99 # discount factor for reward\n","decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n","  \n","grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n","rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n","\n","def discount_rewards(r):\n","  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n","  discounted_r = np.zeros_like(r, dtype=np.float32)\n","  running_add = 0\n","  for t in reversed(range(0, r.size)):\n","    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n","    running_add = running_add * gamma + r[t]\n","    discounted_r[t] = running_add\n","  return discounted_r\n","\n","def policy_backward(epx, eph, epdlogp):\n","  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n","  dW2 = np.dot(eph.T, epdlogp).ravel()\n","  dh = np.outer(epdlogp, model['W2'])\n","  dh[eph <= 0] = 0 # backpro prelu\n","  dW1 = np.dot(dh.T, epx)\n","  return {'W1':dW1, 'W2':dW2}\n","\n","def train_model(env, model, total_episodes = 100):\n","  hist = []\n","  observation = env.reset()\n","\n","  prev_x = None # used in computing the difference frame\n","  xs,hs,dlogps,drs = [],[],[],[]\n","  running_reward = None\n","  reward_sum = 0\n","  episode_number = 0\n","\n","  while True:\n","    # preprocess the observation, set input to network to be difference image\n","    cur_x = prepro(observation)\n","    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","    prev_x = cur_x\n","\n","    # forward the policy network and sample an action from the returned probability\n","    aprob, h = policy_forward(x)\n","    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n","\n","    # record various intermediates (needed later for backprop)\n","    xs.append(x) # observation\n","    hs.append(h) # hidden state\n","    y = 1 if action == 2 else 0 # a \"fake label\"\n","    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n","\n","    # step the environment and get new measurements\n","    observation, reward, done, info = env.step(action)\n","    reward_sum += reward\n","\n","    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n","\n","    if done: # an episode finished\n","      episode_number += 1\n","\n","      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n","      epx = np.vstack(xs)\n","      eph = np.vstack(hs)\n","      epdlogp = np.vstack(dlogps)\n","      epr = np.vstack(drs)\n","      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n","\n","      # compute the discounted reward backwards through time\n","      discounted_epr = discount_rewards(epr)\n","      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n","      discounted_epr -= np.mean(discounted_epr)\n","      discounted_epr /= np.std(discounted_epr)\n","\n","      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n","      grad = policy_backward(epx, eph, epdlogp)\n","      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n","\n","      # perform rmsprop parameter update every batch_size episodes\n","      if episode_number % batch_size == 0:\n","        for k,v in model.items():\n","          g = grad_buffer[k] # gradient\n","          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n","          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n","          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n","\n","      # boring book-keeping\n","      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n","      hist.append((episode_number, reward_sum, running_reward))\n","      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n","      reward_sum = 0\n","      observation = env.reset() # reset env\n","      prev_x = None\n","      if episode_number == total_episodes: return hist\n","\n","      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n","        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6Ka_5Vl9Orm","outputId":"8ca53e35-bb45-4378-fd82-50f8b3671904"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.990000\n","resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.980100\n","resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980299\n","resetting env. episode 8.000000, reward total was -19.000000. running mean: -20.960496\n","resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.960891\n","resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.951282\n","resetting env. episode 11.000000, reward total was -19.000000. running mean: -20.931769\n","resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.932452\n","resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.923127\n","resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.913896\n","resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.904757\n","resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.905709\n","resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.906652\n","resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.907586\n","resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.898510\n","resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.899525\n","resetting env. episode 21.000000, reward total was -18.000000. running mean: -20.870529\n","resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.871824\n","resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.873106\n","resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.854375\n","resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.835831\n","resetting env. episode 26.000000, reward total was -19.000000. running mean: -20.817473\n","resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.819298\n","resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.801105\n","resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.803094\n","resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.795063\n","resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.797113\n","resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.799141\n","resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.791150\n","resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.793238\n","resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.795306\n","resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.787353\n","resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.789479\n","resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.791585\n","resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.773669\n","resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.775932\n","resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.778173\n","resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.780391\n","resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.782587\n","resetting env. episode 44.000000, reward total was -18.000000. running mean: -20.754761\n","resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.757214\n","resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.759642\n","resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.762045\n","resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.754425\n","resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.756880\n","resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.759312\n","resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.761719\n","resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.764101\n","resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.756460\n","resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.748896\n","resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.751407\n","resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.753893\n","resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.756354\n","resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.758790\n","resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.741202\n","resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.743790\n","resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.736352\n","resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.728989\n","resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.731699\n","resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.714382\n","resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.717238\n","resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.720066\n","resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.712865\n","resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.705737\n","resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.698679\n","resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.701692\n","resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.704675\n","resetting env. episode 72.000000, reward total was -18.000000. running mean: -20.677629\n","resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.670852\n","resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.664144\n","resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.667502\n","resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.670827\n","resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.674119\n","resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.677378\n","resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.680604\n","resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.683798\n","resetting env. episode 81.000000, reward total was -18.000000. running mean: -20.656960\n","resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.650391\n","resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.653887\n","resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.637348\n","resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.640974\n","resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.644565\n","resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.648119\n","resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.651638\n","resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.655121\n","resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.648570\n","resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.652084\n","resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.655564\n","resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.659008\n","resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.662418\n","resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.665794\n","resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.669136\n","resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.672444\n","resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.675720\n","resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.668963\n","resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.652273\n","resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.635750\n","resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.629393\n","resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.633099\n","resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.636768\n","resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.640400\n"]}],"source":["%time hist1 = train_model(env, model, total_episodes=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHYCDYwhlVLV"},"outputs":[],"source":["%time hist2 = train_model(env, model, total_episodes=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fheN9DRlWXQ"},"outputs":[],"source":["play_game(env, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AxOcQhIsKow"},"outputs":[],"source":["%time hist3 = train_model(env, model, total_episodes=1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2NblmwDsL3y"},"outputs":[],"source":["play_game(env, model)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"H=200_le_6.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}