{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "842dfab7-e253-4633-ed76-a4245a180462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 33.1 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=b177ad6ec3fb4a0b1b3fecbf1e971c746cc5064788662a96b0dd9505fe760eab\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "41ab654f-b316-408c-8c0b-7f5bdae13754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "32ac4b0b-1d0c-40d9-ad8e-6ee71e608cb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "145b3ee5-0e8c-4a07-8365-f22a32ff8dd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "69cd6c69-387f-4c5b-a647-a4b731f7f01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 800 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "413a0507-895f-42be-9097-39e324946000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.049010\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.048520\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.058035\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.067454\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -20.056780\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.066212\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.065550\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.074894\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.084145\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.083304\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.092471\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.091546\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.090631\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.079724\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.078927\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.088138\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.087257\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.096384\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.105420\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.104366\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.103322\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.102289\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.111266\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.110153\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.119052\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.127861\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.136583\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.145217\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.133765\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.132427\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.141103\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.139692\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.148295\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.156812\n",
            "resetting env. episode 41.000000, reward total was -18.000000. running mean: -20.135244\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.143891\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.142453\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.131028\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.119718\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.128521\n",
            "resetting env. episode 47.000000, reward total was -17.000000. running mean: -20.097235\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.106263\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.115200\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.124048\n",
            "resetting env. episode 51.000000, reward total was -18.000000. running mean: -20.102808\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.101780\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.110762\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.119654\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.128458\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.137173\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.145802\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.144344\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.152900\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.151371\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.159857\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.168259\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.176576\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.174810\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.183062\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.191232\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.189319\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.197426\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.195452\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.193497\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.191562\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.199647\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.207650\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.215574\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.213418\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.221284\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.229071\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.236780\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.244413\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.251968\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.259449\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.266854\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.264186\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.251544\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.259028\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.256438\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.243874\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.251435\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -20.238921\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.246531\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.244066\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.251626\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.259109\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.266518\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.273853\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.281114\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.278303\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.275520\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.262765\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.270137\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.277436\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.264662\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.262015\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.269395\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.276701\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.283934\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.271095\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.268384\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.275700\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.282943\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.290113\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.287212\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.284340\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.281497\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.288682\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.295795\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.302837\n",
            "resetting env. episode 118.000000, reward total was -18.000000. running mean: -20.279809\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.287011\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.274140\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.261399\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.268785\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -20.256097\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.263536\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.270901\n",
            "resetting env. episode 126.000000, reward total was -18.000000. running mean: -20.248192\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.245710\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.243253\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.240820\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.248412\n",
            "resetting env. episode 131.000000, reward total was -18.000000. running mean: -20.225928\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.223669\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.221432\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.229218\n",
            "resetting env. episode 135.000000, reward total was -16.000000. running mean: -20.186926\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.185056\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.183206\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.191374\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.199460\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.207465\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.215391\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.223237\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.221004\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.218794\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.226606\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.234340\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.231997\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.239677\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.237280\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.244907\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.242458\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.230034\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.237733\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.245356\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.242903\n",
            "resetting env. episode 156.000000, reward total was -18.000000. running mean: -20.220473\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.218269\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.226086\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.233825\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.241487\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.249072\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.236581\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.244216\n",
            "resetting env. episode 164.000000, reward total was -18.000000. running mean: -20.221773\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.229556\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.237260\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.244887\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.242439\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.220014\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.217814\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.225636\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.223380\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.221146\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.228934\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.236645\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.234279\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.241936\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.239516\n",
            "resetting env. episode 179.000000, reward total was -17.000000. running mean: -20.207121\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.195050\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.203100\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.211069\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.208958\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.216868\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.214700\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.222553\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.230327\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.238024\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.245644\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.253187\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.260655\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.258049\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.265468\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.262814\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.270185\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.267484\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.274809\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.282061\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.279240\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.266448\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.263783\n",
            "resetting env. episode 202.000000, reward total was -18.000000. running mean: -20.241145\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.228734\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.226447\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.234182\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.231840\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.229522\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.237227\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.244854\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.252406\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.259882\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.257283\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.254710\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.262163\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.269541\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.266846\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.274177\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.261436\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.268821\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.266133\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.263472\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.270837\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.278129\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.285347\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.292494\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.289569\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.286673\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.273807\n",
            "resetting env. episode 229.000000, reward total was -16.000000. running mean: -20.231069\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.238758\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.246370\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.253907\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.251367\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.238854\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.246465\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.254001\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.251461\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.258946\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.266357\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.273693\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.280956\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.288146\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.295265\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.282312\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.289489\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.296594\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.303628\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.300592\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.297586\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.304610\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.291564\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.298649\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.305662\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.312606\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.299479\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.296485\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.303520\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.300485\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.297480\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.304505\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.311460\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.318345\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.325162\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.321910\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.328691\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.335404\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.342050\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.348630\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.355143\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.361592\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.367976\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.374296\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.380553\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.376748\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.372980\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.379250\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.385458\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.391603\n",
            "resetting env. episode 279.000000, reward total was -18.000000. running mean: -20.367687\n",
            "resetting env. episode 280.000000, reward total was -17.000000. running mean: -20.334011\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.330670\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.327364\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.334090\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.320749\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.327542\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.334266\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.340924\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.347514\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.344039\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.340599\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.347193\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.343721\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.350284\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.346781\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.343313\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.349880\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.346381\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.342917\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.339488\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.326093\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.322832\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.329604\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.336308\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.342945\n",
            "resetting env. episode 305.000000, reward total was -18.000000. running mean: -20.319515\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.326320\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.333057\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.339726\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.336329\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.342966\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.349536\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.356041\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.362481\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.368856\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.355167\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.361615\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.357999\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.364419\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.360775\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.367167\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.373496\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.379761\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.375963\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.372204\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.368481\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.374797\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.381049\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.377238\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.373466\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.379731\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.385934\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.392075\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.378154\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.374372\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.380629\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.376822\n",
            "resetting env. episode 337.000000, reward total was -17.000000. running mean: -20.343054\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.349623\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.356127\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.362566\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.348940\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.355451\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.341896\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.348477\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.354993\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.341443\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.338028\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.344648\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.341202\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.327790\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.324512\n",
            "resetting env. episode 352.000000, reward total was -18.000000. running mean: -20.301267\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.298254\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.305271\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.302219\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.309196\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.306104\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.313043\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.319913\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.326714\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.333447\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.330112\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.336811\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.333443\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.340109\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.326707\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.323440\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.330206\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.316904\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.303735\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.300698\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.297691\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.304714\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.311667\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.308550\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.315464\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.322310\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.319087\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.325896\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.322637\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.329410\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.336116\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.322755\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.329528\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.316232\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.323070\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.329839\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.326541\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.333276\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.329943\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.326643\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.323377\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.330143\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.326842\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.333573\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.340238\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.336835\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.333467\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.340132\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.346731\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.353264\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.359731\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.366134\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.362472\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.368848\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.365159\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.371507\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.367792\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.374114\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.370373\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.356670\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.353103\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.359572\n",
            "resetting env. episode 414.000000, reward total was -18.000000. running mean: -20.335976\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.332616\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.339290\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.345897\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.342438\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.329014\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.335724\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.342367\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.348943\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.355453\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.361899\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.348280\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.334797\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.341449\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.348035\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.344554\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.341109\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.347698\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.344221\n",
            "resetting env. episode 433.000000, reward total was -18.000000. running mean: -20.320779\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.317571\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.314395\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.321251\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.318039\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.324858\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.331610\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.338294\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.344911\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.351461\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.347947\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.354467\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.350923\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.347413\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.353939\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.360400\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.366796\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.363128\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.359497\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.365902\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.372243\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.378520\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.384735\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.390888\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.396979\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.403009\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.408979\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.414889\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.420740\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.416533\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.422368\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.428144\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.423862\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.429624\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.415328\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.421174\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.416963\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.422793\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.408565\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.414479\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.400335\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.406331\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.402268\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.408245\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.404163\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.410121\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.416020\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.411860\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.417741\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.413564\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.409428\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.395334\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.401380\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.407367\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.413293\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.419160\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.414968\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.400819\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.386811\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.382943\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.389113\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.395222\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.401270\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.407257\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.403184\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.409153\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.395061\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.381110\n",
            "CPU times: user 1h 4min 35s, sys: 12min 11s, total: 1h 16min 47s\n",
            "Wall time: 39min 35s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "92f84142-8119-4858-b666-f5d376d8f64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.019900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.039701\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -19.039304\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.048911\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.068422\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.087738\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.106860\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -19.105792\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -19.104734\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -19.103686\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.122650\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.131423\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -19.140109\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.148708\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.167221\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.175548\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.193793\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.201855\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.209836\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.227738\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -19.225461\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.233206\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.250874\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.268365\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -19.275682\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -19.272925\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -19.270196\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.287494\n",
            "resetting env. episode 31.000000, reward total was -18.000000. running mean: -19.274619\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -19.291873\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -19.298954\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.305964\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.312905\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.329776\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -19.336478\n",
            "resetting env. episode 38.000000, reward total was -18.000000. running mean: -19.323113\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.339882\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.346483\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.353018\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -19.349488\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.365993\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.382333\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -19.378510\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -19.374725\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -19.370978\n",
            "resetting env. episode 48.000000, reward total was -17.000000. running mean: -19.347268\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.363795\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -19.370157\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.386456\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.402591\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.408565\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.424479\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -19.410235\n",
            "resetting env. episode 56.000000, reward total was -18.000000. running mean: -19.396132\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.412171\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.418049\n",
            "resetting env. episode 59.000000, reward total was -18.000000. running mean: -19.403869\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.419830\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.425632\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.441376\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.446962\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.462492\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -19.457867\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.473289\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.488556\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.493670\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.508733\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.523646\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.538410\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.553026\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.567495\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.571820\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.586102\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.600241\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.614239\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -19.608096\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -19.602015\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.615995\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -19.609835\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.623737\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.637499\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.651124\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.664613\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -19.677967\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.681187\n",
            "resetting env. episode 88.000000, reward total was -16.000000. running mean: -19.644376\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.647932\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.661452\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.674838\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.678090\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.691309\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.694396\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.697452\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.710477\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.723372\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.726139\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -19.718877\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.731688\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.744372\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -19.736928\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.739559\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.742163\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.744741\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.757294\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.769721\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.772024\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.774304\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.786561\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.788695\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.790808\n",
            "resetting env. episode 113.000000, reward total was -16.000000. running mean: -19.752900\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.755371\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -19.757817\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.760239\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -19.762637\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -19.755010\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -19.767460\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.769786\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.782088\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.784267\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -19.776424\n",
            "resetting env. episode 124.000000, reward total was -18.000000. running mean: -19.758660\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.761073\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.773463\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -19.765728\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -19.768071\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.770390\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.782686\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.794859\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.806911\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.818842\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -19.820653\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.832447\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -19.834122\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.845781\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.857323\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -19.858750\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -19.850162\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.861661\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.863044\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -19.864414\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -19.865770\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -19.867112\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.878441\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -19.879656\n",
            "resetting env. episode 148.000000, reward total was -17.000000. running mean: -19.850860\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -19.842351\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.853928\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -19.865388\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.876734\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -19.867967\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -19.859287\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.870695\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -19.881988\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -19.883168\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.894336\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -19.905393\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.916339\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -19.927175\n",
            "resetting env. episode 162.000000, reward total was -18.000000. running mean: -19.907904\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -19.908825\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -19.919736\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -19.930539\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -19.931234\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -19.931921\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -19.942602\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -19.953176\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -19.953644\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.964108\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -19.964467\n",
            "resetting env. episode 173.000000, reward total was -18.000000. running mean: -19.944822\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -19.955374\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -19.965820\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.976162\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.986400\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -19.996536\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.006571\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.016505\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.016340\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.016177\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.026015\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.015755\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.005597\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.005541\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -19.995486\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.005531\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.005476\n",
            "resetting env. episode 190.000000, reward total was -17.000000. running mean: -19.975421\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -19.985667\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -19.995810\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -19.985852\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -19.995994\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -19.996034\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -19.996073\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.006113\n",
            "resetting env. episode 198.000000, reward total was -18.000000. running mean: -19.986051\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -19.996191\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.006229\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -19.996167\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.006205\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.016143\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.015982\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.005822\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.015763\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.025606\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.035350\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.044996\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.054546\n",
            "resetting env. episode 211.000000, reward total was -18.000000. running mean: -20.034001\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.043661\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.043224\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.052792\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.052264\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.041741\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -20.021324\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.031111\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.040800\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.050392\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.049888\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.049389\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.058895\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.068306\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.067623\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.066947\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.066277\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.075615\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.074858\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.074110\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.083369\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.082535\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.091710\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.090793\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.099885\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.108886\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.107797\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.106719\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.105652\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.104595\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.113549\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.102414\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.101390\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.110376\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.119272\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.128079\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.136799\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.135431\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.144076\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.142635\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.151209\n",
            "resetting env. episode 252.000000, reward total was -18.000000. running mean: -20.129697\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.128400\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.127116\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.135845\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.144486\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.153042\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.151511\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.149996\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.148496\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.157011\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.155441\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.153887\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.152348\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.140824\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.149416\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.137922\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.136543\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.145177\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.143725\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.152288\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.160765\n",
            "resetting env. episode 273.000000, reward total was -18.000000. running mean: -20.139158\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.147766\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.156288\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.154726\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.153178\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.141646\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.150230\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.158728\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.167140\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.175469\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.183714\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.191877\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.199958\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.197959\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.195979\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.204019\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.211979\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.219859\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.227661\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.225384\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.223130\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.230899\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.238590\n",
            "resetting env. episode 296.000000, reward total was -18.000000. running mean: -20.216204\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.214042\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.221902\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.229683\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.227386\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.215112\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.222961\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.230731\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.218424\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.206240\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.204177\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.202136\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.210114\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.218013\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.225833\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.223575\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.231339\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.229026\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.236735\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.244368\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.251924\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.249405\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.236911\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.224542\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.232296\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.239973\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.247574\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.255098\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.242547\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.250122\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.257620\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.255044\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.252494\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.249969\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.247469\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.244994\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.252544\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.250019\n",
            "resetting env. episode 334.000000, reward total was -18.000000. running mean: -20.227519\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.225244\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.232991\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.240661\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.228255\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.235972\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.233612\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.221276\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.219063\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.216873\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.224704\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.232457\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.240133\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.247731\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.255254\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.252701\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.260174\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.267573\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.264897\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.272248\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.279525\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.286730\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.293863\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.300924\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.307915\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.294836\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.301887\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.288869\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.295980\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.283020\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.290190\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.287288\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.294415\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.291471\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.298556\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.285571\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.292715\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.289788\n",
            "resetting env. episode 372.000000, reward total was -18.000000. running mean: -20.266890\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.264221\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.261579\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.268963\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.256273\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.263711\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.261074\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.268463\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.255778\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.243220\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.250788\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -20.228280\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.235998\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.243638\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.251201\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.238689\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.246302\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.243839\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.231401\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.239087\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.226696\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.224429\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.232185\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.229863\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.237564\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.235189\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.242837\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.250408\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.257904\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.255325\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.262772\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.260144\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.267543\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.274867\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.272119\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.269398\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.266704\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.274037\n",
            "resetting env. episode 410.000000, reward total was -18.000000. running mean: -20.251296\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.248783\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.256295\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.243732\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.231295\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.238982\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.236592\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.234226\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.241884\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.229465\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.227171\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.234899\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.232550\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.240224\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.247822\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.255344\n",
            "resetting env. episode 426.000000, reward total was -18.000000. running mean: -20.232791\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.240463\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.228058\n",
            "resetting env. episode 429.000000, reward total was -18.000000. running mean: -20.205777\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.203720\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.211682\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.209566\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.207470\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.205395\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.213341\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.221208\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.208996\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.206906\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.214837\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.222688\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.210462\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.218357\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.216173\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.224012\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.221772\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.229554\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.217258\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.215086\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.212935\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.210805\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.208697\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.196610\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.204644\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.212598\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.210472\n",
            "resetting env. episode 456.000000, reward total was -18.000000. running mean: -20.188367\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.196484\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.194519\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.202574\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.190548\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.178642\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.176856\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.185087\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.193236\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.201304\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.199291\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.207298\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.215225\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.223073\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.210842\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.218734\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.226546\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.224281\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.222038\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.229818\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.227520\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.225244\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.222992\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.230762\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.228454\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.226170\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.233908\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.231569\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.219253\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.227061\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.224790\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.222542\n",
            "resetting env. episode 488.000000, reward total was -17.000000. running mean: -20.190317\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.188414\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.186530\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.194664\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.202718\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.190690\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.198784\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.206796\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.214728\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.212581\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.220455\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.228250\n",
            "resetting env. episode 500.000000, reward total was -18.000000. running mean: -20.205968\n",
            "CPU times: user 1h 8min 38s, sys: 12min 59s, total: 1h 21min 38s\n",
            "Wall time: 42min 8s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "d63070ee-cdef-461f-9eee-34567503c70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "18bd645d-1d03-4d34-955e-5c7045dfd873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.019701\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.029504\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.029209\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.038917\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.028528\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.018242\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.018060\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.027879\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.037601\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.047225\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.056752\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.056185\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.045623\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.045167\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.044715\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.054268\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.063725\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.063088\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.062457\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.071833\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.071114\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.070403\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.079699\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.088902\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.078013\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.087233\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.086361\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.075497\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.074742\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.083995\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.093155\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.092223\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.101301\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.110288\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.119185\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.127993\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -20.116713\n",
            "resetting env. episode 42.000000, reward total was -17.000000. running mean: -20.085546\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.084691\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.083844\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.073005\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.072275\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.071552\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.070837\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.070129\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.059427\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.048833\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.058345\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.057761\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.057184\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.056612\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.056046\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.065485\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.054830\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.044282\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.053839\n",
            "resetting env. episode 61.000000, reward total was -17.000000. running mean: -20.023301\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.033068\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.022737\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.032510\n",
            "resetting env. episode 65.000000, reward total was -18.000000. running mean: -20.012185\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.012063\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.001942\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -19.991923\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.992004\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.002084\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -19.992063\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.002142\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.012121\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.021999\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.021779\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.031562\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.021246\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.031034\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.030723\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.040416\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.040012\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.049612\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.059116\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.048524\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.048039\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.057559\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.066983\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.066313\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.075650\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.074894\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.074145\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -20.063403\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.072769\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.082042\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.091221\n",
            "resetting env. episode 96.000000, reward total was -17.000000. running mean: -20.060309\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.069706\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.079009\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.078219\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.087437\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.096562\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.095597\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.084641\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -20.073794\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -20.063056\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.062426\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.061801\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.061183\n",
            "resetting env. episode 109.000000, reward total was -16.000000. running mean: -20.020572\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.020366\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.030162\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.019861\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.019662\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.019465\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.029271\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.028978\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.028688\n",
            "resetting env. episode 118.000000, reward total was -18.000000. running mean: -20.008401\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.008317\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -19.998234\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.998252\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.998269\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.998287\n",
            "resetting env. episode 124.000000, reward total was -18.000000. running mean: -19.978304\n",
            "resetting env. episode 125.000000, reward total was -18.000000. running mean: -19.958521\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -19.958936\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -19.959346\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -19.949753\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.950255\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -19.940753\n",
            "resetting env. episode 131.000000, reward total was -18.000000. running mean: -19.921345\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -19.902132\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.913110\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.923979\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -19.914739\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.925592\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -19.916336\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -19.917173\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.928001\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.938721\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.949334\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -19.939840\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.950442\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.960938\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -19.961328\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.971715\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -19.971998\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -19.982278\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -19.982455\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.992631\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.002704\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.012677\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.012550\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.012425\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.022301\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.032078\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.041757\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.041339\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.040926\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.030517\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.030211\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.039909\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.049510\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.039015\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.048625\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.048139\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.057657\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.047081\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.056610\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.066044\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.055383\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.054830\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.054281\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.043739\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.033301\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.032968\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.042638\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.032212\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.041890\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.041471\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.041056\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.030646\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.020339\n",
            "resetting env. episode 184.000000, reward total was -18.000000. running mean: -20.000136\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.000135\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -19.990133\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.000232\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.010230\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.010127\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.020026\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.019826\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.009627\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.019531\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.019336\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.019143\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.028951\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.018662\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.018475\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.018290\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.018107\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.027926\n",
            "resetting env. episode 202.000000, reward total was -18.000000. running mean: -20.007647\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.007571\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.017495\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.027320\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.027047\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.036776\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.026408\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.036144\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.045783\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.045325\n",
            "resetting env. episode 212.000000, reward total was -18.000000. running mean: -20.024872\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.024623\n",
            "resetting env. episode 214.000000, reward total was -17.000000. running mean: -19.994377\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -19.994433\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.004489\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.004444\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -19.994399\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -19.994455\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -19.994511\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -19.994566\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.004620\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -19.994574\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -19.984628\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -19.984782\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -19.974934\n",
            "resetting env. episode 227.000000, reward total was -16.000000. running mean: -19.935185\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -19.945833\n",
            "resetting env. episode 229.000000, reward total was -18.000000. running mean: -19.926375\n",
            "resetting env. episode 230.000000, reward total was -18.000000. running mean: -19.907111\n",
            "resetting env. episode 231.000000, reward total was -16.000000. running mean: -19.868040\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -19.859359\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -19.870766\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -19.882058\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -19.893237\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -19.884305\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -19.895462\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -19.896507\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -19.907542\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -19.918467\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -19.929282\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -19.929989\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -19.940690\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -19.941283\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -19.941870\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -19.932451\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -19.943127\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -19.953695\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -19.964158\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -19.974517\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -19.984772\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -19.974924\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -19.985175\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -19.975323\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -19.985570\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -19.985714\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -19.985857\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -19.985998\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -19.976138\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -19.976377\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -19.986613\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -19.986747\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -19.976880\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -19.977111\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -19.977340\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -19.987566\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -19.997691\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -19.997714\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -19.987737\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -19.977859\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -19.988081\n",
            "resetting env. episode 272.000000, reward total was -18.000000. running mean: -19.968200\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -19.978518\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -19.988733\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -19.998845\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.008857\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.008768\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -19.998681\n",
            "resetting env. episode 279.000000, reward total was -18.000000. running mean: -19.978694\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -19.978907\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -19.989118\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -19.999227\n",
            "resetting env. episode 283.000000, reward total was -19.000000. running mean: -19.989234\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -19.979342\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -19.989549\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -19.989653\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -19.999757\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -19.999759\n",
            "resetting env. episode 289.000000, reward total was -16.000000. running mean: -19.959761\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -19.970164\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -19.980462\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -19.970658\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -19.980951\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -19.981141\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -19.981330\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -19.981517\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -19.981702\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -19.991885\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.001966\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -19.991946\n",
            "resetting env. episode 301.000000, reward total was -18.000000. running mean: -19.972027\n",
            "resetting env. episode 302.000000, reward total was -18.000000. running mean: -19.952306\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -19.962783\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -19.953155\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -19.963624\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -19.973988\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -19.984248\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -19.994405\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -19.984461\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -19.994617\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.004670\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.004624\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -19.994578\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -19.994632\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.004685\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.014639\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.004492\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -19.994447\n",
            "resetting env. episode 319.000000, reward total was -18.000000. running mean: -19.974503\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -19.984758\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -19.994910\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -19.994961\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -19.985011\n",
            "resetting env. episode 324.000000, reward total was -17.000000. running mean: -19.955161\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -19.965610\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -19.975954\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -19.976194\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -19.976432\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -19.986668\n",
            "resetting env. episode 330.000000, reward total was -18.000000. running mean: -19.966801\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -19.977133\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -19.967362\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -19.967688\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -19.968011\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -19.978331\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -19.988548\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -19.978662\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -19.978876\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -19.989087\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -19.989196\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -19.989304\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -19.989411\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -19.989517\n",
            "resetting env. episode 344.000000, reward total was -18.000000. running mean: -19.969622\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -19.959926\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -19.960326\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -19.960723\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -19.971116\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -19.971405\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -19.971691\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -19.971974\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -19.982254\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -19.982432\n",
            "resetting env. episode 354.000000, reward total was -17.000000. running mean: -19.952607\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -19.963081\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -19.953450\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -19.943916\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -19.934477\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -19.935132\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -19.945781\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -19.946323\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -19.936860\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -19.937491\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -19.928116\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -19.928835\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -19.929547\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -19.930251\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -19.940949\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -19.931539\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -19.932224\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -19.932901\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -19.943572\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -19.944137\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -19.954695\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -19.955148\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -19.965597\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -19.965941\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -19.956282\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -19.966719\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -19.977052\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -19.987281\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -19.997408\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.007434\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -19.997360\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -19.987386\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -19.997512\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -19.977537\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -19.987762\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -19.987884\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -19.998005\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -19.988025\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -19.988145\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -19.998264\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.008281\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.018198\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.018016\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.027836\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.027558\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.027282\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.027009\n",
            "resetting env. episode 401.000000, reward total was -17.000000. running mean: -19.996739\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.006772\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.006704\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.016637\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.016471\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.026306\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.036043\n",
            "resetting env. episode 408.000000, reward total was -18.000000. running mean: -20.015682\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.025526\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.035270\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.034918\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.034568\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.034223\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.043881\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.053442\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.052907\n",
            "resetting env. episode 417.000000, reward total was -17.000000. running mean: -20.022378\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.032154\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.041833\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.051415\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.060900\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.060291\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.069689\n",
            "resetting env. episode 424.000000, reward total was -18.000000. running mean: -20.048992\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.038502\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.048117\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.037636\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.037259\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.026887\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.026618\n",
            "resetting env. episode 431.000000, reward total was -18.000000. running mean: -20.006352\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.006288\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -19.996225\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.006263\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -19.996200\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -19.996238\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.006276\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.016213\n",
            "resetting env. episode 439.000000, reward total was -18.000000. running mean: -19.996051\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.006091\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.006030\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.005969\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.005910\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.015851\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.005692\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.015635\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.005479\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.015424\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.015270\n",
            "resetting env. episode 450.000000, reward total was -18.000000. running mean: -19.995117\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.005166\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.015114\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.024963\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.024713\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.014466\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.024322\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.034078\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.023738\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.023500\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.033265\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.042933\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.042503\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.052078\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.041557\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.051142\n",
            "resetting env. episode 466.000000, reward total was -17.000000. running mean: -20.020630\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.030424\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.040120\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.029719\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.029422\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.029127\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.038836\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.038448\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.048063\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.047583\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.047107\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.056636\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.056069\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.065509\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.074854\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.074105\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.083364\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.082530\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.091705\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.090788\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.079880\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.069081\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.058390\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.057807\n",
            "resetting env. episode 490.000000, reward total was -18.000000. running mean: -20.037228\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.026856\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.016588\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.026422\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.026158\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.035896\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.025537\n",
            "resetting env. episode 497.000000, reward total was -18.000000. running mean: -20.005282\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.015229\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.005077\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.005026\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.004976\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.014926\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.014777\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.024629\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.024382\n",
            "resetting env. episode 506.000000, reward total was -19.000000. running mean: -20.014139\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.013997\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.013857\n",
            "resetting env. episode 509.000000, reward total was -19.000000. running mean: -20.003719\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.003682\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.003645\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.013608\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.023472\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.033237\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -20.022905\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.022676\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.032449\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.042125\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.051704\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.061186\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.070575\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.079869\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.089070\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.088179\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.087298\n",
            "resetting env. episode 526.000000, reward total was -18.000000. running mean: -20.066425\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.065760\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.065103\n",
            "resetting env. episode 529.000000, reward total was -18.000000. running mean: -20.044452\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.044007\n",
            "resetting env. episode 531.000000, reward total was -18.000000. running mean: -20.023567\n",
            "resetting env. episode 532.000000, reward total was -18.000000. running mean: -20.003332\n",
            "resetting env. episode 533.000000, reward total was -19.000000. running mean: -19.993298\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.003365\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.003332\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.013298\n",
            "resetting env. episode 537.000000, reward total was -20.000000. running mean: -20.013165\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.023034\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.022803\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.032575\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.042250\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.041827\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.051409\n",
            "resetting env. episode 544.000000, reward total was -18.000000. running mean: -20.030895\n",
            "resetting env. episode 545.000000, reward total was -19.000000. running mean: -20.020586\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -20.010380\n",
            "resetting env. episode 547.000000, reward total was -17.000000. running mean: -19.980276\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -19.980473\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -19.990669\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.000762\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.010754\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.020647\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.010440\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.020336\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.020133\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.019931\n",
            "resetting env. episode 557.000000, reward total was -19.000000. running mean: -20.009732\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.009635\n",
            "resetting env. episode 559.000000, reward total was -17.000000. running mean: -19.979538\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -19.979743\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -19.979945\n",
            "resetting env. episode 562.000000, reward total was -19.000000. running mean: -19.970146\n",
            "resetting env. episode 563.000000, reward total was -19.000000. running mean: -19.960444\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -19.950840\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -19.961332\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -19.961718\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -19.972101\n",
            "resetting env. episode 568.000000, reward total was -19.000000. running mean: -19.962380\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -19.962756\n",
            "resetting env. episode 570.000000, reward total was -19.000000. running mean: -19.953129\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -19.963597\n",
            "resetting env. episode 572.000000, reward total was -19.000000. running mean: -19.953962\n",
            "resetting env. episode 573.000000, reward total was -19.000000. running mean: -19.944422\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -19.944978\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -19.945528\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -19.946073\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -19.946612\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -19.957146\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -19.967574\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -19.957899\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -19.958320\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -19.958736\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -19.959149\n",
            "resetting env. episode 584.000000, reward total was -19.000000. running mean: -19.949558\n",
            "resetting env. episode 585.000000, reward total was -19.000000. running mean: -19.940062\n",
            "resetting env. episode 586.000000, reward total was -19.000000. running mean: -19.930661\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -19.931355\n",
            "resetting env. episode 588.000000, reward total was -20.000000. running mean: -19.932041\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -19.932721\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -19.943394\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -19.943960\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -19.954520\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -19.964975\n",
            "resetting env. episode 594.000000, reward total was -19.000000. running mean: -19.955325\n",
            "resetting env. episode 595.000000, reward total was -19.000000. running mean: -19.945772\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -19.946314\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -19.946851\n",
            "resetting env. episode 598.000000, reward total was -19.000000. running mean: -19.937382\n",
            "resetting env. episode 599.000000, reward total was -19.000000. running mean: -19.928009\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -19.928729\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -19.919441\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -19.930247\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -19.940944\n",
            "resetting env. episode 604.000000, reward total was -18.000000. running mean: -19.921535\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -19.922320\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -19.933096\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -19.933765\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -19.944428\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -19.944983\n",
            "resetting env. episode 610.000000, reward total was -19.000000. running mean: -19.935534\n",
            "resetting env. episode 611.000000, reward total was -18.000000. running mean: -19.916178\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -19.927017\n",
            "resetting env. episode 613.000000, reward total was -19.000000. running mean: -19.917746\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -19.918569\n",
            "resetting env. episode 615.000000, reward total was -19.000000. running mean: -19.909383\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -19.920289\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -19.931086\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -19.931776\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -19.932458\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -19.933133\n",
            "resetting env. episode 621.000000, reward total was -18.000000. running mean: -19.913802\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -19.914664\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -19.925517\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -19.936262\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -19.946900\n",
            "resetting env. episode 626.000000, reward total was -20.000000. running mean: -19.947431\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -19.937956\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -19.948577\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -19.959091\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -19.969500\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -19.979805\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -19.980007\n",
            "resetting env. episode 633.000000, reward total was -19.000000. running mean: -19.970207\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -19.970505\n",
            "resetting env. episode 635.000000, reward total was -19.000000. running mean: -19.960800\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -19.971192\n",
            "resetting env. episode 637.000000, reward total was -19.000000. running mean: -19.961480\n",
            "resetting env. episode 638.000000, reward total was -19.000000. running mean: -19.951865\n",
            "resetting env. episode 639.000000, reward total was -18.000000. running mean: -19.932346\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -19.933023\n",
            "resetting env. episode 641.000000, reward total was -19.000000. running mean: -19.923693\n",
            "resetting env. episode 642.000000, reward total was -19.000000. running mean: -19.914456\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -19.915311\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -19.926158\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -19.916897\n",
            "resetting env. episode 646.000000, reward total was -19.000000. running mean: -19.907728\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -19.918650\n",
            "resetting env. episode 648.000000, reward total was -19.000000. running mean: -19.909464\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -19.910369\n",
            "resetting env. episode 650.000000, reward total was -19.000000. running mean: -19.901265\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -19.912253\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -19.923130\n",
            "resetting env. episode 653.000000, reward total was -19.000000. running mean: -19.913899\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -19.924760\n",
            "resetting env. episode 655.000000, reward total was -18.000000. running mean: -19.905512\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -19.906457\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -19.917393\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -19.908219\n",
            "resetting env. episode 659.000000, reward total was -19.000000. running mean: -19.899137\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -19.910145\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -19.921044\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -19.931833\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -19.922515\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -19.923290\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -19.924057\n",
            "resetting env. episode 666.000000, reward total was -18.000000. running mean: -19.904816\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -19.915768\n",
            "resetting env. episode 668.000000, reward total was -19.000000. running mean: -19.906610\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -19.907544\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -19.908469\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -19.919384\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -19.930190\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -19.940889\n",
            "resetting env. episode 674.000000, reward total was -19.000000. running mean: -19.931480\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -19.942165\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -19.952743\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -19.963216\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -19.953584\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -19.954048\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -19.964507\n",
            "resetting env. episode 681.000000, reward total was -16.000000. running mean: -19.924862\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -19.925614\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -19.936357\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -19.946994\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -19.957524\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -19.967949\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -19.978269\n",
            "resetting env. episode 688.000000, reward total was -19.000000. running mean: -19.968487\n",
            "resetting env. episode 689.000000, reward total was -20.000000. running mean: -19.968802\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -19.969114\n",
            "resetting env. episode 691.000000, reward total was -18.000000. running mean: -19.949423\n",
            "resetting env. episode 692.000000, reward total was -18.000000. running mean: -19.929928\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -19.930629\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -19.941323\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -19.931909\n",
            "resetting env. episode 696.000000, reward total was -19.000000. running mean: -19.922590\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -19.933364\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -19.944031\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -19.954591\n",
            "resetting env. episode 700.000000, reward total was -19.000000. running mean: -19.945045\n",
            "resetting env. episode 701.000000, reward total was -19.000000. running mean: -19.935594\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -19.926238\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -19.936976\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -19.947606\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -19.958130\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -19.968549\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -19.978863\n",
            "resetting env. episode 708.000000, reward total was -18.000000. running mean: -19.959075\n",
            "resetting env. episode 709.000000, reward total was -19.000000. running mean: -19.949484\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -19.959989\n",
            "resetting env. episode 711.000000, reward total was -17.000000. running mean: -19.930389\n",
            "resetting env. episode 712.000000, reward total was -19.000000. running mean: -19.921085\n",
            "resetting env. episode 713.000000, reward total was -19.000000. running mean: -19.911874\n",
            "resetting env. episode 714.000000, reward total was -19.000000. running mean: -19.902756\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -19.913728\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -19.924591\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -19.935345\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -19.935991\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -19.946632\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -19.947165\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -19.947694\n",
            "resetting env. episode 722.000000, reward total was -17.000000. running mean: -19.918217\n",
            "resetting env. episode 723.000000, reward total was -19.000000. running mean: -19.909034\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -19.909944\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -19.920845\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -19.931636\n",
            "resetting env. episode 727.000000, reward total was -19.000000. running mean: -19.922320\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -19.933097\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -19.933766\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -19.944428\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -19.944984\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -19.955534\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -19.965979\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -19.976319\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -19.986556\n",
            "resetting env. episode 736.000000, reward total was -19.000000. running mean: -19.976690\n",
            "resetting env. episode 737.000000, reward total was -18.000000. running mean: -19.956923\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -19.957354\n",
            "resetting env. episode 739.000000, reward total was -19.000000. running mean: -19.947780\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -19.948303\n",
            "resetting env. episode 741.000000, reward total was -18.000000. running mean: -19.928820\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -19.929531\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -19.930236\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -19.940934\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -19.951524\n",
            "resetting env. episode 746.000000, reward total was -17.000000. running mean: -19.922009\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -19.932789\n",
            "resetting env. episode 748.000000, reward total was -18.000000. running mean: -19.913461\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -19.914327\n",
            "resetting env. episode 750.000000, reward total was -19.000000. running mean: -19.905183\n",
            "resetting env. episode 751.000000, reward total was -18.000000. running mean: -19.886131\n",
            "resetting env. episode 752.000000, reward total was -18.000000. running mean: -19.867270\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -19.878597\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -19.889811\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -19.890913\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -19.892004\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -19.903084\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -19.914053\n",
            "resetting env. episode 759.000000, reward total was -19.000000. running mean: -19.904913\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -19.905864\n",
            "resetting env. episode 761.000000, reward total was -16.000000. running mean: -19.866805\n",
            "resetting env. episode 762.000000, reward total was -19.000000. running mean: -19.858137\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -19.849556\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -19.861060\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -19.852449\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -19.863925\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -19.855286\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -19.866733\n",
            "resetting env. episode 769.000000, reward total was -20.000000. running mean: -19.868065\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -19.879385\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -19.880591\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -19.891785\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -19.902867\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -19.913839\n",
            "resetting env. episode 775.000000, reward total was -18.000000. running mean: -19.894700\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -19.885753\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -19.886896\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -19.898027\n",
            "resetting env. episode 779.000000, reward total was -17.000000. running mean: -19.869046\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -19.880356\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -19.871552\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -19.882837\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -19.884009\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -19.885168\n",
            "resetting env. episode 785.000000, reward total was -17.000000. running mean: -19.856317\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -19.857754\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -19.859176\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -19.860584\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -19.871978\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -19.873259\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -19.884526\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -19.895681\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -19.906724\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -19.917657\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -19.918480\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -19.929295\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -19.940002\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -19.950602\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -19.961096\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -19.971485\n",
            "resetting env. episode 801.000000, reward total was -17.000000. running mean: -19.941771\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -19.932353\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -19.933029\n",
            "resetting env. episode 804.000000, reward total was -19.000000. running mean: -19.923699\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -19.934462\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -19.925117\n",
            "resetting env. episode 807.000000, reward total was -19.000000. running mean: -19.915866\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -19.926708\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -19.927441\n",
            "resetting env. episode 810.000000, reward total was -19.000000. running mean: -19.918166\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -19.928984\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -19.939695\n",
            "resetting env. episode 813.000000, reward total was -17.000000. running mean: -19.910298\n",
            "resetting env. episode 814.000000, reward total was -18.000000. running mean: -19.891195\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -19.892283\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -19.903360\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -19.914326\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -19.925183\n",
            "resetting env. episode 819.000000, reward total was -19.000000. running mean: -19.915931\n",
            "resetting env. episode 820.000000, reward total was -19.000000. running mean: -19.906772\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -19.917704\n",
            "resetting env. episode 822.000000, reward total was -19.000000. running mean: -19.908527\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -19.909442\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -19.900347\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -19.901344\n",
            "resetting env. episode 826.000000, reward total was -19.000000. running mean: -19.892331\n",
            "resetting env. episode 827.000000, reward total was -18.000000. running mean: -19.873407\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -19.884673\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -19.895826\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -19.886868\n",
            "resetting env. episode 831.000000, reward total was -18.000000. running mean: -19.867999\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -19.879319\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -19.870526\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -19.871821\n",
            "resetting env. episode 835.000000, reward total was -19.000000. running mean: -19.863103\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -19.864472\n",
            "resetting env. episode 837.000000, reward total was -16.000000. running mean: -19.825827\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -19.837569\n",
            "resetting env. episode 839.000000, reward total was -18.000000. running mean: -19.819193\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -19.821001\n",
            "resetting env. episode 841.000000, reward total was -19.000000. running mean: -19.812791\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -19.814663\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -19.816517\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -19.828351\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -19.840068\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -19.851667\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -19.853151\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -19.864619\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -19.875973\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -19.887213\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -19.888341\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -19.889458\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -19.890563\n",
            "resetting env. episode 854.000000, reward total was -18.000000. running mean: -19.871657\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -19.882941\n",
            "resetting env. episode 856.000000, reward total was -19.000000. running mean: -19.874111\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -19.885370\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -19.896517\n",
            "resetting env. episode 859.000000, reward total was -19.000000. running mean: -19.887551\n",
            "resetting env. episode 860.000000, reward total was -18.000000. running mean: -19.868676\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -19.879989\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -19.881189\n",
            "resetting env. episode 863.000000, reward total was -18.000000. running mean: -19.862377\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -19.863754\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -19.875116\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -19.886365\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -19.897501\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -19.908526\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -19.919441\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -19.930247\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -19.930944\n",
            "resetting env. episode 872.000000, reward total was -18.000000. running mean: -19.911635\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -19.922518\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -19.923293\n",
            "resetting env. episode 875.000000, reward total was -19.000000. running mean: -19.914060\n",
            "resetting env. episode 876.000000, reward total was -16.000000. running mean: -19.874920\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -19.876170\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -19.867409\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -19.858735\n",
            "resetting env. episode 880.000000, reward total was -18.000000. running mean: -19.840147\n",
            "resetting env. episode 881.000000, reward total was -19.000000. running mean: -19.831746\n",
            "resetting env. episode 882.000000, reward total was -18.000000. running mean: -19.813428\n",
            "resetting env. episode 883.000000, reward total was -18.000000. running mean: -19.795294\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -19.807341\n",
            "resetting env. episode 885.000000, reward total was -17.000000. running mean: -19.779268\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -19.781475\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -19.783660\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -19.795824\n",
            "resetting env. episode 889.000000, reward total was -18.000000. running mean: -19.777865\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -19.780087\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -19.792286\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -19.784363\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -19.786519\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -19.798654\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -19.810668\n",
            "resetting env. episode 896.000000, reward total was -19.000000. running mean: -19.802561\n",
            "resetting env. episode 897.000000, reward total was -18.000000. running mean: -19.784535\n",
            "resetting env. episode 898.000000, reward total was -19.000000. running mean: -19.776690\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -19.778923\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -19.791134\n",
            "resetting env. episode 901.000000, reward total was -19.000000. running mean: -19.783223\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -19.795390\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -19.787436\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -19.789562\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -19.801666\n",
            "resetting env. episode 906.000000, reward total was -20.000000. running mean: -19.803650\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -19.815613\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -19.807457\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -19.819383\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -19.831189\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -19.832877\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -19.844548\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -19.836103\n",
            "resetting env. episode 914.000000, reward total was -19.000000. running mean: -19.827742\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -19.829464\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -19.831170\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -19.832858\n",
            "resetting env. episode 918.000000, reward total was -18.000000. running mean: -19.814529\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -19.816384\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -19.828220\n",
            "resetting env. episode 921.000000, reward total was -17.000000. running mean: -19.799938\n",
            "resetting env. episode 922.000000, reward total was -19.000000. running mean: -19.791939\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -19.794019\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -19.806079\n",
            "resetting env. episode 925.000000, reward total was -19.000000. running mean: -19.798018\n",
            "resetting env. episode 926.000000, reward total was -17.000000. running mean: -19.770038\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -19.782338\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -19.784514\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -19.796669\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -19.798702\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -19.810715\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -19.802608\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -19.814582\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -19.806436\n",
            "resetting env. episode 935.000000, reward total was -19.000000. running mean: -19.798372\n",
            "resetting env. episode 936.000000, reward total was -19.000000. running mean: -19.790388\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -19.802484\n",
            "resetting env. episode 938.000000, reward total was -17.000000. running mean: -19.774460\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -19.776715\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -19.788948\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -19.801058\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -19.803048\n",
            "resetting env. episode 943.000000, reward total was -17.000000. running mean: -19.775017\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -19.777267\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -19.789494\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -19.781599\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -19.793783\n",
            "resetting env. episode 948.000000, reward total was -19.000000. running mean: -19.785846\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -19.787987\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -19.790107\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -19.802206\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -19.814184\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -19.816042\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -19.817882\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -19.829703\n",
            "resetting env. episode 956.000000, reward total was -19.000000. running mean: -19.821406\n",
            "resetting env. episode 957.000000, reward total was -19.000000. running mean: -19.813192\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -19.825060\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -19.836809\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -19.838441\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -19.840057\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -19.831656\n",
            "resetting env. episode 963.000000, reward total was -19.000000. running mean: -19.823340\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -19.835106\n",
            "resetting env. episode 965.000000, reward total was -18.000000. running mean: -19.816755\n",
            "resetting env. episode 966.000000, reward total was -19.000000. running mean: -19.808588\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -19.810502\n",
            "resetting env. episode 968.000000, reward total was -17.000000. running mean: -19.782397\n",
            "resetting env. episode 969.000000, reward total was -18.000000. running mean: -19.764573\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -19.776927\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -19.769158\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -19.761466\n",
            "resetting env. episode 973.000000, reward total was -18.000000. running mean: -19.743852\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -19.746413\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -19.748949\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -19.761460\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -19.773845\n",
            "resetting env. episode 978.000000, reward total was -19.000000. running mean: -19.766107\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -19.768445\n",
            "resetting env. episode 980.000000, reward total was -19.000000. running mean: -19.760761\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -19.763153\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -19.775522\n",
            "resetting env. episode 983.000000, reward total was -19.000000. running mean: -19.767767\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -19.770089\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -19.772388\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -19.784664\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -19.776818\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -19.779049\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -19.781259\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -19.793446\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -19.805512\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -19.807457\n",
            "resetting env. episode 993.000000, reward total was -19.000000. running mean: -19.799382\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -19.801388\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -19.803374\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -19.815341\n",
            "resetting env. episode 997.000000, reward total was -17.000000. running mean: -19.787187\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -19.779315\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -19.771522\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -19.773807\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -19.766069\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -19.768408\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -19.780724\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -19.792917\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -19.794988\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -19.797038\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -19.799068\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -19.811077\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -19.822966\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -19.834736\n",
            "resetting env. episode 1011.000000, reward total was -17.000000. running mean: -19.806389\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -19.818325\n",
            "resetting env. episode 1013.000000, reward total was -19.000000. running mean: -19.810142\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -19.822041\n",
            "resetting env. episode 1015.000000, reward total was -18.000000. running mean: -19.803820\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -19.795782\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -19.807824\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -19.799746\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -19.811748\n",
            "resetting env. episode 1020.000000, reward total was -19.000000. running mean: -19.803631\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -19.815595\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -19.807439\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -19.819364\n",
            "resetting env. episode 1024.000000, reward total was -19.000000. running mean: -19.811171\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -19.823059\n",
            "resetting env. episode 1026.000000, reward total was -19.000000. running mean: -19.814828\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -19.816680\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -19.828513\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -19.830228\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -19.831926\n",
            "resetting env. episode 1031.000000, reward total was -18.000000. running mean: -19.813607\n",
            "resetting env. episode 1032.000000, reward total was -19.000000. running mean: -19.805470\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -19.817416\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -19.829242\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -19.840949\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -19.852540\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -19.844014\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -19.845574\n",
            "resetting env. episode 1039.000000, reward total was -20.000000. running mean: -19.847118\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -19.848647\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -19.850161\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -19.851659\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -19.863143\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -19.864511\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -19.875866\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -19.867107\n",
            "resetting env. episode 1047.000000, reward total was -19.000000. running mean: -19.858436\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -19.859852\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -19.861253\n",
            "resetting env. episode 1050.000000, reward total was -19.000000. running mean: -19.852641\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -19.864114\n",
            "resetting env. episode 1052.000000, reward total was -19.000000. running mean: -19.855473\n",
            "resetting env. episode 1053.000000, reward total was -19.000000. running mean: -19.846919\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -19.858449\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -19.859865\n",
            "resetting env. episode 1056.000000, reward total was -19.000000. running mean: -19.851266\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -19.842754\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -19.844326\n",
            "resetting env. episode 1059.000000, reward total was -19.000000. running mean: -19.835883\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -19.837524\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -19.839149\n",
            "resetting env. episode 1062.000000, reward total was -17.000000. running mean: -19.810757\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -19.822650\n",
            "resetting env. episode 1064.000000, reward total was -20.000000. running mean: -19.824423\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -19.826179\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -19.837917\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -19.849538\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -19.861043\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -19.872432\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -19.873708\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -19.884971\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -19.876121\n",
            "resetting env. episode 1073.000000, reward total was -19.000000. running mean: -19.867360\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -19.868686\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -19.869999\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -19.881299\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -19.892486\n",
            "resetting env. episode 1078.000000, reward total was -16.000000. running mean: -19.853562\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -19.865026\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -19.856376\n",
            "resetting env. episode 1081.000000, reward total was -17.000000. running mean: -19.827812\n",
            "resetting env. episode 1082.000000, reward total was -16.000000. running mean: -19.789534\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -19.791638\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -19.783722\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -19.795885\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -19.807926\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -19.809847\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -19.821748\n",
            "resetting env. episode 1089.000000, reward total was -17.000000. running mean: -19.793531\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -19.785595\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -19.787740\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -19.779862\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -19.782064\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -19.774243\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.786500\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -19.788635\n",
            "resetting env. episode 1097.000000, reward total was -19.000000. running mean: -19.780749\n",
            "resetting env. episode 1098.000000, reward total was -19.000000. running mean: -19.772942\n",
            "resetting env. episode 1099.000000, reward total was -19.000000. running mean: -19.765212\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -19.767560\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -19.779884\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -19.762086\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -19.754465\n",
            "resetting env. episode 1104.000000, reward total was -18.000000. running mean: -19.736920\n",
            "resetting env. episode 1105.000000, reward total was -17.000000. running mean: -19.709551\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -19.722455\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -19.735231\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -19.737879\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -19.740500\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -19.743095\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -19.755664\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -19.758107\n",
            "resetting env. episode 1113.000000, reward total was -17.000000. running mean: -19.730526\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -19.743221\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -19.745789\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -19.758331\n",
            "resetting env. episode 1117.000000, reward total was -18.000000. running mean: -19.740747\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -19.753340\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -19.755807\n",
            "resetting env. episode 1120.000000, reward total was -19.000000. running mean: -19.748249\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -19.750766\n",
            "resetting env. episode 1122.000000, reward total was -19.000000. running mean: -19.743258\n",
            "resetting env. episode 1123.000000, reward total was -17.000000. running mean: -19.715826\n",
            "resetting env. episode 1124.000000, reward total was -17.000000. running mean: -19.688668\n",
            "resetting env. episode 1125.000000, reward total was -19.000000. running mean: -19.681781\n",
            "resetting env. episode 1126.000000, reward total was -19.000000. running mean: -19.674963\n",
            "resetting env. episode 1127.000000, reward total was -20.000000. running mean: -19.678213\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -19.671431\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -19.674717\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -19.677970\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -19.691190\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -19.704278\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -19.717235\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -19.730063\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -19.742762\n",
            "resetting env. episode 1136.000000, reward total was -18.000000. running mean: -19.725335\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -19.738081\n",
            "resetting env. episode 1138.000000, reward total was -18.000000. running mean: -19.720701\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -19.733494\n",
            "resetting env. episode 1140.000000, reward total was -17.000000. running mean: -19.706159\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -19.699097\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -19.702106\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -19.705085\n",
            "resetting env. episode 1144.000000, reward total was -19.000000. running mean: -19.698034\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -19.711054\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -19.723943\n",
            "resetting env. episode 1147.000000, reward total was -19.000000. running mean: -19.716704\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -19.729537\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -19.742242\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -19.754819\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -19.767271\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -19.779598\n",
            "resetting env. episode 1153.000000, reward total was -18.000000. running mean: -19.761802\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -19.774184\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -19.776442\n",
            "resetting env. episode 1156.000000, reward total was -19.000000. running mean: -19.768678\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -19.780991\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -19.793181\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -19.795249\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -19.807297\n",
            "resetting env. episode 1161.000000, reward total was -17.000000. running mean: -19.779224\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -19.791432\n",
            "resetting env. episode 1163.000000, reward total was -18.000000. running mean: -19.773517\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -19.785782\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -19.787924\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -19.790045\n",
            "resetting env. episode 1167.000000, reward total was -20.000000. running mean: -19.792145\n",
            "resetting env. episode 1168.000000, reward total was -16.000000. running mean: -19.754223\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -19.756681\n",
            "resetting env. episode 1170.000000, reward total was -19.000000. running mean: -19.749114\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -19.741623\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -19.744207\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.756765\n",
            "resetting env. episode 1174.000000, reward total was -19.000000. running mean: -19.749197\n",
            "resetting env. episode 1175.000000, reward total was -18.000000. running mean: -19.731705\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -19.744388\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -19.756944\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -19.769375\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -19.781681\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -19.793864\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -19.795926\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -19.797966\n",
            "resetting env. episode 1183.000000, reward total was -19.000000. running mean: -19.789987\n",
            "resetting env. episode 1184.000000, reward total was -20.000000. running mean: -19.792087\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -19.794166\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -19.806224\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -19.798162\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -19.800180\n",
            "resetting env. episode 1189.000000, reward total was -19.000000. running mean: -19.792179\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -19.804257\n",
            "resetting env. episode 1191.000000, reward total was -18.000000. running mean: -19.786214\n",
            "resetting env. episode 1192.000000, reward total was -19.000000. running mean: -19.778352\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -19.780569\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -19.782763\n",
            "resetting env. episode 1195.000000, reward total was -19.000000. running mean: -19.774935\n",
            "resetting env. episode 1196.000000, reward total was -19.000000. running mean: -19.767186\n",
            "resetting env. episode 1197.000000, reward total was -17.000000. running mean: -19.739514\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -19.742119\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -19.754698\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -19.757151\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -19.769579\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -19.781883\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -19.774065\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -19.786324\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -19.788461\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -19.790576\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -19.782670\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -19.794844\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -19.786895\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -19.789026\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -19.781136\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -19.793325\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -19.795391\n",
            "resetting env. episode 1214.000000, reward total was -18.000000. running mean: -19.777437\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -19.779663\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -19.771866\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -19.774148\n",
            "resetting env. episode 1218.000000, reward total was -19.000000. running mean: -19.766406\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -19.758742\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -19.771155\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -19.773443\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -19.765709\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -19.778052\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -19.780271\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -19.782469\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -19.794644\n",
            "resetting env. episode 1227.000000, reward total was -19.000000. running mean: -19.786697\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -19.798830\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -19.810842\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -19.812734\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -19.824606\n",
            "resetting env. episode 1232.000000, reward total was -18.000000. running mean: -19.806360\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.818297\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -19.830114\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -19.821813\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -19.823594\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -19.835359\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -19.847005\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -19.848535\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -19.860050\n",
            "resetting env. episode 1241.000000, reward total was -19.000000. running mean: -19.851449\n",
            "resetting env. episode 1242.000000, reward total was -17.000000. running mean: -19.822935\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -19.824705\n",
            "resetting env. episode 1244.000000, reward total was -18.000000. running mean: -19.806458\n",
            "resetting env. episode 1245.000000, reward total was -18.000000. running mean: -19.788394\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -19.790510\n",
            "resetting env. episode 1247.000000, reward total was -18.000000. running mean: -19.772605\n",
            "resetting env. episode 1248.000000, reward total was -17.000000. running mean: -19.744879\n",
            "resetting env. episode 1249.000000, reward total was -17.000000. running mean: -19.717430\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -19.710255\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -19.723153\n",
            "resetting env. episode 1252.000000, reward total was -18.000000. running mean: -19.705921\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -19.708862\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -19.721774\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -19.714556\n",
            "resetting env. episode 1256.000000, reward total was -19.000000. running mean: -19.707410\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -19.720336\n",
            "resetting env. episode 1258.000000, reward total was -18.000000. running mean: -19.703133\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -19.706101\n",
            "resetting env. episode 1260.000000, reward total was -19.000000. running mean: -19.699040\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -19.712050\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -19.724930\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -19.727680\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -19.740403\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -19.742999\n",
            "resetting env. episode 1266.000000, reward total was -19.000000. running mean: -19.735569\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -19.748214\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -19.750732\n",
            "resetting env. episode 1269.000000, reward total was -18.000000. running mean: -19.733224\n",
            "resetting env. episode 1270.000000, reward total was -19.000000. running mean: -19.725892\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -19.728633\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -19.721347\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -19.724133\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -19.736892\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -19.729523\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -19.742228\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -19.754806\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -19.757257\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -19.769685\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -19.771988\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -19.784268\n",
            "resetting env. episode 1282.000000, reward total was -17.000000. running mean: -19.756425\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -19.758861\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -19.761273\n",
            "resetting env. episode 1285.000000, reward total was -19.000000. running mean: -19.753660\n",
            "resetting env. episode 1286.000000, reward total was -19.000000. running mean: -19.746123\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -19.758662\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -19.751075\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -19.753565\n",
            "resetting env. episode 1290.000000, reward total was -19.000000. running mean: -19.746029\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -19.748569\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -19.751083\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -19.763572\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -19.765937\n",
            "resetting env. episode 1295.000000, reward total was -19.000000. running mean: -19.758277\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -19.760694\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -19.773087\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -19.785357\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -19.777503\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -19.789728\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -19.781831\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -19.794012\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -19.786072\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -19.778212\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.780429\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -19.782625\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -19.784799\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -19.786951\n",
            "resetting env. episode 1309.000000, reward total was -17.000000. running mean: -19.759081\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -19.771491\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -19.783776\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -19.795938\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -19.787979\n",
            "resetting env. episode 1314.000000, reward total was -18.000000. running mean: -19.770099\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -19.772398\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -19.784674\n",
            "resetting env. episode 1317.000000, reward total was -18.000000. running mean: -19.766827\n",
            "resetting env. episode 1318.000000, reward total was -16.000000. running mean: -19.729159\n",
            "resetting env. episode 1319.000000, reward total was -19.000000. running mean: -19.721867\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -19.714648\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -19.727502\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -19.740227\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -19.752825\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -19.765296\n",
            "resetting env. episode 1325.000000, reward total was -19.000000. running mean: -19.757644\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -19.760067\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -19.762466\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -19.774842\n",
            "resetting env. episode 1329.000000, reward total was -19.000000. running mean: -19.767093\n",
            "resetting env. episode 1330.000000, reward total was -18.000000. running mean: -19.749422\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -19.751928\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -19.744409\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -19.746965\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -19.749495\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -19.762000\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -19.774380\n",
            "resetting env. episode 1337.000000, reward total was -17.000000. running mean: -19.746636\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -19.749170\n",
            "resetting env. episode 1339.000000, reward total was -18.000000. running mean: -19.731678\n",
            "resetting env. episode 1340.000000, reward total was -18.000000. running mean: -19.714362\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -19.717218\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -19.730046\n",
            "resetting env. episode 1343.000000, reward total was -19.000000. running mean: -19.722745\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -19.715518\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -19.708363\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -19.711279\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -19.714166\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -19.707025\n",
            "resetting env. episode 1349.000000, reward total was -19.000000. running mean: -19.699954\n",
            "resetting env. episode 1350.000000, reward total was -18.000000. running mean: -19.682955\n",
            "resetting env. episode 1351.000000, reward total was -18.000000. running mean: -19.666125\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -19.659464\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -19.662869\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -19.676241\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -19.689478\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -19.682583\n",
            "resetting env. episode 1357.000000, reward total was -18.000000. running mean: -19.665758\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -19.659100\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -19.662509\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -19.675884\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -19.669125\n",
            "resetting env. episode 1362.000000, reward total was -17.000000. running mean: -19.642434\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -19.656010\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -19.669449\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -19.682755\n",
            "resetting env. episode 1366.000000, reward total was -18.000000. running mean: -19.665927\n",
            "resetting env. episode 1367.000000, reward total was -17.000000. running mean: -19.639268\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -19.632875\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -19.636547\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -19.640181\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -19.653779\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -19.657242\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -19.670669\n",
            "resetting env. episode 1374.000000, reward total was -19.000000. running mean: -19.663963\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -19.677323\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -19.690550\n",
            "resetting env. episode 1377.000000, reward total was -19.000000. running mean: -19.683644\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -19.686808\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -19.679940\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -19.693140\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -19.686209\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -19.679347\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -19.672553\n",
            "resetting env. episode 1384.000000, reward total was -19.000000. running mean: -19.665828\n",
            "resetting env. episode 1385.000000, reward total was -19.000000. running mean: -19.659169\n",
            "resetting env. episode 1386.000000, reward total was -19.000000. running mean: -19.652578\n",
            "resetting env. episode 1387.000000, reward total was -19.000000. running mean: -19.646052\n",
            "resetting env. episode 1388.000000, reward total was -16.000000. running mean: -19.609591\n",
            "resetting env. episode 1389.000000, reward total was -18.000000. running mean: -19.593496\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -19.587561\n",
            "resetting env. episode 1391.000000, reward total was -17.000000. running mean: -19.561685\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -19.566068\n",
            "resetting env. episode 1393.000000, reward total was -19.000000. running mean: -19.560407\n",
            "resetting env. episode 1394.000000, reward total was -15.000000. running mean: -19.514803\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -19.519655\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -19.534459\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -19.539114\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -19.553723\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -19.568186\n",
            "resetting env. episode 1400.000000, reward total was -18.000000. running mean: -19.552504\n",
            "resetting env. episode 1401.000000, reward total was -19.000000. running mean: -19.546979\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -19.551509\n",
            "resetting env. episode 1403.000000, reward total was -18.000000. running mean: -19.535994\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -19.540634\n",
            "resetting env. episode 1405.000000, reward total was -17.000000. running mean: -19.515228\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -19.530076\n",
            "resetting env. episode 1407.000000, reward total was -19.000000. running mean: -19.524775\n",
            "resetting env. episode 1408.000000, reward total was -19.000000. running mean: -19.519527\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -19.524332\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -19.529088\n",
            "resetting env. episode 1411.000000, reward total was -18.000000. running mean: -19.513798\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -19.528660\n",
            "resetting env. episode 1413.000000, reward total was -19.000000. running mean: -19.523373\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -19.538139\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.542758\n",
            "resetting env. episode 1416.000000, reward total was -18.000000. running mean: -19.527330\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.532057\n",
            "resetting env. episode 1418.000000, reward total was -19.000000. running mean: -19.526736\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -19.521469\n",
            "resetting env. episode 1420.000000, reward total was -19.000000. running mean: -19.516254\n",
            "resetting env. episode 1421.000000, reward total was -19.000000. running mean: -19.511092\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -19.505981\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -19.490921\n",
            "resetting env. episode 1424.000000, reward total was -17.000000. running mean: -19.466012\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -19.461352\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -19.466738\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -19.472071\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -19.487350\n",
            "resetting env. episode 1429.000000, reward total was -19.000000. running mean: -19.482477\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -19.477652\n",
            "resetting env. episode 1431.000000, reward total was -19.000000. running mean: -19.472875\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -19.468147\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -19.483465\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -19.498630\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -19.513644\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -19.518508\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -19.533323\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -19.547989\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -19.542510\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -19.547084\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -19.551614\n",
            "resetting env. episode 1442.000000, reward total was -19.000000. running mean: -19.546097\n",
            "resetting env. episode 1443.000000, reward total was -19.000000. running mean: -19.540636\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -19.535230\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -19.539878\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -19.544479\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -19.539034\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -19.533644\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -19.548307\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -19.542824\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -19.537396\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -19.552022\n",
            "resetting env. episode 1453.000000, reward total was -19.000000. running mean: -19.546502\n",
            "resetting env. episode 1454.000000, reward total was -16.000000. running mean: -19.511037\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -19.525927\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -19.540667\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -19.555261\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -19.559708\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -19.564111\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -19.578470\n",
            "resetting env. episode 1461.000000, reward total was -17.000000. running mean: -19.552685\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -19.557158\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -19.551587\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -19.566071\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -19.560410\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -19.574806\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -19.579058\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -19.593267\n",
            "resetting env. episode 1469.000000, reward total was -18.000000. running mean: -19.577335\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -19.571561\n",
            "resetting env. episode 1471.000000, reward total was -18.000000. running mean: -19.555846\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -19.560287\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -19.554684\n",
            "resetting env. episode 1474.000000, reward total was -17.000000. running mean: -19.529138\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -19.533846\n",
            "resetting env. episode 1476.000000, reward total was -16.000000. running mean: -19.498508\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -19.513523\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.528387\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -19.543104\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -19.547673\n",
            "resetting env. episode 1481.000000, reward total was -19.000000. running mean: -19.542196\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -19.546774\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -19.541306\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -19.555893\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -19.560334\n",
            "resetting env. episode 1486.000000, reward total was -19.000000. running mean: -19.554731\n",
            "resetting env. episode 1487.000000, reward total was -19.000000. running mean: -19.549183\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -19.563692\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -19.578055\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -19.582274\n",
            "resetting env. episode 1491.000000, reward total was -19.000000. running mean: -19.576451\n",
            "resetting env. episode 1492.000000, reward total was -17.000000. running mean: -19.550687\n",
            "resetting env. episode 1493.000000, reward total was -18.000000. running mean: -19.535180\n",
            "resetting env. episode 1494.000000, reward total was -19.000000. running mean: -19.529828\n",
            "resetting env. episode 1495.000000, reward total was -18.000000. running mean: -19.514530\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -19.519385\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -19.514191\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -19.529049\n",
            "resetting env. episode 1499.000000, reward total was -18.000000. running mean: -19.513758\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -19.518621\n",
            "CPU times: user 3h 38min 55s, sys: 43min 41s, total: 4h 22min 37s\n",
            "Wall time: 2h 15min 13s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "w2NblmwDsL3y",
        "outputId": "7dafa501-d6a9-4250-d752-2911c9f2752a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=800_le_4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}