{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "a685fac9-e480-4d6e-c6a3-559d49798037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=70792bdaaca0458ed1e70bed3667823b5dd4d507b882886dc06be92a993620a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "021b08d9-9b18-4a2b-e80f-df8103643d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "5a938f43-fdf9-49a2-da3b-4fb409ec3a3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "126cd705-9d2b-4f2d-b79e-f770665d4b08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "f54b39b0-a8a7-4224-f618-3666b6eebb46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -19.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 600 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "54766e28-bd01-4f5f-b30b-a0d7bc525d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049601\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -19.039105\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.058714\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -19.058127\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.077546\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.096770\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.105802\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.124744\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.143497\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.162062\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.180441\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.198637\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.216651\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.234484\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.252139\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.259618\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.267022\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.284351\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.301508\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.318493\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.335308\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.351955\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.358435\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.374851\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.381102\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.397291\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -19.393318\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.409385\n",
            "resetting env. episode 32.000000, reward total was -17.000000. running mean: -19.385291\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.401439\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.417424\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.433250\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.448917\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -19.454428\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.469884\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.475185\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.490433\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.505529\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.520474\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.525269\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -19.520016\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.534816\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.549468\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.563973\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -19.568333\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.582650\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -19.586824\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.590955\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -19.585046\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.599195\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.613203\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.627071\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.640801\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.654393\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -19.647849\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.661370\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.674757\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.688009\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.701129\n",
            "resetting env. episode 63.000000, reward total was -18.000000. running mean: -19.684118\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -19.687276\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.700404\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -19.693400\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.696466\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.699501\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.712506\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -19.715381\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.728227\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.740945\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -19.733535\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -19.726200\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.738938\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.751549\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.764033\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.776393\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.778629\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.790843\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.802934\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.814905\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -19.806756\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.818688\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.830501\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.832196\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.843874\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.855436\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.866881\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.868212\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.879530\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.890735\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.891828\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -19.882909\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.894080\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.905140\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.906088\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.907027\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -19.897957\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -19.898977\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.899988\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.910988\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.921878\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.932659\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.943332\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.953899\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.954360\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.964817\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.975168\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.975417\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.985663\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.985806\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.995948\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.005988\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.015929\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.015769\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.015612\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.015455\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.025301\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.025048\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.034797\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.044449\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.044005\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.043565\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.053129\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.062598\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.071972\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.081252\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.090440\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.099535\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.098540\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.097555\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.106579\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.105513\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.104458\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -20.093414\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.092479\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.101555\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.110539\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.119434\n",
            "resetting env. episode 141.000000, reward total was -18.000000. running mean: -20.098239\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.107257\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.116184\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.115022\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.123872\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.112634\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.111507\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.120392\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.119188\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.127996\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.126716\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.135449\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.134095\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.142754\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.151326\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.159813\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.168215\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.166533\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.154867\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.163319\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.141686\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.150269\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.158766\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.167178\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.165507\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.173851\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.182113\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.170292\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.178589\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.186803\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.184935\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.193086\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.201155\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.199143\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.207152\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.205080\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.213029\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.220899\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.228690\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.216403\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.224239\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.221997\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.219777\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.217579\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.225403\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.233149\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.230818\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.228510\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.216225\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.224062\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.231822\n",
            "resetting env. episode 192.000000, reward total was -18.000000. running mean: -20.209503\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.197408\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.205434\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.203380\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.191346\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.199433\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.187438\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.185564\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.183708\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.171871\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.180153\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.188351\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.196468\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.184503\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.192658\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.200731\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.208724\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.216637\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.224470\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.232226\n",
            "resetting env. episode 212.000000, reward total was -18.000000. running mean: -20.209903\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.217804\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.215626\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.223470\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.231235\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.228923\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.236634\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.244267\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.241825\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.249406\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.256912\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.264343\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.261700\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.259083\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.266492\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.273827\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.271089\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.268378\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.265694\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.273037\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.260307\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.267704\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.255027\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.262476\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.269852\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.267153\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -20.254482\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.261937\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.259317\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.266724\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.254057\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.251517\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.259001\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.246411\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.253947\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.251408\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.248894\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.256405\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.243841\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.251402\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.248888\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.236399\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.234035\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.231695\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.239378\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.246984\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.254514\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.251969\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.239450\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.237055\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.234685\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.222338\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.220114\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.227913\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.225634\n",
            "resetting env. episode 267.000000, reward total was -18.000000. running mean: -20.203378\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.211344\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.219231\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.217038\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.224868\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.212619\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.210493\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.208388\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.216304\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.224141\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.221900\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.229681\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.237384\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.235010\n",
            "resetting env. episode 281.000000, reward total was -16.000000. running mean: -20.192660\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.200733\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.208726\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.216639\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.214472\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.222328\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.230104\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.227803\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.215525\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.223370\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.231136\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.228825\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.226537\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.224271\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.232029\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.239708\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.237311\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.234938\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.242589\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.250163\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.257661\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.255085\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.262534\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.269908\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.267209\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.264537\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.271892\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.269173\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.276481\n",
            "resetting env. episode 310.000000, reward total was -18.000000. running mean: -20.253716\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.261179\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.268567\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.275882\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.283123\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.280292\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.287489\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.294614\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.301668\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.308651\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.315565\n",
            "resetting env. episode 321.000000, reward total was -18.000000. running mean: -20.292409\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.299485\n",
            "resetting env. episode 323.000000, reward total was -17.000000. running mean: -20.266490\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.273825\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.281087\n",
            "resetting env. episode 326.000000, reward total was -18.000000. running mean: -20.258276\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.255693\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.253136\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.260605\n",
            "resetting env. episode 330.000000, reward total was -18.000000. running mean: -20.237999\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -20.225619\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.233363\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.241029\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.248619\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.256133\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.263571\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.270936\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.278226\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.275444\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.282690\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.289863\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.286964\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.284094\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.281253\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.288441\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.295556\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.292601\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.299675\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.306678\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.303611\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.290575\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.297670\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.304693\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.311646\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.308529\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.305444\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.312390\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.309266\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.306173\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.313111\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.309980\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.296880\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.293912\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.300973\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.287963\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.295083\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.292132\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.299211\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.286219\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.293357\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.300423\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.297419\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.304445\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.311400\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.298286\n",
            "resetting env. episode 376.000000, reward total was -18.000000. running mean: -20.275303\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.282550\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.279725\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.286928\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.284058\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.271218\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.278506\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.285721\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.292863\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.299935\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.306935\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.313866\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.310727\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.307620\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.294544\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.301598\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.308582\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.315497\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.322342\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.319118\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.305927\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.312868\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.319739\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.326542\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.323276\n",
            "resetting env. episode 401.000000, reward total was -18.000000. running mean: -20.300044\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.307043\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.303973\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.310933\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.317824\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.324645\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.331399\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.318085\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.324904\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.321655\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.328439\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.325154\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.321903\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.328684\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.335397\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.332043\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.338722\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.345335\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.351882\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.348363\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.354879\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.361331\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.367717\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.374040\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.370300\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.376597\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.362831\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.369202\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.375510\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.371755\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.378038\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.384257\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.390415\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.396511\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.372545\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.378820\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.375032\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.371282\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.357569\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.363993\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.350353\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.356850\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.343281\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.329848\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.336550\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.333184\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.339852\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.326454\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.323189\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.319957\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.306758\n",
            "resetting env. episode 452.000000, reward total was -18.000000. running mean: -20.283690\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.280853\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.288045\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.295164\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.282213\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.289391\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.296497\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.303532\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.310496\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.317392\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.304218\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.311175\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.318064\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.324883\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.321634\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.328418\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.325134\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.331882\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.328564\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.335278\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.331925\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.338606\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.345220\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.351768\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.358250\n",
            "resetting env. episode 477.000000, reward total was -18.000000. running mean: -20.334667\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.331321\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.338008\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.334627\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.341281\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.347868\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.344390\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.350946\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.357436\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.363862\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.360223\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.366621\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.372955\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.379225\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.365433\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.371779\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.378061\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.384280\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.370438\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.376733\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.382966\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.389136\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.375245\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.361492\n",
            "CPU times: user 46min 51s, sys: 11min 41s, total: 58min 33s\n",
            "Wall time: 30min 12s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "1cad65b6-83f3-4a37-f60f-2a33056e4886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.039800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.059402\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -19.048808\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.058320\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.077737\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.086959\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.106090\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -19.105029\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -19.103979\n",
            "resetting env. episode 12.000000, reward total was -16.000000. running mean: -19.072939\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.082209\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.101387\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -19.110373\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -19.109270\n",
            "resetting env. episode 17.000000, reward total was -18.000000. running mean: -19.098177\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.107195\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.126123\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.134862\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.153513\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.171978\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.190258\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.208356\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -19.216272\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.234110\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.251769\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -19.249251\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.266758\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.284091\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.291250\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.298337\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.315354\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.332200\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.348878\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.365390\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -19.361736\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.378118\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.394337\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.400394\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.416390\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.432226\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -19.427904\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.443625\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -19.449188\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.454697\n",
            "resetting env. episode 47.000000, reward total was -17.000000. running mean: -19.430150\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.445848\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -19.431390\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.447076\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.462605\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.477979\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.483199\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.488367\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -19.483483\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.498649\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.503662\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -19.498626\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.513639\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -19.508503\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.513418\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -19.508284\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.513201\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.528069\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.542788\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.557360\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.561787\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.576169\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.580407\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.594603\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.608657\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.622570\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.636345\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.649981\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.653481\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -19.646947\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -19.650477\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -19.643972\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.657533\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.660957\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.674348\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.687604\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.700728\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.713721\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.726584\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.729318\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -19.722025\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.734805\n",
            "resetting env. episode 89.000000, reward total was -18.000000. running mean: -19.717456\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.720282\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -19.713079\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.725948\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.738689\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -19.731302\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.743989\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -19.736549\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.739184\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.751792\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -19.754274\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -19.746731\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.759264\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.771671\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.773954\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.786215\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.798353\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.810369\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.812265\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.824143\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.825901\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.827642\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.839366\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.840972\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -19.832563\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.834237\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.845895\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.857436\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.868861\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -19.870173\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.871471\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.882756\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.893929\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -19.904989\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.915939\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -19.906780\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.917712\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.928535\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -19.919250\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -19.910057\n",
            "resetting env. episode 129.000000, reward total was -18.000000. running mean: -19.890957\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.902047\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.913027\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.923896\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.924657\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.935411\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.946057\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.956596\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -19.947030\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -19.937560\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -19.938184\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.948803\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -19.949314\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -19.939821\n",
            "resetting env. episode 143.000000, reward total was -18.000000. running mean: -19.920423\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.931219\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -19.921907\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.932688\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -19.933361\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -19.944027\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -19.934587\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.945241\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -19.955789\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -19.956231\n",
            "resetting env. episode 153.000000, reward total was -17.000000. running mean: -19.926668\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.927402\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -19.938128\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -19.938746\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -19.949359\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.959865\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -19.960267\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.970664\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -19.980957\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -19.991148\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -19.991236\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -19.991324\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -19.991411\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.001497\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.001482\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -19.991467\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.001552\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.001537\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.001521\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.011506\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.021391\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.031177\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.020865\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.030657\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.030350\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.030047\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.039746\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.049349\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.058855\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.068267\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.057584\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.067008\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.076338\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.085575\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.094719\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.103772\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.092734\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.101807\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.110789\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.109681\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.108584\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.097498\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.106523\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.105458\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.114403\n",
            "resetting env. episode 198.000000, reward total was -18.000000. running mean: -20.093259\n",
            "resetting env. episode 199.000000, reward total was -18.000000. running mean: -20.072327\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.071603\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.080887\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.090079\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.089178\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.098286\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.107303\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.096230\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.105268\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.094215\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.103273\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.112240\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.111118\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.120007\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.118807\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.117618\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.126442\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.115178\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.124026\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.132786\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.141458\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.130043\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.128743\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.127456\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.126181\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.134919\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.133570\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.122234\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.131012\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.139702\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.148305\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.146822\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.155354\n",
            "resetting env. episode 232.000000, reward total was -18.000000. running mean: -20.133800\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.132462\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.141137\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.139726\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.148329\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.146845\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.155377\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.163823\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.172185\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.170463\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.158759\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.167171\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.165499\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.153844\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.142306\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.140883\n",
            "resetting env. episode 248.000000, reward total was -18.000000. running mean: -20.119474\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.128279\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.136996\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.145626\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.154170\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.162628\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.161002\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.159392\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.167798\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.166120\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.174459\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.182714\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.180887\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.189078\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.187188\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.185316\n",
            "resetting env. episode 264.000000, reward total was -18.000000. running mean: -20.163463\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.171828\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.180110\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.168309\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.166626\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.164959\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.173310\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.161577\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.159961\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.168361\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.156678\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.165111\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.173460\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.161725\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.160108\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.148507\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.137022\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.145651\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.154195\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.152653\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.161126\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.169515\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.177820\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.176042\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.184281\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.192439\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.190514\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.178609\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.186823\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.194955\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.203005\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.210975\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.198865\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.206877\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.204808\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.202760\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.190732\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.198825\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.196837\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.184868\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.183020\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.181190\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.189378\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.187484\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.185609\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.173753\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.172015\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.180295\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.168492\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.166807\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.175139\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.183388\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.191554\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.189638\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.187742\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.195865\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.183906\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.192067\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.190146\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.198245\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.186262\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.184400\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.172556\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.170830\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.169122\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.167431\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.165756\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.174099\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.182358\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.180534\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.188729\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.176842\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.165073\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.163422\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.161788\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.160170\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.168569\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.176883\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.185114\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.193263\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.191330\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.199417\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.197423\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.185449\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.193594\n",
            "resetting env. episode 349.000000, reward total was -18.000000. running mean: -20.171658\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.169942\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.178242\n",
            "resetting env. episode 352.000000, reward total was -18.000000. running mean: -20.156460\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.154895\n",
            "resetting env. episode 354.000000, reward total was -18.000000. running mean: -20.133346\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.142013\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.130593\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.119287\n",
            "resetting env. episode 358.000000, reward total was -18.000000. running mean: -20.098094\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.097113\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.096142\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.105180\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.104129\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.113087\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.101956\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.110937\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.109827\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.108729\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.097642\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.096666\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.105699\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.114642\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.103495\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.112460\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.101336\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.110323\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.109219\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.108127\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.117046\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.105875\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.114817\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.123668\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.132432\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.121107\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.129896\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.138597\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.147211\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.145739\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.154282\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.162739\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.161112\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.169501\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.167806\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.176128\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.164366\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.172723\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.180995\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.189185\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.197294\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.205321\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.213267\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.221135\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.228923\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.236634\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.234268\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.241925\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.229506\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.237211\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.244839\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.232390\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.240066\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -20.217666\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.215489\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.213334\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.211201\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.209089\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.196998\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.205028\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.192978\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.201048\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.209037\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.206947\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.194878\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.202929\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.210900\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.218791\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.226603\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.234337\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.241993\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.249573\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.257078\n",
            "resetting env. episode 431.000000, reward total was -18.000000. running mean: -20.234507\n",
            "resetting env. episode 432.000000, reward total was -18.000000. running mean: -20.212162\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.220040\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.217840\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.195661\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.193705\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.191768\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.189850\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.187952\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.186072\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.184211\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.192369\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.190445\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.198541\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.196556\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.204590\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.212544\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.210419\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.218315\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.226131\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.213870\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.221731\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.219514\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.227319\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.225046\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.232795\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.230467\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.228163\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.225881\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.213622\n",
            "resetting env. episode 461.000000, reward total was -18.000000. running mean: -20.191486\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.199571\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.197575\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.195600\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.183644\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.181807\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.189989\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.198089\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.196108\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.204147\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.192106\n",
            "resetting env. episode 472.000000, reward total was -17.000000. running mean: -20.160185\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.168583\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.176897\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.185128\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.183277\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.191444\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -20.169530\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.177834\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.176056\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.174295\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.172552\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.170827\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.179119\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.187327\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.175454\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.183700\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.181863\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.180044\n",
            "resetting env. episode 490.000000, reward total was -18.000000. running mean: -20.158244\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.166661\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.174995\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.183245\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.181412\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.179598\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.167802\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.166124\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.154463\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.162918\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.161289\n",
            "CPU times: user 48min 17s, sys: 11min 59s, total: 1h 16s\n",
            "Wall time: 31min 12s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "dfeab853-9c34-4dfd-f120-1a186b4f8765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "8ea63cc7-8cb6-48ca-f0b6-011275df0235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980299\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980496\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980691\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980884\n",
            "resetting env. episode 8.000000, reward total was -18.000000. running mean: -20.951075\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.951565\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.932049\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.932728\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.933401\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.934067\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.934726\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.915379\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.906225\n",
            "resetting env. episode 17.000000, reward total was -16.000000. running mean: -20.857163\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.858592\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.850006\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.841506\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.843090\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.844660\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.846213\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.847751\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.849273\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.850781\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.852273\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.853750\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.855213\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.836660\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.828294\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.810011\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.801911\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.803892\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.785853\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.787994\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -20.770114\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.772413\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.764689\n",
            "resetting env. episode 40.000000, reward total was -18.000000. running mean: -20.737042\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.729672\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.732375\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.725051\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.707801\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.710723\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.703615\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.706579\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.699514\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.692518\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.695593\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.688637\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.691751\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.684833\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.677985\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.671205\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.674493\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.657748\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.641171\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.634759\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.638411\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.632027\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.635707\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.619350\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.603157\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.587125\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.591254\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.595341\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.599388\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.593394\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.597460\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.601485\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.595470\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.599516\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.583521\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.567685\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.562009\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.556388\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.550825\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.545316\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.549863\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.554365\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.558821\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.543233\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.547800\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.552322\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.556799\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.541231\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.545819\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.540361\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.524957\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.529707\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.534410\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.539066\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.533676\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.528339\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.513055\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.497925\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.502946\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.497916\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.502937\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.487908\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.483029\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.478198\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.473416\n",
            "resetting env. episode 105.000000, reward total was -18.000000. running mean: -20.448682\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.444195\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.449753\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.445256\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.440803\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.436395\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.432031\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.427711\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.423434\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.419200\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.425008\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.430757\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -20.416450\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.402285\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.408263\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.404180\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.400138\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.406137\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -20.382075\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.378255\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.384472\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.380627\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.366821\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.373153\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.369421\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.355727\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.362170\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.358548\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.354963\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.351413\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.357899\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.364320\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -20.350677\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.357170\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.363598\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.369962\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.356263\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.362700\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.349073\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.355582\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.362026\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.368406\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.374722\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.370975\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.377265\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.383493\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.389658\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.385761\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.391903\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.397984\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.404005\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.409965\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.405865\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.401806\n",
            "resetting env. episode 159.000000, reward total was -18.000000. running mean: -20.377788\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.384010\n",
            "resetting env. episode 161.000000, reward total was -16.000000. running mean: -20.340170\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.336768\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.333401\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.320067\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.316866\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.323697\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.310460\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.317356\n",
            "resetting env. episode 169.000000, reward total was -16.000000. running mean: -20.274182\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.271440\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.278726\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.285939\n",
            "resetting env. episode 173.000000, reward total was -18.000000. running mean: -20.263079\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.270449\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.277744\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.274967\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.282217\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.289395\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.276501\n",
            "resetting env. episode 180.000000, reward total was -18.000000. running mean: -20.253736\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.251199\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.248687\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.246200\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.243738\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.241300\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.228887\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.236598\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.234232\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.221890\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.229671\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.227375\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.225101\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.232850\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.240521\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.238116\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.245735\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.243278\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.250845\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.238336\n",
            "resetting env. episode 200.000000, reward total was -18.000000. running mean: -20.215953\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.223793\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.231556\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.239240\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.246848\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.234379\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.232035\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.219715\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.207518\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.205443\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.213388\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.221254\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.219042\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.216851\n",
            "resetting env. episode 214.000000, reward total was -18.000000. running mean: -20.194683\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.202736\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.210709\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.218602\n",
            "resetting env. episode 218.000000, reward total was -18.000000. running mean: -20.196416\n",
            "resetting env. episode 219.000000, reward total was -18.000000. running mean: -20.174451\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.162707\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.161080\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.159469\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.157874\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.156296\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.154733\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.163185\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.171553\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.169838\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.178140\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.186358\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.194495\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.202550\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.210524\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.218419\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.206235\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.214172\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.222031\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.219810\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.227612\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.215336\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.213183\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.211051\n",
            "resetting env. episode 243.000000, reward total was -16.000000. running mean: -20.168940\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.177251\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.185478\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.173624\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.181887\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.180069\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.178268\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.166485\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.174820\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.173072\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.161341\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.169728\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.178031\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.176250\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.184488\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.192643\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.190717\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.198809\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.206821\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.204753\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.202706\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.190679\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.198772\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.196784\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.194816\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.202868\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.200839\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.208831\n",
            "resetting env. episode 271.000000, reward total was -18.000000. running mean: -20.186743\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.184875\n",
            "resetting env. episode 273.000000, reward total was -18.000000. running mean: -20.163026\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.151396\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.149882\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.158383\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.156800\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.165232\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.173579\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.181843\n",
            "resetting env. episode 281.000000, reward total was -18.000000. running mean: -20.160025\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.158425\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.166841\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.175172\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.183420\n",
            "resetting env. episode 286.000000, reward total was -18.000000. running mean: -20.161586\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.149970\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.158471\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.166886\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.175217\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.183465\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.191630\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.179714\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.187917\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.196038\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.194077\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.202137\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.200115\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.198114\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.186133\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.194272\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.202329\n",
            "resetting env. episode 303.000000, reward total was -18.000000. running mean: -20.180306\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.178502\n",
            "resetting env. episode 305.000000, reward total was -18.000000. running mean: -20.156717\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.165150\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.163499\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.161864\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.150245\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.138743\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.147355\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.145882\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.144423\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.142979\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.141549\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.140133\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.148732\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.137245\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.135872\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.114514\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.123368\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.132135\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.120813\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.129605\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.138309\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.136926\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.145557\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.144101\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.152660\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.161134\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.169522\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.177827\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.186049\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.194188\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.202246\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.200224\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.198222\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.206240\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.194177\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.202235\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.210213\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.198111\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.196130\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.204168\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.202127\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.210106\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.208004\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.215924\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.223765\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.221528\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.219312\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.227119\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.224848\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.232599\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.230273\n",
            "resetting env. episode 356.000000, reward total was -18.000000. running mean: -20.207971\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.205891\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.203832\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.211794\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.219676\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.217479\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.215304\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.213151\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.191020\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.189110\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.187218\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.185346\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.193493\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.191558\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.189642\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.177746\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.175968\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.184209\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.172367\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.170643\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.168937\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.157247\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.155675\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.164118\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.162477\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.170852\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.179144\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.187352\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.185479\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.173624\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.171888\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.170169\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.158467\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.156882\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.145313\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.143860\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.142422\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.150998\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.139488\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.128093\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.126812\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.125544\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.124288\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.123045\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.131815\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.120497\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.119292\n",
            "resetting env. episode 403.000000, reward total was -18.000000. running mean: -20.098099\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.107118\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.106047\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.114986\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.103836\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.112798\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.111670\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.100553\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.099548\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.108552\n",
            "resetting env. episode 413.000000, reward total was -18.000000. running mean: -20.087467\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.096592\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.105626\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.114570\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.123424\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.112190\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.111068\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.119957\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.118758\n",
            "resetting env. episode 422.000000, reward total was -18.000000. running mean: -20.097570\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.096595\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.095629\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.084672\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.083826\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.092987\n",
            "resetting env. episode 428.000000, reward total was -17.000000. running mean: -20.062057\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.051437\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.050922\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.060413\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.059809\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.069211\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.078519\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.087734\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.096856\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.105888\n",
            "resetting env. episode 438.000000, reward total was -17.000000. running mean: -20.074829\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.084081\n",
            "resetting env. episode 440.000000, reward total was -17.000000. running mean: -20.053240\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.052707\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.052180\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.051659\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.061142\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.060531\n",
            "resetting env. episode 446.000000, reward total was -18.000000. running mean: -20.039925\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.039526\n",
            "resetting env. episode 448.000000, reward total was -18.000000. running mean: -20.019131\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.018939\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.028750\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.038463\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.048078\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.057597\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.057021\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.056451\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.065886\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.075228\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.084475\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.073631\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.082894\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.072065\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.071345\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.070631\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.069925\n",
            "resetting env. episode 465.000000, reward total was -18.000000. running mean: -20.049226\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.058733\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.048146\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.037665\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.037288\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.036915\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.036546\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.046180\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.035719\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.045361\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.044908\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.054459\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.053914\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.053375\n",
            "resetting env. episode 479.000000, reward total was -18.000000. running mean: -20.032841\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.032513\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.032188\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.021866\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.031647\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.021331\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.021117\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.010906\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.010797\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.000689\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.010682\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.010576\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.010470\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.010365\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.010261\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.010159\n",
            "resetting env. episode 495.000000, reward total was -18.000000. running mean: -19.990057\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -19.980157\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -19.990355\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.000452\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.000447\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -19.990443\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -19.990538\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.000633\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.000626\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.000620\n",
            "resetting env. episode 505.000000, reward total was -19.000000. running mean: -19.990614\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -19.990708\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -19.990801\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -19.990893\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.000984\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.000974\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.000964\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.000955\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.000945\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.000936\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.010926\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.020817\n",
            "resetting env. episode 517.000000, reward total was -18.000000. running mean: -20.000609\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.010603\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.020497\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.020292\n",
            "resetting env. episode 521.000000, reward total was -18.000000. running mean: -20.000089\n",
            "resetting env. episode 522.000000, reward total was -19.000000. running mean: -19.990088\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -19.980187\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -19.970385\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -19.970681\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -19.980974\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -19.991165\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -19.991253\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.001341\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.001327\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.001314\n",
            "resetting env. episode 532.000000, reward total was -19.000000. running mean: -19.991301\n",
            "resetting env. episode 533.000000, reward total was -16.000000. running mean: -19.951388\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -19.961874\n",
            "resetting env. episode 535.000000, reward total was -18.000000. running mean: -19.942255\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -19.942833\n",
            "resetting env. episode 537.000000, reward total was -18.000000. running mean: -19.923404\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -19.924170\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -19.934929\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -19.945579\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -19.956123\n",
            "resetting env. episode 542.000000, reward total was -19.000000. running mean: -19.946562\n",
            "resetting env. episode 543.000000, reward total was -18.000000. running mean: -19.927097\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -19.937826\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -19.938447\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -19.949063\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -19.949572\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -19.960077\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -19.970476\n",
            "resetting env. episode 550.000000, reward total was -19.000000. running mean: -19.960771\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -19.971163\n",
            "resetting env. episode 552.000000, reward total was -18.000000. running mean: -19.951452\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -19.951937\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -19.962418\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -19.962794\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -19.973166\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -19.983434\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -19.993600\n",
            "resetting env. episode 559.000000, reward total was -19.000000. running mean: -19.983664\n",
            "resetting env. episode 560.000000, reward total was -19.000000. running mean: -19.973827\n",
            "resetting env. episode 561.000000, reward total was -19.000000. running mean: -19.964089\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -19.974448\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -19.984703\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -19.984856\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -19.995008\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.005058\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.015007\n",
            "resetting env. episode 568.000000, reward total was -18.000000. running mean: -19.994857\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -19.994908\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.004959\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.014910\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.014761\n",
            "resetting env. episode 573.000000, reward total was -19.000000. running mean: -20.004613\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.014567\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -20.014421\n",
            "resetting env. episode 576.000000, reward total was -19.000000. running mean: -20.004277\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.004234\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.014192\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.024050\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.023810\n",
            "resetting env. episode 581.000000, reward total was -18.000000. running mean: -20.003571\n",
            "resetting env. episode 582.000000, reward total was -19.000000. running mean: -19.993536\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -19.993600\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.003664\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -20.003628\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.003591\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.013556\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.003420\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.003386\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.013352\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.023218\n",
            "resetting env. episode 592.000000, reward total was -18.000000. running mean: -20.002986\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.002956\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.002927\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.012898\n",
            "resetting env. episode 596.000000, reward total was -19.000000. running mean: -20.002769\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.012741\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.012613\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.022487\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.032262\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.041940\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.051520\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.051005\n",
            "resetting env. episode 604.000000, reward total was -19.000000. running mean: -20.040495\n",
            "resetting env. episode 605.000000, reward total was -19.000000. running mean: -20.030090\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.029789\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.029491\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.029197\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.028905\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.028616\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.028329\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.018046\n",
            "resetting env. episode 613.000000, reward total was -19.000000. running mean: -20.007866\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.017787\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.027609\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.037333\n",
            "resetting env. episode 617.000000, reward total was -19.000000. running mean: -20.026960\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.036690\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.046323\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.045860\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.055401\n",
            "resetting env. episode 622.000000, reward total was -18.000000. running mean: -20.034847\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.044499\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -20.044054\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.043613\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.053177\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.052645\n",
            "resetting env. episode 628.000000, reward total was -19.000000. running mean: -20.042119\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -20.041698\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.051281\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.060768\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.070160\n",
            "resetting env. episode 633.000000, reward total was -19.000000. running mean: -20.059459\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.068864\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.068175\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.077494\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.086719\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.095852\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.104893\n",
            "resetting env. episode 640.000000, reward total was -19.000000. running mean: -20.093844\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.092906\n",
            "resetting env. episode 642.000000, reward total was -17.000000. running mean: -20.061977\n",
            "resetting env. episode 643.000000, reward total was -18.000000. running mean: -20.041357\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.050943\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -20.040434\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.050030\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.039529\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.049134\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.058643\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.058056\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.057476\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.056901\n",
            "resetting env. episode 653.000000, reward total was -19.000000. running mean: -20.046332\n",
            "resetting env. episode 654.000000, reward total was -19.000000. running mean: -20.035869\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.045510\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.055055\n",
            "resetting env. episode 657.000000, reward total was -18.000000. running mean: -20.034504\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.044159\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.043718\n",
            "resetting env. episode 660.000000, reward total was -18.000000. running mean: -20.023280\n",
            "resetting env. episode 661.000000, reward total was -19.000000. running mean: -20.013048\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.022917\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.022688\n",
            "resetting env. episode 664.000000, reward total was -19.000000. running mean: -20.012461\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.012336\n",
            "resetting env. episode 666.000000, reward total was -19.000000. running mean: -20.002213\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.012191\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.012069\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.021948\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -20.021729\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -20.011512\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.021396\n",
            "resetting env. episode 673.000000, reward total was -19.000000. running mean: -20.011183\n",
            "resetting env. episode 674.000000, reward total was -19.000000. running mean: -20.001071\n",
            "resetting env. episode 675.000000, reward total was -19.000000. running mean: -19.991060\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -19.981149\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -19.981338\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -19.971525\n",
            "resetting env. episode 679.000000, reward total was -18.000000. running mean: -19.951809\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -19.952291\n",
            "resetting env. episode 681.000000, reward total was -18.000000. running mean: -19.932768\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -19.943441\n",
            "resetting env. episode 683.000000, reward total was -19.000000. running mean: -19.934006\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -19.944666\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -19.935219\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -19.935867\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -19.946509\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -19.957043\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -19.967473\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -19.977798\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -19.988020\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -19.998140\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -19.998159\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -19.998177\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -19.988195\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -19.988313\n",
            "resetting env. episode 697.000000, reward total was -19.000000. running mean: -19.978430\n",
            "resetting env. episode 698.000000, reward total was -18.000000. running mean: -19.958646\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -19.969060\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -19.969369\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -19.969675\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -19.969978\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -19.970279\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -19.980576\n",
            "resetting env. episode 705.000000, reward total was -19.000000. running mean: -19.970770\n",
            "resetting env. episode 706.000000, reward total was -18.000000. running mean: -19.951062\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -19.961552\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -19.961936\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -19.972317\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -19.972594\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -19.962868\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -19.963239\n",
            "resetting env. episode 713.000000, reward total was -18.000000. running mean: -19.943607\n",
            "resetting env. episode 714.000000, reward total was -18.000000. running mean: -19.924171\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -19.934929\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -19.945580\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -19.946124\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -19.946663\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -19.957196\n",
            "resetting env. episode 720.000000, reward total was -19.000000. running mean: -19.947624\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -19.958148\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -19.968566\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -19.978881\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -19.989092\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -19.999201\n",
            "resetting env. episode 726.000000, reward total was -18.000000. running mean: -19.979209\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -19.979417\n",
            "resetting env. episode 728.000000, reward total was -18.000000. running mean: -19.959623\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -19.970026\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -19.980326\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -19.990523\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.000618\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.010612\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -20.000505\n",
            "resetting env. episode 735.000000, reward total was -19.000000. running mean: -19.990500\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.000595\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -19.990589\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -19.990684\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -19.990777\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.000869\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.010860\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.010752\n",
            "resetting env. episode 743.000000, reward total was -18.000000. running mean: -19.990644\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.000738\n",
            "resetting env. episode 745.000000, reward total was -19.000000. running mean: -19.990730\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.000823\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.010815\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.010707\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.020600\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.020394\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.030190\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.029888\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.029589\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.029293\n",
            "resetting env. episode 755.000000, reward total was -18.000000. running mean: -20.009000\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.008910\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.008821\n",
            "resetting env. episode 758.000000, reward total was -19.000000. running mean: -19.998733\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -19.998745\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.008758\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.018670\n",
            "resetting env. episode 762.000000, reward total was -19.000000. running mean: -20.008484\n",
            "resetting env. episode 763.000000, reward total was -18.000000. running mean: -19.988399\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -19.988515\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -19.998630\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.008643\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.018557\n",
            "resetting env. episode 768.000000, reward total was -19.000000. running mean: -20.008371\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -19.998288\n",
            "resetting env. episode 770.000000, reward total was -19.000000. running mean: -19.988305\n",
            "resetting env. episode 771.000000, reward total was -17.000000. running mean: -19.958422\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -19.958838\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -19.969249\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -19.979557\n",
            "resetting env. episode 775.000000, reward total was -19.000000. running mean: -19.969761\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -19.960063\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -19.970463\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -19.970758\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -19.961051\n",
            "resetting env. episode 780.000000, reward total was -19.000000. running mean: -19.951440\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -19.941926\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -19.942506\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -19.943081\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -19.953651\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -19.954114\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -19.954573\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -19.955027\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -19.965477\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -19.975822\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -19.986064\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -19.976203\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -19.976441\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -19.986677\n",
            "resetting env. episode 794.000000, reward total was -18.000000. running mean: -19.966810\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -19.967142\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -19.977471\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -19.987696\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -19.977819\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -19.988041\n",
            "resetting env. episode 800.000000, reward total was -20.000000. running mean: -19.988160\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -19.998279\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -19.988296\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -19.998413\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.008429\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -19.998345\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -19.988361\n",
            "resetting env. episode 807.000000, reward total was -19.000000. running mean: -19.978477\n",
            "resetting env. episode 808.000000, reward total was -18.000000. running mean: -19.958693\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -19.959106\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -19.969515\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -19.979820\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -19.980021\n",
            "resetting env. episode 813.000000, reward total was -18.000000. running mean: -19.960221\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -19.960619\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -19.961013\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -19.961403\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -19.971789\n",
            "resetting env. episode 818.000000, reward total was -19.000000. running mean: -19.962071\n",
            "resetting env. episode 819.000000, reward total was -19.000000. running mean: -19.952450\n",
            "resetting env. episode 820.000000, reward total was -19.000000. running mean: -19.942926\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -19.943496\n",
            "resetting env. episode 822.000000, reward total was -18.000000. running mean: -19.924061\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -19.924821\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -19.915572\n",
            "resetting env. episode 825.000000, reward total was -18.000000. running mean: -19.896417\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -19.907453\n",
            "resetting env. episode 827.000000, reward total was -18.000000. running mean: -19.888378\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -19.889494\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -19.900599\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -19.911593\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -19.902477\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -19.913453\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -19.904318\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -19.905275\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -19.906222\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -19.907160\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -19.898088\n",
            "resetting env. episode 838.000000, reward total was -18.000000. running mean: -19.879107\n",
            "resetting env. episode 839.000000, reward total was -19.000000. running mean: -19.870316\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -19.881613\n",
            "resetting env. episode 841.000000, reward total was -19.000000. running mean: -19.872797\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -19.884069\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -19.895228\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -19.906276\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -19.907213\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -19.898141\n",
            "resetting env. episode 847.000000, reward total was -18.000000. running mean: -19.879160\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -19.890368\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -19.891465\n",
            "resetting env. episode 850.000000, reward total was -19.000000. running mean: -19.882550\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -19.883724\n",
            "resetting env. episode 852.000000, reward total was -19.000000. running mean: -19.874887\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -19.886138\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -19.887277\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -19.888404\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -19.899520\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -19.910525\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -19.901420\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -19.912405\n",
            "resetting env. episode 860.000000, reward total was -17.000000. running mean: -19.883281\n",
            "resetting env. episode 861.000000, reward total was -19.000000. running mean: -19.874449\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -19.875704\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -19.886947\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -19.888078\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -19.899197\n",
            "resetting env. episode 866.000000, reward total was -16.000000. running mean: -19.860205\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -19.871603\n",
            "resetting env. episode 868.000000, reward total was -19.000000. running mean: -19.862887\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -19.874258\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -19.875515\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -19.886760\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -19.897893\n",
            "resetting env. episode 873.000000, reward total was -19.000000. running mean: -19.888914\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -19.890025\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -19.891124\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -19.902213\n",
            "resetting env. episode 877.000000, reward total was -19.000000. running mean: -19.893191\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -19.884259\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -19.875416\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -19.876662\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -19.877896\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -19.879117\n",
            "resetting env. episode 883.000000, reward total was -19.000000. running mean: -19.870325\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -19.881622\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -19.882806\n",
            "resetting env. episode 886.000000, reward total was -19.000000. running mean: -19.873978\n",
            "resetting env. episode 887.000000, reward total was -18.000000. running mean: -19.855238\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -19.866686\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -19.868019\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -19.879339\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -19.880545\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -19.891740\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -19.902823\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -19.913794\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -19.914656\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -19.925510\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -19.936255\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -19.946892\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -19.947423\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -19.957949\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -19.958369\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -19.968786\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -19.959098\n",
            "resetting env. episode 904.000000, reward total was -19.000000. running mean: -19.949507\n",
            "resetting env. episode 905.000000, reward total was -18.000000. running mean: -19.930012\n",
            "resetting env. episode 906.000000, reward total was -20.000000. running mean: -19.930712\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -19.941405\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -19.951991\n",
            "resetting env. episode 909.000000, reward total was -18.000000. running mean: -19.932471\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -19.933146\n",
            "resetting env. episode 911.000000, reward total was -19.000000. running mean: -19.923815\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -19.934576\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -19.935231\n",
            "resetting env. episode 914.000000, reward total was -19.000000. running mean: -19.925878\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -19.926620\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -19.937353\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -19.947980\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -19.958500\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -19.958915\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -19.959326\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -19.969733\n",
            "resetting env. episode 922.000000, reward total was -19.000000. running mean: -19.960035\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -19.970435\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -19.980731\n",
            "resetting env. episode 925.000000, reward total was -19.000000. running mean: -19.970923\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -19.981214\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -19.991402\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -19.991488\n",
            "resetting env. episode 929.000000, reward total was -19.000000. running mean: -19.981573\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -19.991757\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.001840\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.001821\n",
            "resetting env. episode 933.000000, reward total was -16.000000. running mean: -19.961803\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -19.952185\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -19.962663\n",
            "resetting env. episode 936.000000, reward total was -19.000000. running mean: -19.953037\n",
            "resetting env. episode 937.000000, reward total was -19.000000. running mean: -19.943506\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -19.954071\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -19.954530\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -19.964985\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -19.975335\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -19.975582\n",
            "resetting env. episode 943.000000, reward total was -18.000000. running mean: -19.955826\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -19.966268\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -19.976605\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -19.966839\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -19.957171\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -19.967599\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -19.967923\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -19.968244\n",
            "resetting env. episode 951.000000, reward total was -19.000000. running mean: -19.958561\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -19.948976\n",
            "resetting env. episode 953.000000, reward total was -18.000000. running mean: -19.929486\n",
            "resetting env. episode 954.000000, reward total was -19.000000. running mean: -19.920191\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -19.930989\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -19.941679\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -19.942263\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -19.952840\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -19.963312\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -19.973678\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -19.973942\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -19.964202\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -19.974560\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -19.964815\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -19.965166\n",
            "resetting env. episode 966.000000, reward total was -19.000000. running mean: -19.955515\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -19.955960\n",
            "resetting env. episode 968.000000, reward total was -17.000000. running mean: -19.926400\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -19.927136\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -19.927865\n",
            "resetting env. episode 971.000000, reward total was -17.000000. running mean: -19.898586\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -19.899600\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -19.910604\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -19.901498\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -19.902483\n",
            "resetting env. episode 976.000000, reward total was -19.000000. running mean: -19.893458\n",
            "resetting env. episode 977.000000, reward total was -18.000000. running mean: -19.874524\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -19.875778\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -19.877021\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -19.888250\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -19.889368\n",
            "resetting env. episode 982.000000, reward total was -19.000000. running mean: -19.880474\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -19.891670\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -19.902753\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -19.913725\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -19.904588\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -19.915542\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -19.926387\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -19.927123\n",
            "resetting env. episode 990.000000, reward total was -18.000000. running mean: -19.907852\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -19.908773\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -19.919685\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -19.930489\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -19.941184\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -19.941772\n",
            "resetting env. episode 996.000000, reward total was -19.000000. running mean: -19.932354\n",
            "resetting env. episode 997.000000, reward total was -20.000000. running mean: -19.933031\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -19.943700\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -19.944263\n",
            "resetting env. episode 1000.000000, reward total was -17.000000. running mean: -19.914821\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -19.925672\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -19.926416\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -19.937152\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -19.947780\n",
            "resetting env. episode 1005.000000, reward total was -18.000000. running mean: -19.928302\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -19.939019\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -19.929629\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -19.940333\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -19.950929\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -19.951420\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -19.951906\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -19.962387\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -19.972763\n",
            "resetting env. episode 1014.000000, reward total was -19.000000. running mean: -19.963035\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -19.963405\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -19.973771\n",
            "resetting env. episode 1017.000000, reward total was -19.000000. running mean: -19.964033\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -19.964393\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -19.954749\n",
            "resetting env. episode 1020.000000, reward total was -19.000000. running mean: -19.945201\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -19.945749\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -19.956292\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -19.956729\n",
            "resetting env. episode 1024.000000, reward total was -19.000000. running mean: -19.947162\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -19.957690\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -19.958113\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -19.968532\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -19.958847\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -19.969258\n",
            "resetting env. episode 1030.000000, reward total was -19.000000. running mean: -19.959566\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -19.969970\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -19.980270\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -19.980468\n",
            "resetting env. episode 1034.000000, reward total was -17.000000. running mean: -19.950663\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -19.961156\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -19.971545\n",
            "resetting env. episode 1037.000000, reward total was -18.000000. running mean: -19.951829\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -19.952311\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -19.962788\n",
            "resetting env. episode 1040.000000, reward total was -18.000000. running mean: -19.943160\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -19.943728\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -19.954291\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -19.954748\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -19.965201\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -19.975549\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -19.975793\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -19.976035\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -19.986275\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -19.986412\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -19.986548\n",
            "resetting env. episode 1051.000000, reward total was -17.000000. running mean: -19.956683\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -19.957116\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -19.967545\n",
            "resetting env. episode 1054.000000, reward total was -18.000000. running mean: -19.947869\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -19.938391\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -19.939007\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -19.939617\n",
            "resetting env. episode 1058.000000, reward total was -18.000000. running mean: -19.920220\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -19.921018\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -19.921808\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -19.922590\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -19.933364\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -19.934030\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -19.944690\n",
            "resetting env. episode 1065.000000, reward total was -18.000000. running mean: -19.925243\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -19.935991\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -19.946631\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -19.947165\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -19.947693\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -19.958216\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -19.968634\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -19.958947\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -19.959358\n",
            "resetting env. episode 1074.000000, reward total was -19.000000. running mean: -19.949764\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -19.940267\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -19.930864\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -19.941555\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -19.952140\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -19.962619\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -19.962992\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -19.973362\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -19.983629\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -19.973792\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -19.964055\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -19.974414\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -19.984670\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -19.984823\n",
            "resetting env. episode 1088.000000, reward total was -19.000000. running mean: -19.974975\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -19.975225\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -19.975473\n",
            "resetting env. episode 1091.000000, reward total was -20.000000. running mean: -19.975718\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -19.965961\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -19.966301\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -19.966638\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.976972\n",
            "resetting env. episode 1096.000000, reward total was -18.000000. running mean: -19.957202\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -19.957630\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -19.968054\n",
            "resetting env. episode 1099.000000, reward total was -19.000000. running mean: -19.958373\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -19.948790\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -19.939302\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -19.919909\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -19.930710\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -19.931403\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -19.932089\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -19.942768\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -19.953340\n",
            "resetting env. episode 1108.000000, reward total was -16.000000. running mean: -19.913807\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -19.904669\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -19.905622\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -19.896566\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -19.897600\n",
            "resetting env. episode 1113.000000, reward total was -18.000000. running mean: -19.878624\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -19.889838\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -19.900939\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -19.911930\n",
            "resetting env. episode 1117.000000, reward total was -19.000000. running mean: -19.902811\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -19.903783\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -19.914745\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -19.915597\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -19.926441\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -19.937177\n",
            "resetting env. episode 1123.000000, reward total was -19.000000. running mean: -19.927805\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -19.938527\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -19.939142\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -19.939750\n",
            "resetting env. episode 1127.000000, reward total was -20.000000. running mean: -19.940353\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -19.950949\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -19.951440\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -19.941925\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -19.952506\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -19.962981\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -19.973351\n",
            "resetting env. episode 1134.000000, reward total was -18.000000. running mean: -19.953618\n",
            "resetting env. episode 1135.000000, reward total was -19.000000. running mean: -19.944082\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -19.934641\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -19.935294\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -19.935941\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -19.926582\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -19.937316\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -19.947943\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -19.948464\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -19.958979\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -19.969389\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -19.959695\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -19.960098\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -19.970497\n",
            "resetting env. episode 1148.000000, reward total was -19.000000. running mean: -19.960792\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -19.961185\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -19.961573\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -19.961957\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -19.972337\n",
            "resetting env. episode 1153.000000, reward total was -18.000000. running mean: -19.952614\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -19.953088\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -19.953557\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -19.964021\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -19.974381\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -19.984637\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -19.984791\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -19.994943\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.004994\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -19.994944\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -19.994994\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.005044\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.014994\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.014844\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -20.004696\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.014649\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.014502\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.024357\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.034113\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.033772\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.043435\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.043000\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.052570\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.052045\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -20.041524\n",
            "resetting env. episode 1178.000000, reward total was -19.000000. running mean: -20.031109\n",
            "resetting env. episode 1179.000000, reward total was -18.000000. running mean: -20.010798\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -20.000690\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -20.000683\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -19.990676\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -19.990769\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.000862\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.010853\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -20.010744\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -20.000637\n",
            "resetting env. episode 1188.000000, reward total was -19.000000. running mean: -19.990631\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.000724\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.010717\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.010610\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.010504\n",
            "resetting env. episode 1193.000000, reward total was -19.000000. running mean: -20.000399\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.010395\n",
            "resetting env. episode 1195.000000, reward total was -19.000000. running mean: -20.000291\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.000288\n",
            "resetting env. episode 1197.000000, reward total was -19.000000. running mean: -19.990285\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -19.990382\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -19.990478\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.000574\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.010568\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -20.010462\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.010358\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.020254\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -20.020051\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.019851\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -20.019652\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -20.009456\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.009361\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.019268\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.019075\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -20.008884\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -19.998795\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -19.998808\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.008819\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -19.998731\n",
            "resetting env. episode 1217.000000, reward total was -19.000000. running mean: -19.988744\n",
            "resetting env. episode 1218.000000, reward total was -18.000000. running mean: -19.968857\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -19.979168\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -19.989376\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -19.999483\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.009488\n",
            "resetting env. episode 1223.000000, reward total was -17.000000. running mean: -19.979393\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -19.979599\n",
            "resetting env. episode 1225.000000, reward total was -19.000000. running mean: -19.969803\n",
            "resetting env. episode 1226.000000, reward total was -18.000000. running mean: -19.950105\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -19.960604\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -19.970998\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -19.971288\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -19.971575\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -19.971859\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -19.982141\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.992319\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -19.992396\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -19.982472\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -19.992647\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -19.992721\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.002794\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.012766\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -20.002638\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.012612\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.022486\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -20.012261\n",
            "resetting env. episode 1244.000000, reward total was -19.000000. running mean: -20.002138\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -19.992117\n",
            "resetting env. episode 1246.000000, reward total was -18.000000. running mean: -19.972196\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -19.962474\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -19.962849\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -19.963220\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -19.973588\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -19.973852\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -19.984114\n",
            "resetting env. episode 1253.000000, reward total was -17.000000. running mean: -19.954273\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -19.954730\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -19.965183\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -19.965531\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -19.975875\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -19.986117\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -19.996256\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -19.996293\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -19.986330\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -19.976467\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -19.986702\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -19.976835\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -19.987067\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -19.997196\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.007224\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.007152\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.017080\n",
            "resetting env. episode 1270.000000, reward total was -19.000000. running mean: -20.006910\n",
            "resetting env. episode 1271.000000, reward total was -19.000000. running mean: -19.996840\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.006872\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.006803\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.016735\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.026568\n",
            "resetting env. episode 1276.000000, reward total was -17.000000. running mean: -19.996302\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.006339\n",
            "resetting env. episode 1278.000000, reward total was -18.000000. running mean: -19.986276\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -19.996413\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.006449\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.016384\n",
            "resetting env. episode 1282.000000, reward total was -18.000000. running mean: -19.996221\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -19.986258\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -19.986396\n",
            "resetting env. episode 1285.000000, reward total was -19.000000. running mean: -19.976532\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -19.986767\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -19.976899\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -19.987130\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -19.997259\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -19.997286\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.007313\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -20.007240\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -20.007168\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -20.007096\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.007025\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.016955\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -20.006785\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -19.996717\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -19.996750\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -19.996783\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -19.986815\n",
            "resetting env. episode 1302.000000, reward total was -17.000000. running mean: -19.956947\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -19.957377\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -19.957803\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.958225\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -19.958643\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -19.969057\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -19.969366\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -19.969672\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -19.959976\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -19.960376\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -19.950772\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -19.941265\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -19.951852\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -19.962333\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -19.972710\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -19.972983\n",
            "resetting env. episode 1318.000000, reward total was -18.000000. running mean: -19.953253\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -19.963721\n",
            "resetting env. episode 1320.000000, reward total was -20.000000. running mean: -19.964083\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -19.954443\n",
            "resetting env. episode 1322.000000, reward total was -18.000000. running mean: -19.934898\n",
            "resetting env. episode 1323.000000, reward total was -18.000000. running mean: -19.915549\n",
            "resetting env. episode 1324.000000, reward total was -18.000000. running mean: -19.896394\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -19.907430\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -19.918355\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -19.919172\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -19.929980\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -19.930680\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -19.931374\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -19.942060\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -19.952639\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -19.953113\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -19.963582\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -19.973946\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -19.984206\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.984364\n",
            "resetting env. episode 1338.000000, reward total was -17.000000. running mean: -19.954521\n",
            "resetting env. episode 1339.000000, reward total was -18.000000. running mean: -19.934975\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -19.935626\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -19.926269\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -19.927007\n",
            "resetting env. episode 1343.000000, reward total was -19.000000. running mean: -19.917737\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -19.928559\n",
            "resetting env. episode 1345.000000, reward total was -18.000000. running mean: -19.909274\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -19.910181\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -19.921079\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -19.921868\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -19.932650\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -19.933323\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -19.943990\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -19.954550\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -19.955005\n",
            "resetting env. episode 1354.000000, reward total was -17.000000. running mean: -19.925455\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -19.916200\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -19.907038\n",
            "resetting env. episode 1357.000000, reward total was -19.000000. running mean: -19.897968\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -19.888988\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -19.890098\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -19.901197\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -19.902185\n",
            "resetting env. episode 1362.000000, reward total was -19.000000. running mean: -19.893163\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -19.884232\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -19.885389\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -19.896535\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -19.897570\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -19.898594\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -19.899608\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -19.910612\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -19.911506\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -19.922391\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -19.933167\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -19.943836\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -19.954397\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -19.964853\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -19.975205\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -19.975453\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -19.975698\n",
            "resetting env. episode 1379.000000, reward total was -18.000000. running mean: -19.955941\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -19.946382\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -19.946918\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -19.947449\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -19.937974\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -19.938595\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -19.949209\n",
            "resetting env. episode 1386.000000, reward total was -18.000000. running mean: -19.929716\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -19.940419\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -19.941015\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -19.951605\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -19.952089\n",
            "resetting env. episode 1391.000000, reward total was -18.000000. running mean: -19.932568\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -19.943242\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -19.943810\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -19.944372\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -19.944928\n",
            "resetting env. episode 1396.000000, reward total was -20.000000. running mean: -19.945479\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -19.946024\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -19.936564\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -19.927198\n",
            "resetting env. episode 1400.000000, reward total was -17.000000. running mean: -19.897926\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -19.908947\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -19.919857\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -19.930659\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -19.941352\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -19.941939\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -19.942519\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -19.953094\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -19.953563\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -19.954028\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -19.954487\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -19.964942\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -19.965293\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -19.965640\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -19.975984\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -19.966224\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -19.976562\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.976796\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -19.987028\n",
            "resetting env. episode 1419.000000, reward total was -18.000000. running mean: -19.967158\n",
            "resetting env. episode 1420.000000, reward total was -17.000000. running mean: -19.937486\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -19.938111\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -19.938730\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -19.949343\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -19.959849\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -19.960251\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -19.970648\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -19.980942\n",
            "resetting env. episode 1428.000000, reward total was -18.000000. running mean: -19.961133\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -19.971521\n",
            "resetting env. episode 1430.000000, reward total was -18.000000. running mean: -19.951806\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -19.962288\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -19.972665\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -19.982938\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -19.993109\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -19.993178\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.003246\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.013214\n",
            "resetting env. episode 1438.000000, reward total was -18.000000. running mean: -19.993082\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.003151\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -20.003119\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.003088\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.013057\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.022927\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.032697\n",
            "resetting env. episode 1445.000000, reward total was -18.000000. running mean: -20.012370\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.022247\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.032024\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.021704\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.021487\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.021272\n",
            "resetting env. episode 1451.000000, reward total was -20.000000. running mean: -20.021059\n",
            "resetting env. episode 1452.000000, reward total was -19.000000. running mean: -20.010849\n",
            "resetting env. episode 1453.000000, reward total was -17.000000. running mean: -19.980740\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -19.980933\n",
            "resetting env. episode 1455.000000, reward total was -19.000000. running mean: -19.971124\n",
            "resetting env. episode 1456.000000, reward total was -18.000000. running mean: -19.951412\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -19.951898\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -19.952379\n",
            "resetting env. episode 1459.000000, reward total was -19.000000. running mean: -19.942855\n",
            "resetting env. episode 1460.000000, reward total was -19.000000. running mean: -19.933427\n",
            "resetting env. episode 1461.000000, reward total was -19.000000. running mean: -19.924093\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -19.934852\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -19.945503\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -19.946048\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -19.956588\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -19.957022\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -19.967452\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -19.977777\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -19.977999\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -19.968219\n",
            "resetting env. episode 1471.000000, reward total was -19.000000. running mean: -19.958537\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -19.968952\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -19.969262\n",
            "resetting env. episode 1474.000000, reward total was -19.000000. running mean: -19.959570\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -19.969974\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.980274\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -19.990471\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -19.970567\n",
            "resetting env. episode 1479.000000, reward total was -20.000000. running mean: -19.970861\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -19.981152\n",
            "resetting env. episode 1481.000000, reward total was -19.000000. running mean: -19.971341\n",
            "resetting env. episode 1482.000000, reward total was -19.000000. running mean: -19.961627\n",
            "resetting env. episode 1483.000000, reward total was -18.000000. running mean: -19.942011\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -19.942591\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -19.953165\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -19.953633\n",
            "resetting env. episode 1487.000000, reward total was -20.000000. running mean: -19.954097\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.954556\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -19.955011\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -19.965461\n",
            "resetting env. episode 1491.000000, reward total was -19.000000. running mean: -19.955806\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -19.946248\n",
            "resetting env. episode 1493.000000, reward total was -19.000000. running mean: -19.936785\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -19.937418\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -19.938043\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -19.948663\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -19.949176\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -19.949685\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.950188\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -19.960686\n",
            "CPU times: user 2h 35min 34s, sys: 38min 44s, total: 3h 14min 18s\n",
            "Wall time: 1h 40min 17s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "w2NblmwDsL3y",
        "outputId": "9299b089-b5c7-42f9-fe2e-3eed03bf517b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAG8ElEQVR4nO3dT29cVx2A4TtJmsR/YqcZx6imEEChm6wQ3XbFhvJNWKB+CrZI8A3Y8gW6ZseqQizYIBKIAm6K3ThxHCcUGFaVaKYFvxOHO06eZ3mkc+e3ejXnSFd3MpvNBoDi3NgDAGePcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcADZhUU3/vDmyolfqz03GYb3blwaVt9Y/k5Nr24Om+tXXvg5j44eD3sPDk5hIk7bwxtbw9Fbb77wc1bvPxyu3vnkFCYazwcffjpZZN/C4Xj/uyuLbl1q06tXhxs7Oy/8nHsf3xeOJfXwW9vDJ9//9gs/Z+t3fz7z4VjU8v8FAJaOcACZcACZcADZwpejr5uDw8Ph0eHjufUr62vDmxsbI0zEaVvbfTCs7c5faD/52ubw+OvXRphoeQnHCe0/OBhu37s3t35jZ0c4XhGbd/427PzmD3PrH7/7HeF4jqMKkAkHkAkHkAkHkLkcPaEra6vDW9evz61vrK+NMA2MSzhOaHs6Hban07HHgKXgqAJkwgFkwgFkwgFkLkdP6PGTJ8PR8fHc+trllWF9bXWEiWA8wnFC9/f2v/JdlXfWbowwEYzHUQXIhAPIhAPIhAPIXI6e0MrlS8O1zc259dXLl0eYhpfh2ebq8Oib868VPL3qfaTnCccJ7WxvDzvb22OPwUu0f+vtYf/W22OPcSY4qgCZcACZcACZcACZy9HnPH329+Hh4eELP+f42dNTmIaX4dLh8Zd+PyU/5+H8u0uvC+F4zt3d3eHu7u7YY/ASbX90Z9j+6M7YY5xpwsFrZzL2AK8AdxxAJhxAtvBR5b2f/OI05wDOkMlsNlto4/7+/mIbgaUxnU4XuvJxVAEy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QCyhV+r/+2vfnaacwAj+MGPf7rQvoVfq//5+9e8Vg9n3Acffuq1euD/QziATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiAbOHPI7xst27eHNZXV+bWf//H28Ph0dEIEwGfW9pwrK+uDBvr619Ym81mw4Xz50eaCPicowqQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQLe3nEf70l78OF9+YH+/J0+MRpgH+09KG4/7+/tgjAF/BUQXIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPIhAPILow9ALzuPlu9OBx+Yzq3fv74s2Hj7t4wGWGm/0U4YGTH0yvD7R99bxgmX0zE2u7BsHF3b6Sp/jtHFSATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiDzeQQY2bl//HO4dPBkbv3i4fEI05yMcMDI1nYPhlu//PXc+mQ2wjAnJBwwsskwDJN/LXElvoQ7DiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiC7sOjG6++8e5pzAGfIZDabLbRxb29vsY3A0tja2possm/hfxyTyUK/B7wC3HEAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAAmXAA2cLfVQFeX/5xAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwANm/AStbrEUXAo3YAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=600_le_4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}