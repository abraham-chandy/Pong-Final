{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H=1000_1e_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "c3bac19f-05d3-4556-e94f-f9949eb12896"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 31.0 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=ff131c81531270d87883a122da25e97b4f5b74309b3bd2afaf15e8b3daa0af54\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "ff58f210-6263-4e65-dd4e-21cb72b7fbbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "1cd8c80a-640a-4d58-d40f-358be4a6355c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "f84f0c0f-d93c-4ff3-d948-6378b30251c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "f8f7b622-9016-4218-a587-4eabb200859f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 1000 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "2c2d6a87-b153-4eb6-d84d-fd0fa7e4d301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.980394\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.970590\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.970884\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.961175\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.961564\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.961948\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.962328\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.962705\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.953078\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.933547\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.934212\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.924870\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.925621\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.926365\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.917101\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.917930\n",
            "resetting env. episode 22.000000, reward total was -18.000000. running mean: -20.888751\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.869863\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.871165\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.872453\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.863729\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.855091\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.836540\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.838175\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.839793\n",
            "resetting env. episode 31.000000, reward total was -19.000000. running mean: -20.821395\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.813181\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.815050\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.816899\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.808730\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.810643\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.812536\n",
            "resetting env. episode 38.000000, reward total was -18.000000. running mean: -20.784411\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.786567\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.768701\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -20.751014\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -20.733504\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.716169\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.699007\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.702017\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.704997\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.707947\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.710868\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.713759\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.706621\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.699555\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.692560\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.675634\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.668878\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.672189\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.675467\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.668712\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.672025\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.675305\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.668552\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.651866\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.645348\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.638894\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.632505\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.636180\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.629818\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.633520\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.627185\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.620913\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.624704\n",
            "resetting env. episode 71.000000, reward total was -18.000000. running mean: -20.598457\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.592472\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.596548\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.590582\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.594676\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.588730\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.582842\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -20.557014\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.561444\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.565829\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.560171\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.564569\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.558924\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.543334\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.547901\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.552422\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.556898\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.561329\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -20.545716\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.550258\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.544756\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.549308\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.553815\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.558277\n",
            "resetting env. episode 95.000000, reward total was -15.000000. running mean: -20.502694\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.507667\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.502591\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.507565\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.492489\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.497564\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.502589\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.487563\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.472687\n",
            "resetting env. episode 104.000000, reward total was -18.000000. running mean: -20.447960\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.453481\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.448946\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.434456\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.440112\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.445711\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.431254\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.436941\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.422572\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.418346\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.414162\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.410021\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.415921\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.421761\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.427544\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.433268\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.418936\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.424746\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.420499\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.416294\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.422131\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.417910\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.413730\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.419593\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.405397\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.401343\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.397330\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.403357\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.389323\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.395430\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -20.381475\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.367661\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.373984\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.380244\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.386442\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.372577\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.378852\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.385063\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.391212\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.387300\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.393427\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.399493\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.405498\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.411443\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.417329\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.413155\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.409024\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.404934\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.400884\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.406875\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.412807\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.408679\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.414592\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.420446\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.416241\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.422079\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.417858\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.423680\n",
            "resetting env. episode 162.000000, reward total was -17.000000. running mean: -20.389443\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.385548\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.371693\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -20.347976\n",
            "resetting env. episode 166.000000, reward total was -18.000000. running mean: -20.324496\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.331251\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.327939\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.334659\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.331313\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.328000\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.334720\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.321373\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.328159\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.324877\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.321628\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.328412\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.325128\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.331877\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.328558\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.335272\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.331920\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.338600\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.335214\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.331862\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.338544\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.325158\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.321907\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.328688\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.325401\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.312147\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.319025\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.325835\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.332577\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.329251\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.325958\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.322699\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.319472\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.326277\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.313014\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.319884\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.326685\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.323418\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.320184\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.326982\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.333713\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.340376\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.346972\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.343502\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.350067\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.356566\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.353001\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.359471\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.365876\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.372217\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.368495\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.364810\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.351162\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.347650\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.354174\n",
            "resetting env. episode 221.000000, reward total was -19.000000. running mean: -20.340632\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.347226\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.343754\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.350316\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.346813\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.353345\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.359811\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.356213\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.362651\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.369025\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.375334\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.381581\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.377765\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.363987\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.350348\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.356844\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.363276\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.369643\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.365946\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.372287\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.358564\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.354979\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.361429\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.357814\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.364236\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.370594\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.376888\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.383119\n",
            "resetting env. episode 249.000000, reward total was -18.000000. running mean: -20.359288\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.365695\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.372038\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.368318\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.374635\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.380888\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.387079\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.393209\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.399276\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.385284\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.371431\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.377717\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.383939\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.380100\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.386299\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.392436\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.388512\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.374626\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.360880\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.367271\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.373599\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.379863\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.386064\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.372203\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.368481\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.374797\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.361049\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.357438\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.353864\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.360325\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.346722\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.353255\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.339722\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.336325\n",
            "resetting env. episode 283.000000, reward total was -19.000000. running mean: -20.322962\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.329732\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.336435\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.343070\n",
            "resetting env. episode 287.000000, reward total was -18.000000. running mean: -20.319640\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.316443\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.323279\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.310046\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.306946\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.293876\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.300937\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.307928\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.314849\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.311700\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.318583\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.325397\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.312143\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.309022\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.305932\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.312872\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.309744\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.306646\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.313580\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.310444\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.317340\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.324166\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.320925\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.317715\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.324538\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.321293\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.318080\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.324899\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.321650\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.318434\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.325249\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.331997\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.338677\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.335290\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.321937\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.328718\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.335431\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.342076\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.338655\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.345269\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.341816\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.328398\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.335114\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.341763\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.348345\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.344862\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.351413\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.347899\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.354420\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.360876\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.367267\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.363594\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.369959\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.376259\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.382496\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.388671\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.384785\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.380937\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.387127\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.393256\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.399324\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.405330\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.401277\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.407264\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.403192\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.409160\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.415068\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.400917\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.396908\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.392939\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.399010\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.405020\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.410970\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.416860\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.422691\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.428464\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.434180\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.439838\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.445440\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.450985\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.456475\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.451911\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.457391\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.462817\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.468189\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.473507\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.478772\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.473985\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.469245\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.474552\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.469807\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.475109\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.470358\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.475654\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.480898\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.476089\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.481328\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.486514\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.481649\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.486833\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.491964\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.497045\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.492074\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.497154\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.492182\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.497260\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.502288\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.507265\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.492192\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.497270\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.492297\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.497375\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.492401\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.497477\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.492502\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.497577\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.502601\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.497575\n",
            "resetting env. episode 405.000000, reward total was -18.000000. running mean: -20.472599\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.477873\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.473095\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.478364\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.473580\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.468844\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.474156\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.469414\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.464720\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.470073\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.475372\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.480619\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.485812\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.490954\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.486045\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.481184\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.486372\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.491509\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.496594\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.501628\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.496611\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.491645\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.496729\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.491762\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.496844\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.481875\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.487057\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.492186\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.497264\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.502292\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.497269\n",
            "resetting env. episode 436.000000, reward total was -18.000000. running mean: -20.472296\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.477573\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.482797\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.487969\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.493090\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.498159\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.483177\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.488345\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.483462\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.478627\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.483841\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.479003\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.484213\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.489371\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.494477\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.489532\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.494637\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.489690\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.484793\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.489946\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.485046\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.480196\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.485394\n",
            "resetting env. episode 459.000000, reward total was -18.000000. running mean: -20.460540\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.455934\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.461375\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.456761\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.462194\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.467572\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.472896\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.478167\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.473385\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.478651\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.483865\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.469026\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.464336\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.469693\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.474996\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.470246\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.465543\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.460888\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.456279\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.461716\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.467099\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.452428\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.437904\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.423525\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.429290\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.434997\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.440647\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.446240\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.451778\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.447260\n",
            "resetting env. episode 489.000000, reward total was -18.000000. running mean: -20.422787\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.428560\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.434274\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.439931\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.445532\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.451077\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.456566\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.452000\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.437480\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.443105\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.438674\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.444288\n",
            "CPU times: user 1h 22min 2s, sys: 12min 43s, total: 1h 34min 46s\n",
            "Wall time: 49min 19s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "838b2242-17c0-4e3c-8945-8891c4b3b8d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980398\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980594\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -20.960788\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.941180\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.941768\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.942351\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.922927\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.923698\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.914461\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.915316\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.916163\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.917002\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.917832\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.898653\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.899667\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.900670\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.901663\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.902647\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.903620\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.904584\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.905538\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.886483\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.887618\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.888742\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.889854\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.880956\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.862146\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.863525\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.864890\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.856241\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.847678\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.849201\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.830709\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.822402\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.824178\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.825937\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.827677\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.829400\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.821106\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.822895\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.814666\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.796520\n",
            "resetting env. episode 46.000000, reward total was -17.000000. running mean: -20.758555\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.750969\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.753459\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.745925\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.748465\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.740981\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.743571\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.736135\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.728774\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.721486\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.724271\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.727029\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.719758\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.712561\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.705435\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.708381\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.711297\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.694184\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.697242\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.690270\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.683367\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.666533\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.659868\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.663269\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.666637\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.659970\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.663371\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.656737\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.660170\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.663568\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.656932\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.660363\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.663759\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.667122\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.660450\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.663846\n",
            "resetting env. episode 82.000000, reward total was -18.000000. running mean: -20.637207\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.640835\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.634427\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.638083\n",
            "resetting env. episode 86.000000, reward total was -18.000000. running mean: -20.611702\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.615585\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.609429\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.603335\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.597301\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.601328\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.595315\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.599362\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.603368\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.597335\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.601361\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.585348\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.589494\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.583599\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.587763\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.591886\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.585967\n",
            "resetting env. episode 103.000000, reward total was -18.000000. running mean: -20.560107\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.564506\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.568861\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.563172\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.567541\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.551865\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.556347\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.560783\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.555175\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.549624\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.554127\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.558586\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.543000\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.537570\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.532194\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.536873\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.541504\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.546089\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.550628\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.545122\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.549670\n",
            "resetting env. episode 124.000000, reward total was -17.000000. running mean: -20.514174\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.509032\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.503942\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.488902\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.484013\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.479173\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.474381\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.469638\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.474941\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.480192\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.485390\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.490536\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.485631\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.490774\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.485867\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.491008\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.496098\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.491137\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.496225\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.491263\n",
            "resetting env. episode 144.000000, reward total was -17.000000. running mean: -20.456351\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.451787\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.457269\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.462696\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.468069\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.473389\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.478655\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.483868\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.479030\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.484239\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.479397\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.484603\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.479757\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.474959\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.480210\n",
            "resetting env. episode 159.000000, reward total was -18.000000. running mean: -20.455408\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.440854\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.446445\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.451981\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.457461\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.452886\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.448357\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.453874\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.459335\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.444742\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.440294\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.435891\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.441532\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.427117\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.432846\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.428517\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.424232\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.409990\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.415890\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.421731\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.427514\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.433239\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.428906\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.424617\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.430371\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.436067\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.421707\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.407490\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.403415\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.409381\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.415287\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.421134\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.426923\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.432653\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.438327\n",
            "resetting env. episode 194.000000, reward total was -18.000000. running mean: -20.413944\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.419804\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.425606\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.421350\n",
            "resetting env. episode 198.000000, reward total was -18.000000. running mean: -20.397137\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.393165\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.379234\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.375441\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.371687\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.367970\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.374290\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.380547\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.386742\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.382874\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.379046\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.365255\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.371603\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.357887\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.364308\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.350665\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.357158\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.363586\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.369951\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.376251\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.382489\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.378664\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.384877\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.391028\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.397118\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.403147\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.409115\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.415024\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.420874\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.426665\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.422399\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.418175\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.423993\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.429753\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.435455\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.431101\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.426790\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.432522\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.428197\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.433915\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.439576\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.435180\n",
            "resetting env. episode 240.000000, reward total was -18.000000. running mean: -20.410828\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.406720\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.412653\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.408526\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.414441\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.410296\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.416193\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.412031\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.407911\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.413832\n",
            "resetting env. episode 250.000000, reward total was -18.000000. running mean: -20.389694\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.395797\n",
            "resetting env. episode 252.000000, reward total was -18.000000. running mean: -20.371839\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.378120\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.384339\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.390496\n",
            "resetting env. episode 256.000000, reward total was -18.000000. running mean: -20.366591\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.372925\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.379196\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.365404\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.351750\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.348232\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -20.334750\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.341402\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.347988\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.354508\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.360963\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.357354\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.363780\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.370142\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.366441\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.372777\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.379049\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.385258\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.381406\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.387592\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.383716\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.369879\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.366180\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.372518\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.368793\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.355105\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.361554\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.357938\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.354359\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.350815\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.357307\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.353734\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.340197\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.346795\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.353327\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.359794\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.366196\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.372534\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.368808\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.375120\n",
            "resetting env. episode 296.000000, reward total was -18.000000. running mean: -20.351369\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.347855\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.344377\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.340933\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.337524\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.344149\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.330707\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.317400\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.324226\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.310984\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.317874\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.324695\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.331448\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.338134\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.344752\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.351305\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.357792\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.364214\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.370572\n",
            "resetting env. episode 315.000000, reward total was -18.000000. running mean: -20.346866\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.343397\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.329963\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.326664\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.333397\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.310063\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.306963\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.313893\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.310754\n",
            "resetting env. episode 324.000000, reward total was -18.000000. running mean: -20.287646\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.294770\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.301822\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.308804\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.315716\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.322559\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.319333\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.326140\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.312879\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.309750\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.316652\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.323486\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.310251\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.297148\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.304177\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.291135\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.288224\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.285341\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.282488\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.269663\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.276967\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.284197\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.281355\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.288541\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.295656\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.292699\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.289772\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.276875\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.284106\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.281265\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.288452\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.275568\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.262812\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.260184\n",
            "resetting env. episode 358.000000, reward total was -18.000000. running mean: -20.237582\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.245206\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.242754\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.240327\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.247923\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.245444\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.252990\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.250460\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.257955\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.265376\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.272722\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.269995\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.257295\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.254722\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.262175\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.259553\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.266957\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.254288\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.261745\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.269127\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.256436\n",
            "resetting env. episode 379.000000, reward total was -18.000000. running mean: -20.233872\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.231533\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.229218\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.226926\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.214656\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.222510\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.230285\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.227982\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.235702\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.243345\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.240911\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.248502\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.256017\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.263457\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.270823\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.268114\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.275433\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.282679\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.279852\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.277054\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.274283\n",
            "resetting env. episode 400.000000, reward total was -18.000000. running mean: -20.251540\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.249025\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.256535\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.263969\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.271330\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.268616\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.275930\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.283171\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.270339\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.277636\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.274859\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.282111\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.289290\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.276397\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.283633\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.290796\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.297888\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.294910\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.301960\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.298941\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.305951\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.302892\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.299863\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.296864\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.303896\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.310857\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.317748\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.314571\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.321425\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.328211\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.324929\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.331679\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.318363\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.305179\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.302127\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.299106\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.306115\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.313054\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.319923\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.316724\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.323557\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.310321\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.317218\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.314046\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.320905\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.317696\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.324519\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.331274\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.327961\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.314682\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.301535\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.308520\n",
            "resetting env. episode 452.000000, reward total was -18.000000. running mean: -20.285434\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.292580\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.299654\n",
            "resetting env. episode 455.000000, reward total was -17.000000. running mean: -20.266658\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.253991\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.241451\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.249037\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.246546\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.244081\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.241640\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.249224\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.256731\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.264164\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.251522\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.259007\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.266417\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.273753\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.261015\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.268405\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.265721\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.273064\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.260333\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.267730\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.265053\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.272402\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.269678\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.266981\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.274312\n",
            "resetting env. episode 480.000000, reward total was -18.000000. running mean: -20.251568\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.259053\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.266462\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.263798\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.261160\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.238548\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.246163\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.253701\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.261164\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.248552\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.246067\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.253606\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.261070\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.268459\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.255775\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.243217\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.250785\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.238277\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.235894\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.233535\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.241200\n",
            "CPU times: user 1h 25min 34s, sys: 13min 9s, total: 1h 38min 43s\n",
            "Wall time: 51min 27s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "a79b6e03-d6c7-4b80-b3a9-1e2619a35282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "b19d402f-86b9-49ab-813f-20071c61ef41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -18.000000. running mean: -20.960100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.960499\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.960894\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.961285\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.961672\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.962055\n",
            "resetting env. episode 9.000000, reward total was -18.000000. running mean: -20.932435\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.933111\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.933779\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.934442\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.935097\n",
            "resetting env. episode 14.000000, reward total was -17.000000. running mean: -20.895746\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.896789\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.887821\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.888943\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.890053\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.891153\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.882241\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.863419\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.864785\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.866137\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.867475\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -20.838801\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.830413\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.832109\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.823787\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.825550\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.817294\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.809121\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.811030\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.812920\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.804790\n",
            "resetting env. episode 35.000000, reward total was -18.000000. running mean: -20.776743\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.768975\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.761285\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.743673\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.726236\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.708973\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.711884\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.704765\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.697717\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.680740\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.673933\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.667193\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.660521\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.663916\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.647277\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.650804\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.634296\n",
            "resetting env. episode 52.000000, reward total was -17.000000. running mean: -20.597953\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.601974\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.605954\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.589894\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.583995\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.578156\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.582374\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.586550\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -20.570685\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.574978\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.579228\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.573436\n",
            "resetting env. episode 64.000000, reward total was -17.000000. running mean: -20.537701\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.532324\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.527001\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.521731\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.506514\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.511449\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.496334\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.501371\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.506357\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.511294\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.506181\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.511119\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.516008\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.500848\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.505839\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.500781\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.505773\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.510715\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.515608\n",
            "resetting env. episode 83.000000, reward total was -17.000000. running mean: -20.480452\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.485647\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.480791\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.475983\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.481223\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.476411\n",
            "resetting env. episode 89.000000, reward total was -18.000000. running mean: -20.451647\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.447130\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.452659\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.448133\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.453651\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.459115\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.454524\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.449978\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.445479\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.451024\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.446514\n",
            "resetting env. episode 100.000000, reward total was -14.000000. running mean: -20.382048\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.378228\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.384446\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.380601\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.376795\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.373027\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.359297\n",
            "resetting env. episode 107.000000, reward total was -17.000000. running mean: -20.325704\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.322447\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.329222\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.335930\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.342571\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.329145\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.325854\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.332595\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.329269\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -20.315977\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -20.302817\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.309789\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.316691\n",
            "resetting env. episode 120.000000, reward total was -18.000000. running mean: -20.293524\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.300589\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.307583\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.314507\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.301362\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.288348\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.285465\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.272610\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.279884\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.277085\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.274314\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.281571\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.288755\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.285868\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.293009\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.300079\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.307078\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.314008\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -20.300867\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.297859\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.284880\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.292031\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.289111\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.296220\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.283258\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.290425\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.297521\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.304546\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.311500\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.308385\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.305301\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.302248\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.299226\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.306234\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.303171\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.300140\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.297138\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.284167\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.291325\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.298412\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.295428\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.282474\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.279649\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.286852\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.293984\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.291044\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.298133\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.285152\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.292301\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.299378\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.286384\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.283520\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.280685\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.287878\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.284999\n",
            "resetting env. episode 175.000000, reward total was -17.000000. running mean: -20.252149\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.259628\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.267031\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.274361\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.271617\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.278901\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.286112\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.283251\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.290419\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.287514\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.294639\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.291693\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.288776\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.275888\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.283129\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.280298\n",
            "resetting env. episode 191.000000, reward total was -18.000000. running mean: -20.257495\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.264920\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.262271\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.259648\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.267052\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.274381\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.271637\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.278921\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.276132\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.273371\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.270637\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.267930\n",
            "resetting env. episode 203.000000, reward total was -18.000000. running mean: -20.245251\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.252799\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.250271\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.237768\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.245390\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.232936\n",
            "resetting env. episode 209.000000, reward total was -18.000000. running mean: -20.210607\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.218501\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.226316\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.224053\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.221812\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.229594\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.237298\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.234925\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.232576\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.240250\n",
            "resetting env. episode 219.000000, reward total was -18.000000. running mean: -20.217848\n",
            "resetting env. episode 220.000000, reward total was -17.000000. running mean: -20.185669\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.193813\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.191874\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.199956\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.207956\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.205877\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.203818\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.211780\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.219662\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.227465\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.215191\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.213039\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.220908\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.218699\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.216512\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.214347\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.212204\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.200082\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.198081\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.196100\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.194139\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.192198\n",
            "resetting env. episode 242.000000, reward total was -18.000000. running mean: -20.170276\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.178573\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.176787\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.185019\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.173169\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.161437\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.169823\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.158125\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.166543\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.174878\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.163129\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.151498\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.159983\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.168383\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.176699\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.184932\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.183083\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.181252\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.169440\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.167745\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.166068\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.164407\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.172763\n",
            "resetting env. episode 265.000000, reward total was -18.000000. running mean: -20.151035\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.159525\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.167930\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.176251\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.174488\n",
            "resetting env. episode 270.000000, reward total was -18.000000. running mean: -20.152743\n",
            "resetting env. episode 271.000000, reward total was -18.000000. running mean: -20.131216\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.129904\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.138604\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.147218\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.135746\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.134389\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.143045\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.141614\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.140198\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.148796\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.147308\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.145835\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.154377\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.162833\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.161205\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.159593\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.167997\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.176317\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.184554\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.192708\n",
            "resetting env. episode 291.000000, reward total was -18.000000. running mean: -20.170781\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.179073\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.187283\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.185410\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.183556\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.171720\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.170003\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.178303\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.166520\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.164855\n",
            "resetting env. episode 301.000000, reward total was -17.000000. running mean: -20.133206\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.121874\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.110655\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.119549\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.128353\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.137070\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.135699\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.144342\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.142899\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.141470\n",
            "resetting env. episode 311.000000, reward total was -17.000000. running mean: -20.110055\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.108954\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.117865\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.126686\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.125419\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.134165\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.132823\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.121495\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.130280\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.138977\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.137588\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.136212\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.144850\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.153401\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.151867\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.150349\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.138845\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.137457\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.146082\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.144621\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.143175\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.151743\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.150226\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.138724\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.147336\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.155863\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.164304\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.152661\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.141135\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.129723\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.138426\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.127042\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.125771\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.124514\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.133269\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.141936\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.130517\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.129211\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.137919\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.146540\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.135075\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.123724\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.132487\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.141162\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.139750\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.138353\n",
            "resetting env. episode 357.000000, reward total was -18.000000. running mean: -20.116969\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.125799\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.134541\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.143196\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.141764\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.140346\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.138943\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.147554\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.156078\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.154517\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.162972\n",
            "resetting env. episode 368.000000, reward total was -17.000000. running mean: -20.131342\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.120029\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.128829\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.137540\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.126165\n",
            "resetting env. episode 373.000000, reward total was -17.000000. running mean: -20.094903\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.093954\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.093015\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.102085\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.111064\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.119953\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.128754\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.137466\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.146091\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.134630\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.123284\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.112051\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.100931\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.089921\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -20.069022\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.058332\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.067749\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.067071\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.076401\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.085637\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.074780\n",
            "resetting env. episode 394.000000, reward total was -18.000000. running mean: -20.054032\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.063492\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.072857\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.062129\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.051507\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.050992\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.040482\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.040077\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.029677\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.029380\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.029086\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.038795\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.028407\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.028123\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.017842\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.017664\n",
            "resetting env. episode 410.000000, reward total was -18.000000. running mean: -19.997487\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -19.997512\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -19.997537\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.007562\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.017486\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.007311\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.007238\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.007166\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.007094\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.017023\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.006853\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -19.996784\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -19.986816\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -19.996948\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -19.996979\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.007009\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.016939\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.016769\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.016602\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.006436\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -19.996371\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -19.986408\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -19.986544\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -19.986678\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -19.996811\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.006843\n",
            "resetting env. episode 436.000000, reward total was -18.000000. running mean: -19.986775\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -19.976907\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -19.977138\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -19.987367\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -19.987493\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -19.987618\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -19.997742\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.007764\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.017687\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.007510\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.017435\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.027261\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.036988\n",
            "resetting env. episode 449.000000, reward total was -17.000000. running mean: -20.006618\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -19.996552\n",
            "resetting env. episode 451.000000, reward total was -18.000000. running mean: -19.976586\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -19.986820\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -19.996952\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -19.996983\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.007013\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.006943\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -19.996873\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -19.996905\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -19.996936\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -19.996966\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.006997\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -19.996927\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.006957\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.006888\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.016819\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.016651\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.026484\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.036219\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.025857\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.025599\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.035343\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.044989\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.054539\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.063994\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.073354\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.082620\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.081794\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -20.070976\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.070266\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.069564\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.078868\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.088079\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.097199\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.106227\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.095164\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.104213\n",
            "resetting env. episode 487.000000, reward total was -17.000000. running mean: -20.073171\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.062439\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.071815\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.081096\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.070285\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.079583\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.078787\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.087999\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.097119\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.096148\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.105186\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.114134\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.112993\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.121863\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.120644\n",
            "resetting env. episode 502.000000, reward total was -19.000000. running mean: -20.109438\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.108344\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.107260\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.116188\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.125026\n",
            "resetting env. episode 507.000000, reward total was -19.000000. running mean: -20.113775\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.122638\n",
            "resetting env. episode 509.000000, reward total was -17.000000. running mean: -20.091411\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.100497\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.099492\n",
            "resetting env. episode 512.000000, reward total was -18.000000. running mean: -20.078497\n",
            "resetting env. episode 513.000000, reward total was -19.000000. running mean: -20.067712\n",
            "resetting env. episode 514.000000, reward total was -19.000000. running mean: -20.057035\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.066465\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.075800\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.085042\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.094192\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.093250\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.092317\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.101394\n",
            "resetting env. episode 522.000000, reward total was -19.000000. running mean: -20.090380\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.089476\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.088582\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.097696\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.096719\n",
            "resetting env. episode 527.000000, reward total was -19.000000. running mean: -20.085752\n",
            "resetting env. episode 528.000000, reward total was -19.000000. running mean: -20.074894\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.084145\n",
            "resetting env. episode 530.000000, reward total was -19.000000. running mean: -20.073304\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.072571\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.071845\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.081127\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.090315\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.089412\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.088518\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.097633\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.096657\n",
            "resetting env. episode 539.000000, reward total was -17.000000. running mean: -20.065690\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.075033\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.074283\n",
            "resetting env. episode 542.000000, reward total was -18.000000. running mean: -20.053540\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.063005\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.062375\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.071751\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.071033\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.080323\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.079520\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.088725\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.097837\n",
            "resetting env. episode 551.000000, reward total was -19.000000. running mean: -20.086859\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.085990\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.085130\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.084279\n",
            "resetting env. episode 555.000000, reward total was -18.000000. running mean: -20.063436\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.062802\n",
            "resetting env. episode 557.000000, reward total was -17.000000. running mean: -20.032174\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.041852\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -20.041434\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.051019\n",
            "resetting env. episode 561.000000, reward total was -17.000000. running mean: -20.020509\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.030304\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.040001\n",
            "resetting env. episode 564.000000, reward total was -18.000000. running mean: -20.019601\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.029405\n",
            "resetting env. episode 566.000000, reward total was -19.000000. running mean: -20.019111\n",
            "resetting env. episode 567.000000, reward total was -18.000000. running mean: -19.998920\n",
            "resetting env. episode 568.000000, reward total was -19.000000. running mean: -19.988931\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -19.999041\n",
            "resetting env. episode 570.000000, reward total was -20.000000. running mean: -19.999051\n",
            "resetting env. episode 571.000000, reward total was -18.000000. running mean: -19.979060\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -19.989270\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -19.989377\n",
            "resetting env. episode 574.000000, reward total was -19.000000. running mean: -19.979483\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -19.979688\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -19.979892\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -19.980093\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -19.990292\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -19.990389\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -19.990485\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.000580\n",
            "resetting env. episode 582.000000, reward total was -17.000000. running mean: -19.970574\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -19.980869\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -19.981060\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -19.991249\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.001337\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.001323\n",
            "resetting env. episode 588.000000, reward total was -20.000000. running mean: -20.001310\n",
            "resetting env. episode 589.000000, reward total was -19.000000. running mean: -19.991297\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.001384\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.011370\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.021257\n",
            "resetting env. episode 593.000000, reward total was -19.000000. running mean: -20.011044\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.010934\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.020824\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.030616\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -20.020310\n",
            "resetting env. episode 598.000000, reward total was -19.000000. running mean: -20.010107\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.020006\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.029806\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.029508\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.039212\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.038820\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.048432\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.057948\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.067368\n",
            "resetting env. episode 607.000000, reward total was -19.000000. running mean: -20.056695\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.056128\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.055566\n",
            "resetting env. episode 610.000000, reward total was -19.000000. running mean: -20.045011\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.044561\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.034115\n",
            "resetting env. episode 613.000000, reward total was -19.000000. running mean: -20.023774\n",
            "resetting env. episode 614.000000, reward total was -19.000000. running mean: -20.013536\n",
            "resetting env. episode 615.000000, reward total was -18.000000. running mean: -19.993401\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -19.993467\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.003532\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.003497\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.003462\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.003427\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.003393\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.003359\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.013325\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.023192\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.022960\n",
            "resetting env. episode 626.000000, reward total was -20.000000. running mean: -20.022731\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -20.012503\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.022378\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.032155\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -20.021833\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.031615\n",
            "resetting env. episode 632.000000, reward total was -18.000000. running mean: -20.011299\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.011186\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.011074\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.020963\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.020753\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.030546\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.030240\n",
            "resetting env. episode 639.000000, reward total was -19.000000. running mean: -20.019938\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.019739\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.029541\n",
            "resetting env. episode 642.000000, reward total was -19.000000. running mean: -20.019246\n",
            "resetting env. episode 643.000000, reward total was -19.000000. running mean: -20.009053\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.008963\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.008873\n",
            "resetting env. episode 646.000000, reward total was -19.000000. running mean: -19.998784\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.008797\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -20.008709\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.018621\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.018435\n",
            "resetting env. episode 651.000000, reward total was -19.000000. running mean: -20.008251\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.008168\n",
            "resetting env. episode 653.000000, reward total was -18.000000. running mean: -19.988087\n",
            "resetting env. episode 654.000000, reward total was -17.000000. running mean: -19.958206\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -19.948624\n",
            "resetting env. episode 656.000000, reward total was -19.000000. running mean: -19.939138\n",
            "resetting env. episode 657.000000, reward total was -18.000000. running mean: -19.919746\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -19.920549\n",
            "resetting env. episode 659.000000, reward total was -19.000000. running mean: -19.911343\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -19.912230\n",
            "resetting env. episode 661.000000, reward total was -19.000000. running mean: -19.903108\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -19.904076\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -19.905036\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -19.915985\n",
            "resetting env. episode 665.000000, reward total was -18.000000. running mean: -19.896825\n",
            "resetting env. episode 666.000000, reward total was -19.000000. running mean: -19.887857\n",
            "resetting env. episode 667.000000, reward total was -17.000000. running mean: -19.858979\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -19.860389\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -19.861785\n",
            "resetting env. episode 670.000000, reward total was -18.000000. running mean: -19.843167\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -19.854735\n",
            "resetting env. episode 672.000000, reward total was -19.000000. running mean: -19.846188\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -19.857726\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -19.859149\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -19.860557\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -19.861952\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -19.863332\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -19.874699\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -19.875952\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -19.887193\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -19.888321\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -19.899437\n",
            "resetting env. episode 683.000000, reward total was -19.000000. running mean: -19.890443\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -19.891539\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -19.882623\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -19.883797\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -19.884959\n",
            "resetting env. episode 688.000000, reward total was -19.000000. running mean: -19.876109\n",
            "resetting env. episode 689.000000, reward total was -20.000000. running mean: -19.877348\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -19.878575\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -19.879789\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -19.890991\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -19.902081\n",
            "resetting env. episode 694.000000, reward total was -19.000000. running mean: -19.893060\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -19.884130\n",
            "resetting env. episode 696.000000, reward total was -19.000000. running mean: -19.875289\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -19.876536\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -19.877770\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -19.888993\n",
            "resetting env. episode 700.000000, reward total was -19.000000. running mean: -19.880103\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -19.881302\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -19.892489\n",
            "resetting env. episode 703.000000, reward total was -19.000000. running mean: -19.883564\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -19.894728\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -19.895781\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -19.886823\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -19.897955\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -19.898975\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -19.909986\n",
            "resetting env. episode 710.000000, reward total was -18.000000. running mean: -19.890886\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -19.881977\n",
            "resetting env. episode 712.000000, reward total was -19.000000. running mean: -19.873157\n",
            "resetting env. episode 713.000000, reward total was -18.000000. running mean: -19.854425\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -19.855881\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -19.857322\n",
            "resetting env. episode 716.000000, reward total was -16.000000. running mean: -19.818749\n",
            "resetting env. episode 717.000000, reward total was -15.000000. running mean: -19.770562\n",
            "resetting env. episode 718.000000, reward total was -18.000000. running mean: -19.752856\n",
            "resetting env. episode 719.000000, reward total was -18.000000. running mean: -19.735327\n",
            "resetting env. episode 720.000000, reward total was -19.000000. running mean: -19.727974\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -19.730694\n",
            "resetting env. episode 722.000000, reward total was -19.000000. running mean: -19.723388\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -19.736154\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -19.748792\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -19.751304\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -19.753791\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -19.766253\n",
            "resetting env. episode 728.000000, reward total was -19.000000. running mean: -19.758591\n",
            "resetting env. episode 729.000000, reward total was -19.000000. running mean: -19.751005\n",
            "resetting env. episode 730.000000, reward total was -18.000000. running mean: -19.733495\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -19.746160\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -19.748698\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -19.761211\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -19.773599\n",
            "resetting env. episode 735.000000, reward total was -18.000000. running mean: -19.755863\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -19.758304\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -19.760721\n",
            "resetting env. episode 738.000000, reward total was -16.000000. running mean: -19.723114\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -19.725883\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -19.728624\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -19.741338\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -19.753925\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -19.746385\n",
            "resetting env. episode 744.000000, reward total was -19.000000. running mean: -19.738922\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -19.751532\n",
            "resetting env. episode 746.000000, reward total was -19.000000. running mean: -19.744017\n",
            "resetting env. episode 747.000000, reward total was -18.000000. running mean: -19.726577\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -19.739311\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -19.751918\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -19.754399\n",
            "resetting env. episode 751.000000, reward total was -18.000000. running mean: -19.736855\n",
            "resetting env. episode 752.000000, reward total was -17.000000. running mean: -19.709486\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -19.722391\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -19.735167\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -19.727816\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -19.740538\n",
            "resetting env. episode 757.000000, reward total was -18.000000. running mean: -19.723132\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -19.725901\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -19.738642\n",
            "resetting env. episode 760.000000, reward total was -18.000000. running mean: -19.721256\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -19.734043\n",
            "resetting env. episode 762.000000, reward total was -19.000000. running mean: -19.726703\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -19.719435\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -19.722241\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -19.715019\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -19.727869\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -19.740590\n",
            "resetting env. episode 768.000000, reward total was -19.000000. running mean: -19.733184\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -19.745852\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -19.758394\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -19.770810\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -19.783102\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -19.795271\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -19.797318\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -19.799345\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -19.811351\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -19.823238\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -19.825005\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -19.816755\n",
            "resetting env. episode 780.000000, reward total was -19.000000. running mean: -19.808588\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -19.820502\n",
            "resetting env. episode 782.000000, reward total was -19.000000. running mean: -19.812297\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -19.814174\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -19.826032\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -19.817772\n",
            "resetting env. episode 786.000000, reward total was -16.000000. running mean: -19.779594\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -19.781798\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -19.793980\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -19.796040\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -19.798080\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -19.790099\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -19.802198\n",
            "resetting env. episode 793.000000, reward total was -20.000000. running mean: -19.804176\n",
            "resetting env. episode 794.000000, reward total was -18.000000. running mean: -19.786134\n",
            "resetting env. episode 795.000000, reward total was -19.000000. running mean: -19.778273\n",
            "resetting env. episode 796.000000, reward total was -18.000000. running mean: -19.760490\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -19.772885\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -19.765157\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -19.767505\n",
            "resetting env. episode 800.000000, reward total was -20.000000. running mean: -19.769830\n",
            "resetting env. episode 801.000000, reward total was -19.000000. running mean: -19.762132\n",
            "resetting env. episode 802.000000, reward total was -16.000000. running mean: -19.724510\n",
            "resetting env. episode 803.000000, reward total was -19.000000. running mean: -19.717265\n",
            "resetting env. episode 804.000000, reward total was -19.000000. running mean: -19.710093\n",
            "resetting env. episode 805.000000, reward total was -18.000000. running mean: -19.692992\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -19.706062\n",
            "resetting env. episode 807.000000, reward total was -19.000000. running mean: -19.699001\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -19.712011\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -19.724891\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -19.727642\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -19.720366\n",
            "resetting env. episode 812.000000, reward total was -16.000000. running mean: -19.683162\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -19.696330\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -19.689367\n",
            "resetting env. episode 815.000000, reward total was -19.000000. running mean: -19.682473\n",
            "resetting env. episode 816.000000, reward total was -18.000000. running mean: -19.665649\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -19.678992\n",
            "resetting env. episode 818.000000, reward total was -18.000000. running mean: -19.662202\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -19.675580\n",
            "resetting env. episode 820.000000, reward total was -19.000000. running mean: -19.668824\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -19.682136\n",
            "resetting env. episode 822.000000, reward total was -19.000000. running mean: -19.675315\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -19.688562\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -19.701676\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -19.704659\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -19.707613\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -19.720537\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -19.733331\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -19.735998\n",
            "resetting env. episode 830.000000, reward total was -18.000000. running mean: -19.718638\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -19.731452\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -19.734137\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -19.746796\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -19.749328\n",
            "resetting env. episode 835.000000, reward total was -18.000000. running mean: -19.731834\n",
            "resetting env. episode 836.000000, reward total was -19.000000. running mean: -19.724516\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -19.737271\n",
            "resetting env. episode 838.000000, reward total was -20.000000. running mean: -19.739898\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -19.742499\n",
            "resetting env. episode 840.000000, reward total was -18.000000. running mean: -19.725074\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -19.727824\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -19.740545\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -19.753140\n",
            "resetting env. episode 844.000000, reward total was -19.000000. running mean: -19.745608\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -19.758152\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -19.760571\n",
            "resetting env. episode 847.000000, reward total was -17.000000. running mean: -19.732965\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -19.745635\n",
            "resetting env. episode 849.000000, reward total was -18.000000. running mean: -19.728179\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -19.730897\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -19.733588\n",
            "resetting env. episode 852.000000, reward total was -19.000000. running mean: -19.726252\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -19.738990\n",
            "resetting env. episode 854.000000, reward total was -19.000000. running mean: -19.731600\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -19.744284\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -19.756841\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -19.769273\n",
            "resetting env. episode 858.000000, reward total was -18.000000. running mean: -19.751580\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -19.754064\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -19.766524\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -19.768858\n",
            "resetting env. episode 862.000000, reward total was -19.000000. running mean: -19.761170\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -19.773558\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -19.765823\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -19.778164\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -19.780383\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -19.782579\n",
            "resetting env. episode 868.000000, reward total was -18.000000. running mean: -19.764753\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -19.777106\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -19.789334\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -19.801441\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -19.813427\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -19.825292\n",
            "resetting env. episode 874.000000, reward total was -19.000000. running mean: -19.817040\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -19.828869\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -19.840580\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -19.842175\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -19.853753\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -19.855215\n",
            "resetting env. episode 880.000000, reward total was -19.000000. running mean: -19.846663\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -19.848197\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -19.859715\n",
            "resetting env. episode 883.000000, reward total was -18.000000. running mean: -19.841117\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -19.852706\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -19.844179\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -19.855737\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -19.857180\n",
            "resetting env. episode 888.000000, reward total was -19.000000. running mean: -19.848608\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -19.840122\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -19.851721\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -19.843204\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -19.844772\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -19.856324\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -19.867761\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -19.869083\n",
            "resetting env. episode 896.000000, reward total was -19.000000. running mean: -19.860392\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -19.871788\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -19.873070\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -19.884340\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -19.885496\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -19.896641\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -19.887675\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -19.878798\n",
            "resetting env. episode 904.000000, reward total was -19.000000. running mean: -19.870010\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -19.881310\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -19.872497\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -19.883772\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -19.874934\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -19.876185\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -19.887423\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -19.898549\n",
            "resetting env. episode 912.000000, reward total was -19.000000. running mean: -19.889563\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -19.880668\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -19.891861\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -19.892943\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -19.904013\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -19.904973\n",
            "resetting env. episode 918.000000, reward total was -19.000000. running mean: -19.895923\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -19.886964\n",
            "resetting env. episode 920.000000, reward total was -19.000000. running mean: -19.878094\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -19.879313\n",
            "resetting env. episode 922.000000, reward total was -18.000000. running mean: -19.860520\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -19.871915\n",
            "resetting env. episode 924.000000, reward total was -19.000000. running mean: -19.863196\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -19.864564\n",
            "resetting env. episode 926.000000, reward total was -17.000000. running mean: -19.835918\n",
            "resetting env. episode 927.000000, reward total was -18.000000. running mean: -19.817559\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -19.809384\n",
            "resetting env. episode 929.000000, reward total was -19.000000. running mean: -19.801290\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -19.803277\n",
            "resetting env. episode 931.000000, reward total was -19.000000. running mean: -19.795244\n",
            "resetting env. episode 932.000000, reward total was -18.000000. running mean: -19.777292\n",
            "resetting env. episode 933.000000, reward total was -20.000000. running mean: -19.779519\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -19.771724\n",
            "resetting env. episode 935.000000, reward total was -19.000000. running mean: -19.764006\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -19.776366\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -19.778603\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -19.790817\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -19.792908\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -19.784979\n",
            "resetting env. episode 941.000000, reward total was -19.000000. running mean: -19.777129\n",
            "resetting env. episode 942.000000, reward total was -19.000000. running mean: -19.769358\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -19.761665\n",
            "resetting env. episode 944.000000, reward total was -17.000000. running mean: -19.734048\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -19.736707\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -19.749340\n",
            "resetting env. episode 947.000000, reward total was -18.000000. running mean: -19.731847\n",
            "resetting env. episode 948.000000, reward total was -17.000000. running mean: -19.704529\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -19.717483\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -19.730308\n",
            "resetting env. episode 951.000000, reward total was -19.000000. running mean: -19.723005\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -19.715775\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -19.728618\n",
            "resetting env. episode 954.000000, reward total was -19.000000. running mean: -19.721331\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -19.734118\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -19.746777\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -19.759309\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -19.751716\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -19.764199\n",
            "resetting env. episode 960.000000, reward total was -19.000000. running mean: -19.756557\n",
            "resetting env. episode 961.000000, reward total was -18.000000. running mean: -19.738991\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -19.741601\n",
            "resetting env. episode 963.000000, reward total was -15.000000. running mean: -19.694185\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -19.687244\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -19.690371\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -19.703467\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -19.706433\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -19.709368\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -19.712275\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -19.715152\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -19.728000\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -19.720720\n",
            "resetting env. episode 973.000000, reward total was -18.000000. running mean: -19.703513\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -19.696478\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -19.689513\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -19.692618\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -19.695692\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -19.698735\n",
            "resetting env. episode 979.000000, reward total was -19.000000. running mean: -19.691748\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -19.694830\n",
            "resetting env. episode 981.000000, reward total was -18.000000. running mean: -19.677882\n",
            "resetting env. episode 982.000000, reward total was -19.000000. running mean: -19.671103\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -19.674392\n",
            "resetting env. episode 984.000000, reward total was -16.000000. running mean: -19.637648\n",
            "resetting env. episode 985.000000, reward total was -19.000000. running mean: -19.631272\n",
            "resetting env. episode 986.000000, reward total was -18.000000. running mean: -19.614959\n",
            "resetting env. episode 987.000000, reward total was -16.000000. running mean: -19.578809\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -19.593021\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -19.607091\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -19.611020\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -19.614910\n",
            "resetting env. episode 992.000000, reward total was -17.000000. running mean: -19.588761\n",
            "resetting env. episode 993.000000, reward total was -18.000000. running mean: -19.572873\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -19.577145\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -19.581373\n",
            "resetting env. episode 996.000000, reward total was -19.000000. running mean: -19.575559\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -19.569804\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -19.584106\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -19.598265\n",
            "resetting env. episode 1000.000000, reward total was -19.000000. running mean: -19.592282\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -19.596359\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -19.590396\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -19.604492\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -19.618447\n",
            "resetting env. episode 1005.000000, reward total was -19.000000. running mean: -19.612262\n",
            "resetting env. episode 1006.000000, reward total was -19.000000. running mean: -19.606140\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -19.620078\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -19.623877\n",
            "resetting env. episode 1009.000000, reward total was -18.000000. running mean: -19.607639\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -19.621562\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -19.625347\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -19.639093\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -19.652702\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -19.656175\n",
            "resetting env. episode 1015.000000, reward total was -18.000000. running mean: -19.639613\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -19.643217\n",
            "resetting env. episode 1017.000000, reward total was -19.000000. running mean: -19.636785\n",
            "resetting env. episode 1018.000000, reward total was -18.000000. running mean: -19.620417\n",
            "resetting env. episode 1019.000000, reward total was -19.000000. running mean: -19.614213\n",
            "resetting env. episode 1020.000000, reward total was -18.000000. running mean: -19.598071\n",
            "resetting env. episode 1021.000000, reward total was -19.000000. running mean: -19.592090\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -19.596169\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -19.610208\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -19.624106\n",
            "resetting env. episode 1025.000000, reward total was -13.000000. running mean: -19.557865\n",
            "resetting env. episode 1026.000000, reward total was -19.000000. running mean: -19.552286\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -19.566763\n",
            "resetting env. episode 1028.000000, reward total was -20.000000. running mean: -19.571095\n",
            "resetting env. episode 1029.000000, reward total was -17.000000. running mean: -19.545384\n",
            "resetting env. episode 1030.000000, reward total was -19.000000. running mean: -19.539931\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -19.554531\n",
            "resetting env. episode 1032.000000, reward total was -18.000000. running mean: -19.538986\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -19.553596\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -19.558060\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -19.572480\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -19.586755\n",
            "resetting env. episode 1037.000000, reward total was -19.000000. running mean: -19.580887\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -19.585078\n",
            "resetting env. episode 1039.000000, reward total was -19.000000. running mean: -19.579228\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -19.593435\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -19.587501\n",
            "resetting env. episode 1042.000000, reward total was -18.000000. running mean: -19.571626\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -19.585910\n",
            "resetting env. episode 1044.000000, reward total was -18.000000. running mean: -19.570051\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -19.584350\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -19.598507\n",
            "resetting env. episode 1047.000000, reward total was -19.000000. running mean: -19.592522\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -19.596596\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -19.610630\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -19.624524\n",
            "resetting env. episode 1051.000000, reward total was -18.000000. running mean: -19.608279\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -19.622196\n",
            "resetting env. episode 1053.000000, reward total was -18.000000. running mean: -19.605974\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -19.619914\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -19.613715\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -19.617578\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -19.611402\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -19.625288\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -19.629035\n",
            "resetting env. episode 1060.000000, reward total was -19.000000. running mean: -19.622745\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -19.636518\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -19.650152\n",
            "resetting env. episode 1063.000000, reward total was -18.000000. running mean: -19.633651\n",
            "resetting env. episode 1064.000000, reward total was -20.000000. running mean: -19.637314\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -19.640941\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -19.644532\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -19.648086\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -19.651606\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -19.665090\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -19.668439\n",
            "resetting env. episode 1071.000000, reward total was -17.000000. running mean: -19.641754\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -19.645337\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -19.648883\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -19.662395\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -19.665771\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -19.659113\n",
            "resetting env. episode 1077.000000, reward total was -17.000000. running mean: -19.632522\n",
            "resetting env. episode 1078.000000, reward total was -17.000000. running mean: -19.606197\n",
            "resetting env. episode 1079.000000, reward total was -17.000000. running mean: -19.580135\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -19.574333\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -19.568590\n",
            "resetting env. episode 1082.000000, reward total was -19.000000. running mean: -19.562904\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -19.567275\n",
            "resetting env. episode 1084.000000, reward total was -20.000000. running mean: -19.571602\n",
            "resetting env. episode 1085.000000, reward total was -18.000000. running mean: -19.555886\n",
            "resetting env. episode 1086.000000, reward total was -19.000000. running mean: -19.550327\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -19.554824\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -19.569276\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -19.573583\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -19.577847\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -19.592069\n",
            "resetting env. episode 1092.000000, reward total was -17.000000. running mean: -19.566148\n",
            "resetting env. episode 1093.000000, reward total was -19.000000. running mean: -19.560487\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -19.564882\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.579233\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -19.593441\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -19.607506\n",
            "resetting env. episode 1098.000000, reward total was -18.000000. running mean: -19.591431\n",
            "resetting env. episode 1099.000000, reward total was -19.000000. running mean: -19.585517\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -19.599662\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -19.613665\n",
            "resetting env. episode 1102.000000, reward total was -19.000000. running mean: -19.607528\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -19.601453\n",
            "resetting env. episode 1104.000000, reward total was -18.000000. running mean: -19.585439\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -19.589584\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -19.603688\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -19.617651\n",
            "resetting env. episode 1108.000000, reward total was -17.000000. running mean: -19.591475\n",
            "resetting env. episode 1109.000000, reward total was -18.000000. running mean: -19.575560\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -19.569805\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -19.564107\n",
            "resetting env. episode 1112.000000, reward total was -19.000000. running mean: -19.558465\n",
            "resetting env. episode 1113.000000, reward total was -19.000000. running mean: -19.552881\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -19.557352\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -19.561778\n",
            "resetting env. episode 1116.000000, reward total was -18.000000. running mean: -19.546161\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -19.560699\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -19.575092\n",
            "resetting env. episode 1119.000000, reward total was -19.000000. running mean: -19.569341\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -19.583648\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -19.587811\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -19.601933\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -19.615914\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -19.619755\n",
            "resetting env. episode 1125.000000, reward total was -19.000000. running mean: -19.613557\n",
            "resetting env. episode 1126.000000, reward total was -19.000000. running mean: -19.607422\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -19.621347\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -19.615134\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -19.618983\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -19.612793\n",
            "resetting env. episode 1131.000000, reward total was -17.000000. running mean: -19.586665\n",
            "resetting env. episode 1132.000000, reward total was -13.000000. running mean: -19.520798\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -19.535590\n",
            "resetting env. episode 1134.000000, reward total was -17.000000. running mean: -19.510234\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -19.515132\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -19.519981\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -19.534781\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -19.549433\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -19.543939\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -19.558499\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -19.562914\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -19.577285\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -19.591512\n",
            "resetting env. episode 1144.000000, reward total was -15.000000. running mean: -19.545597\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -19.560141\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -19.564540\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -19.578894\n",
            "resetting env. episode 1148.000000, reward total was -19.000000. running mean: -19.573105\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -19.577374\n",
            "resetting env. episode 1150.000000, reward total was -19.000000. running mean: -19.571601\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -19.585885\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -19.590026\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -19.594126\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -19.608184\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -19.622102\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -19.635881\n",
            "resetting env. episode 1157.000000, reward total was -16.000000. running mean: -19.599523\n",
            "resetting env. episode 1158.000000, reward total was -16.000000. running mean: -19.563527\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -19.567892\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -19.572213\n",
            "resetting env. episode 1161.000000, reward total was -18.000000. running mean: -19.556491\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -19.550926\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -19.565417\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -19.579763\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -19.583965\n",
            "resetting env. episode 1166.000000, reward total was -19.000000. running mean: -19.578125\n",
            "resetting env. episode 1167.000000, reward total was -20.000000. running mean: -19.582344\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -19.586521\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -19.590656\n",
            "resetting env. episode 1170.000000, reward total was -19.000000. running mean: -19.584749\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -19.588901\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -19.603012\n",
            "resetting env. episode 1173.000000, reward total was -15.000000. running mean: -19.556982\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -19.571413\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -19.575698\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -19.579941\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -19.594142\n",
            "resetting env. episode 1178.000000, reward total was -18.000000. running mean: -19.578201\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -19.592419\n",
            "resetting env. episode 1180.000000, reward total was -20.000000. running mean: -19.596494\n",
            "resetting env. episode 1181.000000, reward total was -19.000000. running mean: -19.590529\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -19.594624\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -19.608678\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -19.622591\n",
            "resetting env. episode 1185.000000, reward total was -18.000000. running mean: -19.606365\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -19.620302\n",
            "resetting env. episode 1187.000000, reward total was -17.000000. running mean: -19.594099\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -19.598158\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -19.612176\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -19.606054\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -19.609994\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -19.623894\n",
            "resetting env. episode 1193.000000, reward total was -19.000000. running mean: -19.617655\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -19.621478\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -19.635263\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -19.638911\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -19.652522\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -19.655997\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -19.659437\n",
            "resetting env. episode 1200.000000, reward total was -19.000000. running mean: -19.652842\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -19.656314\n",
            "resetting env. episode 1202.000000, reward total was -19.000000. running mean: -19.649751\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -19.643253\n",
            "resetting env. episode 1204.000000, reward total was -17.000000. running mean: -19.616821\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -19.610652\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -19.624546\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -19.618300\n",
            "resetting env. episode 1208.000000, reward total was -17.000000. running mean: -19.592117\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -19.606196\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -19.620134\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -19.613933\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -19.607794\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -19.611716\n",
            "resetting env. episode 1214.000000, reward total was -19.000000. running mean: -19.605598\n",
            "resetting env. episode 1215.000000, reward total was -17.000000. running mean: -19.579543\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -19.583747\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -19.597910\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -19.601931\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -19.605911\n",
            "resetting env. episode 1220.000000, reward total was -19.000000. running mean: -19.599852\n",
            "resetting env. episode 1221.000000, reward total was -19.000000. running mean: -19.593854\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -19.597915\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -19.601936\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -19.595917\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -19.599957\n",
            "resetting env. episode 1226.000000, reward total was -19.000000. running mean: -19.593958\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -19.598018\n",
            "resetting env. episode 1228.000000, reward total was -18.000000. running mean: -19.582038\n",
            "resetting env. episode 1229.000000, reward total was -19.000000. running mean: -19.576218\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -19.580455\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -19.574651\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -19.578904\n",
            "resetting env. episode 1233.000000, reward total was -19.000000. running mean: -19.573115\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -19.587384\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -19.601510\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -19.605495\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -19.599440\n",
            "resetting env. episode 1238.000000, reward total was -19.000000. running mean: -19.593446\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -19.597511\n",
            "resetting env. episode 1240.000000, reward total was -18.000000. running mean: -19.581536\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -19.585721\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -19.579864\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -19.594065\n",
            "resetting env. episode 1244.000000, reward total was -19.000000. running mean: -19.588124\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -19.582243\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -19.586421\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -19.590557\n",
            "resetting env. episode 1248.000000, reward total was -19.000000. running mean: -19.584651\n",
            "resetting env. episode 1249.000000, reward total was -19.000000. running mean: -19.578805\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -19.573016\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -19.587286\n",
            "resetting env. episode 1252.000000, reward total was -17.000000. running mean: -19.561413\n",
            "resetting env. episode 1253.000000, reward total was -19.000000. running mean: -19.555799\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -19.570241\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -19.574539\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -19.578794\n",
            "resetting env. episode 1257.000000, reward total was -19.000000. running mean: -19.573006\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -19.587276\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -19.591403\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -19.605489\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -19.609434\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -19.623340\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -19.617106\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -19.610935\n",
            "resetting env. episode 1265.000000, reward total was -19.000000. running mean: -19.604826\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -19.618777\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -19.622590\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -19.636364\n",
            "resetting env. episode 1269.000000, reward total was -19.000000. running mean: -19.630000\n",
            "resetting env. episode 1270.000000, reward total was -17.000000. running mean: -19.603700\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -19.617663\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -19.611487\n",
            "resetting env. episode 1273.000000, reward total was -19.000000. running mean: -19.605372\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -19.619318\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -19.613125\n",
            "resetting env. episode 1276.000000, reward total was -19.000000. running mean: -19.606993\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -19.610924\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -19.624814\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -19.628566\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -19.642281\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -19.635858\n",
            "resetting env. episode 1282.000000, reward total was -18.000000. running mean: -19.619499\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -19.613304\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -19.617171\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -19.620999\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -19.624789\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -19.618542\n",
            "resetting env. episode 1288.000000, reward total was -15.000000. running mean: -19.572356\n",
            "resetting env. episode 1289.000000, reward total was -19.000000. running mean: -19.566633\n",
            "resetting env. episode 1290.000000, reward total was -18.000000. running mean: -19.550966\n",
            "resetting env. episode 1291.000000, reward total was -19.000000. running mean: -19.545457\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -19.550002\n",
            "resetting env. episode 1293.000000, reward total was -17.000000. running mean: -19.524502\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -19.539257\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -19.553864\n",
            "resetting env. episode 1296.000000, reward total was -17.000000. running mean: -19.528326\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -19.523042\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -19.537812\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -19.532434\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -19.547110\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -19.541638\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -19.546222\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -19.550760\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -19.555252\n",
            "resetting env. episode 1305.000000, reward total was -19.000000. running mean: -19.549700\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -19.564203\n",
            "resetting env. episode 1307.000000, reward total was -19.000000. running mean: -19.558561\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -19.572975\n",
            "resetting env. episode 1309.000000, reward total was -19.000000. running mean: -19.567245\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -19.571573\n",
            "resetting env. episode 1311.000000, reward total was -19.000000. running mean: -19.565857\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -19.560199\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -19.564597\n",
            "resetting env. episode 1314.000000, reward total was -18.000000. running mean: -19.548951\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -19.553461\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -19.567927\n",
            "resetting env. episode 1317.000000, reward total was -18.000000. running mean: -19.552247\n",
            "resetting env. episode 1318.000000, reward total was -18.000000. running mean: -19.536725\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -19.551358\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -19.565844\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -19.570186\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -19.574484\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -19.578739\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -19.582951\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -19.597122\n",
            "resetting env. episode 1326.000000, reward total was -19.000000. running mean: -19.591151\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -19.595239\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -19.599287\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -19.613294\n",
            "resetting env. episode 1330.000000, reward total was -19.000000. running mean: -19.607161\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -19.611089\n",
            "resetting env. episode 1332.000000, reward total was -18.000000. running mean: -19.594979\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -19.599029\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -19.613038\n",
            "resetting env. episode 1335.000000, reward total was -19.000000. running mean: -19.606908\n",
            "resetting env. episode 1336.000000, reward total was -16.000000. running mean: -19.570839\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.575131\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -19.579379\n",
            "resetting env. episode 1339.000000, reward total was -19.000000. running mean: -19.573585\n",
            "resetting env. episode 1340.000000, reward total was -17.000000. running mean: -19.547850\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -19.562371\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -19.566747\n",
            "resetting env. episode 1343.000000, reward total was -19.000000. running mean: -19.561080\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -19.565469\n",
            "resetting env. episode 1345.000000, reward total was -20.000000. running mean: -19.569814\n",
            "resetting env. episode 1346.000000, reward total was -19.000000. running mean: -19.564116\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -19.578475\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -19.572690\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -19.586964\n",
            "resetting env. episode 1350.000000, reward total was -17.000000. running mean: -19.561094\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -19.575483\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -19.569728\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -19.584031\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -19.598191\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -19.592209\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -19.586287\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -19.590424\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -19.584519\n",
            "resetting env. episode 1359.000000, reward total was -16.000000. running mean: -19.548674\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -19.563187\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -19.567556\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -19.571880\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -19.586161\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -19.580300\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -19.584497\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -19.598652\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -19.602665\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -19.596639\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -19.600672\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -19.604665\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -19.608619\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -19.612533\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -19.616407\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -19.620243\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -19.634041\n",
            "resetting env. episode 1376.000000, reward total was -18.000000. running mean: -19.617700\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -19.621523\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -19.625308\n",
            "resetting env. episode 1379.000000, reward total was -18.000000. running mean: -19.609055\n",
            "resetting env. episode 1380.000000, reward total was -17.000000. running mean: -19.582964\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -19.587135\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -19.581263\n",
            "resetting env. episode 1383.000000, reward total was -18.000000. running mean: -19.565451\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -19.579796\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -19.583998\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -19.588158\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -19.602277\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -19.606254\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -19.620191\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -19.633990\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -19.647650\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -19.651173\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -19.664661\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -19.678015\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -19.691235\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -19.704322\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -19.707279\n",
            "resetting env. episode 1398.000000, reward total was -18.000000. running mean: -19.690206\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -19.693304\n",
            "resetting env. episode 1400.000000, reward total was -19.000000. running mean: -19.686371\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -19.699507\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -19.702512\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -19.705487\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -19.708432\n",
            "resetting env. episode 1405.000000, reward total was -18.000000. running mean: -19.691348\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -19.704435\n",
            "resetting env. episode 1407.000000, reward total was -16.000000. running mean: -19.667390\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -19.670716\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -19.674009\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -19.687269\n",
            "resetting env. episode 1411.000000, reward total was -18.000000. running mean: -19.670396\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -19.673692\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -19.676956\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -19.670186\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.673484\n",
            "resetting env. episode 1416.000000, reward total was -17.000000. running mean: -19.646749\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.650282\n",
            "resetting env. episode 1418.000000, reward total was -19.000000. running mean: -19.643779\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -19.637341\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -19.650968\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -19.654458\n",
            "resetting env. episode 1422.000000, reward total was -17.000000. running mean: -19.627914\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -19.631634\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -19.635318\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -19.638965\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -19.652575\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -19.666049\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -19.679389\n",
            "resetting env. episode 1429.000000, reward total was -19.000000. running mean: -19.672595\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -19.675869\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -19.679110\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -19.692319\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -19.705396\n",
            "resetting env. episode 1434.000000, reward total was -18.000000. running mean: -19.688342\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -19.691459\n",
            "resetting env. episode 1436.000000, reward total was -19.000000. running mean: -19.684544\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -19.687699\n",
            "resetting env. episode 1438.000000, reward total was -18.000000. running mean: -19.670822\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -19.684114\n",
            "resetting env. episode 1440.000000, reward total was -19.000000. running mean: -19.677272\n",
            "resetting env. episode 1441.000000, reward total was -19.000000. running mean: -19.670500\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -19.673795\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -19.687057\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -19.700186\n",
            "resetting env. episode 1445.000000, reward total was -19.000000. running mean: -19.693184\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -19.696252\n",
            "resetting env. episode 1447.000000, reward total was -15.000000. running mean: -19.649290\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -19.652797\n",
            "resetting env. episode 1449.000000, reward total was -19.000000. running mean: -19.646269\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -19.659806\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -19.653208\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -19.666676\n",
            "resetting env. episode 1453.000000, reward total was -17.000000. running mean: -19.640009\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -19.653609\n",
            "resetting env. episode 1455.000000, reward total was -18.000000. running mean: -19.637073\n",
            "resetting env. episode 1456.000000, reward total was -19.000000. running mean: -19.630703\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -19.624395\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -19.638152\n",
            "resetting env. episode 1459.000000, reward total was -19.000000. running mean: -19.631770\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -19.635452\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -19.639098\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -19.632707\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -19.646380\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -19.649916\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -19.653417\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -19.656883\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -19.660314\n",
            "resetting env. episode 1468.000000, reward total was -17.000000. running mean: -19.633711\n",
            "resetting env. episode 1469.000000, reward total was -16.000000. running mean: -19.597374\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -19.601400\n",
            "resetting env. episode 1471.000000, reward total was -19.000000. running mean: -19.595386\n",
            "resetting env. episode 1472.000000, reward total was -19.000000. running mean: -19.589432\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -19.583538\n",
            "resetting env. episode 1474.000000, reward total was -19.000000. running mean: -19.577702\n",
            "resetting env. episode 1475.000000, reward total was -18.000000. running mean: -19.561925\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.576306\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -19.570543\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.584838\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -19.578989\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -19.583199\n",
            "resetting env. episode 1481.000000, reward total was -19.000000. running mean: -19.577367\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -19.591594\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -19.585678\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -19.589821\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -19.603923\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -19.617883\n",
            "resetting env. episode 1487.000000, reward total was -19.000000. running mean: -19.611705\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.615588\n",
            "resetting env. episode 1489.000000, reward total was -18.000000. running mean: -19.599432\n",
            "resetting env. episode 1490.000000, reward total was -19.000000. running mean: -19.593437\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -19.597503\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -19.601528\n",
            "resetting env. episode 1493.000000, reward total was -18.000000. running mean: -19.585513\n",
            "resetting env. episode 1494.000000, reward total was -18.000000. running mean: -19.569658\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -19.583961\n",
            "resetting env. episode 1496.000000, reward total was -18.000000. running mean: -19.568121\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -19.582440\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -19.596616\n",
            "resetting env. episode 1499.000000, reward total was -19.000000. running mean: -19.590650\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -19.604743\n",
            "CPU times: user 4h 54min 34s, sys: 45min 25s, total: 5h 40min\n",
            "Wall time: 2h 58min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JLMpn5EqYZfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "a6525d59-cf56-4265-e365-3404bdb4df08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}