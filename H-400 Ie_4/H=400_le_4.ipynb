{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "d5cdd6bc-d24d-444f-9de5-0d0aaed46546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 33.3 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=71ec09898f5c7666b8da734f59b966dc3404446dddfb4e4158477a92c0a13111\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "bad6a9d5-457e-40ab-cbf0-dad177ee1ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "de6eff20-35c2-45a3-9100-f71c714970d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "06e8ff8b-6efe-4e6c-9649-e18ba3fa47dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "60d9d348-6fa9-40eb-ace6-1743f779703a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 400 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "a90f8df1-8737-4204-e2fe-5284ca2f07d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019801\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.029603\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.029307\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.019014\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.028824\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.028536\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.028250\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -20.017968\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.017788\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.007610\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -19.997534\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.007559\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.007483\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.017408\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.017234\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.027062\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.016791\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.006623\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.016557\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.026391\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.036128\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.045766\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.045309\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.054856\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.064307\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.063664\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.073027\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.082297\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.091474\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.100559\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.109554\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.118458\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.127274\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.126001\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.124741\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.123493\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.132258\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.140936\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.149527\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.138031\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.146651\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.155184\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.163633\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.171996\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.170276\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.178574\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.186788\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.184920\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.183071\n",
            "resetting env. episode 53.000000, reward total was -18.000000. running mean: -20.161240\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.169628\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.177931\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.176152\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.184391\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.192547\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.190621\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.198715\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.206728\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.214661\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.212514\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.220389\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.218185\n",
            "resetting env. episode 66.000000, reward total was -18.000000. running mean: -20.196003\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.204043\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.212003\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.219883\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.217684\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.225507\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.233252\n",
            "resetting env. episode 73.000000, reward total was -17.000000. running mean: -20.200919\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.208910\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.216821\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.224653\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.232406\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -20.210082\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.207981\n",
            "resetting env. episode 80.000000, reward total was -17.000000. running mean: -20.175902\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.184143\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.192301\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.200378\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.188374\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.196491\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.204526\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.212480\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.220356\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.228152\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.235871\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.243512\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.251077\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.248566\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.256080\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.253520\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.250984\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.258474\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.255890\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.263331\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.250698\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.258191\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.265609\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.262953\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.270323\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.277620\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.274844\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.272095\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.279374\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.286580\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.293715\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.280778\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.287970\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.285090\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.282239\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.269417\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.276723\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.283955\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.291116\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.278205\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.275423\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.282668\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.279842\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.287043\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.294173\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.281231\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.278419\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.275635\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.282878\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.280049\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.287249\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.284376\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -20.261533\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.268917\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.266228\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.263566\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -20.250930\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.258421\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.265837\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.273178\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.280447\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.287642\n",
            "resetting env. episode 142.000000, reward total was -18.000000. running mean: -20.264766\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.272118\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.279397\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.286603\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.283737\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.280900\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.288091\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.285210\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.292358\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.299434\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.296440\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.303475\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.310440\n",
            "resetting env. episode 155.000000, reward total was -17.000000. running mean: -20.277336\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.274563\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.281817\n",
            "resetting env. episode 158.000000, reward total was -18.000000. running mean: -20.258999\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.256409\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.263845\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.261206\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.268594\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.275908\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.263149\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.270518\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.277813\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.275034\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.262284\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.259661\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.267065\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.264394\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.271750\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.279033\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.266242\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.273580\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.280844\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.288036\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.285155\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.282304\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.279481\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.286686\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.273819\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.271081\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.278370\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.285586\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.282730\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.289903\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.287004\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.284134\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.281293\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.288480\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.295595\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.292639\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.299713\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.296716\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.303748\n",
            "resetting env. episode 197.000000, reward total was -18.000000. running mean: -20.280711\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.287904\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.295025\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.302075\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.299054\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.286063\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.293203\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.290271\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.287368\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.284494\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.291649\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.298733\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.285745\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.282888\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.290059\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.287158\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.294287\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.301344\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.308331\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.315247\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.322095\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.318874\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.325685\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.312428\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.319304\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.326111\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.332850\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.339521\n",
            "resetting env. episode 225.000000, reward total was -16.000000. running mean: -20.296126\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.283165\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.290333\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.297430\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.304456\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.311411\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.308297\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.315214\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.302062\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.299041\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.296051\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.303090\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.310059\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -20.296959\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.293989\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.291049\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.278139\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.285357\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.292504\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.299579\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.306583\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.313517\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.310382\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.317278\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.304105\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.311064\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.307954\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.314874\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.311725\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.308608\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.315522\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.302367\n",
            "resetting env. episode 257.000000, reward total was -18.000000. running mean: -20.279343\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.286550\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.293684\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.280747\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.287940\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.285061\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.282210\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.289388\n",
            "resetting env. episode 265.000000, reward total was -18.000000. running mean: -20.266494\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.253829\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.261291\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.268678\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.255991\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.263431\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.270797\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.278089\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.275308\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.272555\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.279829\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.287031\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.274161\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.271419\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.268705\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.276018\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.283258\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.290425\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.287521\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.294646\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.301699\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.298682\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.285695\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.272838\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.270110\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.267409\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.264735\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.262088\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.269467\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.276772\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.284004\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.271164\n",
            "resetting env. episode 297.000000, reward total was -18.000000. running mean: -20.248453\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.255968\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.263408\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.270774\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.268067\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.275386\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.272632\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.269906\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.277207\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.274435\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.271690\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.278973\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.276184\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.283422\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.290588\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.277682\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.264905\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.262256\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.269633\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.276937\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.274168\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.281426\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.288612\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.295726\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.292768\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.299841\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.296842\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.303874\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.300835\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.307827\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.314748\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.321601\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.328385\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.335101\n",
            "resetting env. episode 331.000000, reward total was -16.000000. running mean: -20.291750\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.298833\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.305844\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.312786\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.309658\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.316561\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.313396\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.320262\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.317059\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.313889\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.320750\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.317542\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.314367\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.321223\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.328011\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.334731\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.341383\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.347970\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.344490\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.351045\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.357535\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.363959\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.360320\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.366716\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.363049\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.369419\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.375725\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.381967\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.378148\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.364366\n",
            "resetting env. episode 361.000000, reward total was -17.000000. running mean: -20.330723\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.337415\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.344041\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.330601\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.327295\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.324022\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.320782\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.317574\n",
            "resetting env. episode 369.000000, reward total was -18.000000. running mean: -20.294398\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.301454\n",
            "resetting env. episode 371.000000, reward total was -18.000000. running mean: -20.278439\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.265655\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.262999\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.270369\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.277665\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.284888\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.292039\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.279119\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.286328\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.293464\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.300530\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.307525\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.314449\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.301305\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.308292\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.305209\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.312157\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.319035\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.325845\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.332586\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.329261\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.335968\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.342608\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.349182\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.355690\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.352133\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.348612\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.345126\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.351675\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.348158\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.354676\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.351130\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.357618\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.354042\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.360502\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.356897\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.343328\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.349894\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.356396\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.352832\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.359303\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.345710\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.342253\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.348831\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.355342\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.361789\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.348171\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.354689\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.361142\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.367531\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.373856\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.380117\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.376316\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.372553\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.368827\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.375139\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.381388\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.387574\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.393698\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.399761\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.405763\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.411706\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.417589\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.413413\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.409279\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.415186\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.421034\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.406824\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.412755\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.418628\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.414442\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.420297\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.416094\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.421933\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.417714\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.413537\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.409401\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.415307\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.421154\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.416943\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.412773\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.418646\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.424459\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.430215\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.435912\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.431553\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.427238\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.412965\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.418836\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.424647\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.430401\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.426097\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.431836\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.427518\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.433242\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.438910\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.444521\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.450076\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.455575\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.461019\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.466409\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.451745\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.437227\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.442855\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.438427\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.444042\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.449602\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.445106\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.440655\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.446248\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.451786\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.457268\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.462695\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.468068\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.473388\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.468654\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.463967\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.469328\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.474634\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.469888\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.465189\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.470537\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.475832\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.481073\n",
            "resetting env. episode 495.000000, reward total was -18.000000. running mean: -20.456263\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.441700\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.447283\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.452810\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.458282\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.453699\n",
            "CPU times: user 35min 48s, sys: 11min 37s, total: 47min 26s\n",
            "Wall time: 24min 52s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "a779b2eb-11d8-42da-8b31-80e95946986c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980299\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980496\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980691\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980884\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.981075\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.981265\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.971452\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.961737\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.962120\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.952499\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.952974\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.953444\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.943910\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.944471\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.935026\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.925676\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.916419\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.917255\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.908082\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.899001\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.900011\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.901011\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.902001\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.902981\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.893951\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.895012\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.876062\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.867301\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -20.838628\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.820242\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.822039\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.823819\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.815581\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.817425\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.819251\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.811058\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.802948\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.804918\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.806869\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.788800\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.770912\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.763203\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.755571\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.748015\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.730535\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.723230\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.715998\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.698838\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.701849\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.704831\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.697782\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.700805\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.693797\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.686859\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.689990\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.693090\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.696159\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.679198\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.682406\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.665582\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.668926\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.672236\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.675514\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.668759\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.672071\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.665351\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.668697\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.662010\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -20.645390\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.638936\n",
            "resetting env. episode 74.000000, reward total was -18.000000. running mean: -20.612547\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.606421\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.600357\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.604354\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -20.588310\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.592427\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.596503\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.600538\n",
            "resetting env. episode 82.000000, reward total was -18.000000. running mean: -20.574532\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.568787\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.573099\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.567368\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.571694\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.575977\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.580218\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -20.564415\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.558771\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.553184\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.557652\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.552075\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.546555\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.551089\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.555578\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.550022\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.534522\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.539177\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.543785\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.538347\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.532964\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.537634\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.542258\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.546835\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.551367\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.555853\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.550295\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.554792\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.549244\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.553751\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.548214\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.542732\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.547304\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.551831\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.546313\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.540850\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.525441\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.530187\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.524885\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.519636\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.524440\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.529196\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.533904\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.538565\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.523179\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.517947\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.522768\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.527540\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.532265\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.526942\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.531672\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.536356\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.530992\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.515682\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.510525\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -20.495420\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.490466\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.495561\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.490606\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.495700\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.490743\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.475835\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.481077\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.486266\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.491403\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.496489\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.491525\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.496609\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.491643\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.476727\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.481959\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.487140\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.482268\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.477446\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.462671\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.468045\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.473364\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.478631\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.483844\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.489006\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.484116\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.489275\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.484382\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.479538\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.484743\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.479895\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.485096\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.470245\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.475543\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.480787\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.485980\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.491120\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.486209\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.491346\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.496433\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.481469\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.466654\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.471987\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.467268\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.462595\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.467969\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.473289\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.468556\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.473871\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.469132\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.474441\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.469696\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.464999\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.460349\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.445746\n",
            "resetting env. episode 192.000000, reward total was -19.000000. running mean: -20.431288\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.426976\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.422706\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.428479\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.434194\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.439852\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.445454\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.440999\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.446589\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.452123\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.457602\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.443026\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.438596\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.424210\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.409968\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.405868\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.411809\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.407691\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.393614\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.389678\n",
            "resetting env. episode 212.000000, reward total was -18.000000. running mean: -20.365781\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.372123\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.368402\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.364718\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.371071\n",
            "resetting env. episode 217.000000, reward total was -18.000000. running mean: -20.347360\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.353887\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.360348\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.356744\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.363177\n",
            "resetting env. episode 222.000000, reward total was -18.000000. running mean: -20.339545\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.346150\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.352688\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.359161\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.355570\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.342014\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.348594\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.355108\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.361557\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.367941\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.364262\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.370619\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.376913\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.383144\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.379312\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.385519\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.391664\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.387748\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.393870\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.399931\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.405932\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.401873\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.407854\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.393775\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.399838\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.405839\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.411781\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.417663\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.423486\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.419252\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.425059\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.420808\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.426600\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.412334\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.418211\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.414029\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.399889\n",
            "resetting env. episode 259.000000, reward total was -18.000000. running mean: -20.375890\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.372131\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.368410\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.364725\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.351078\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.347567\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.354092\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.360551\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.366945\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.363276\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.349643\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.356147\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.362585\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.358959\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.365370\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.361716\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.368099\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.374418\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.370674\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.376967\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.383197\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.389365\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.395472\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.401517\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.397502\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.383527\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.369692\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.355995\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.362435\n",
            "resetting env. episode 288.000000, reward total was -18.000000. running mean: -20.338810\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.335422\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.332068\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.318747\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.325560\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.332304\n",
            "resetting env. episode 294.000000, reward total was -18.000000. running mean: -20.308981\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.315891\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.322732\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.329505\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.336210\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.332848\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.319520\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.306324\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.313261\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.300128\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.307127\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.314056\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.320915\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.327706\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.324429\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.321185\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.327973\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.324693\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.331446\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.338132\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.334751\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.341403\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.327989\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.334709\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.341362\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.347948\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.354469\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.360924\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.367315\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.353642\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.350105\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.356604\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.363038\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.359408\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.345814\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.342356\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.338932\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.345543\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.352087\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.348567\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.345081\n",
            "resetting env. episode 335.000000, reward total was -17.000000. running mean: -20.311630\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.308514\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.315429\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.322274\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.329052\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.325761\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -20.302503\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.309478\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.296384\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.293420\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.300486\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.307481\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.314406\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.321262\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.328049\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.334769\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.331421\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.338107\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.344726\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.331279\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.337966\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.344586\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.341140\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.347729\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.354252\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.360709\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.367102\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.373431\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.369697\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.366000\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.372340\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.368616\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.364930\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.361281\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.347668\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.354191\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.350649\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.357143\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.363571\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.369936\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.366236\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.372574\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.368848\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.375160\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.381408\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.387594\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.393718\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.399781\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.395783\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.401825\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.407807\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.403729\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.399692\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.385695\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.381838\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.378020\n",
            "resetting env. episode 391.000000, reward total was -18.000000. running mean: -20.354239\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.360697\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.357090\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.363519\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.359884\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.366285\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.352622\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.349096\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.345605\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.332149\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.328827\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.335539\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.332184\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.338862\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.335473\n",
            "resetting env. episode 406.000000, reward total was -18.000000. running mean: -20.312119\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.318997\n",
            "resetting env. episode 408.000000, reward total was -17.000000. running mean: -20.285807\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.292949\n",
            "resetting env. episode 410.000000, reward total was -18.000000. running mean: -20.270020\n",
            "resetting env. episode 411.000000, reward total was -18.000000. running mean: -20.247320\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.234846\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.232498\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.240173\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.247771\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.255294\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.262741\n",
            "resetting env. episode 418.000000, reward total was -18.000000. running mean: -20.240113\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.237712\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.235335\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.242982\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.240552\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.238146\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.235765\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.233407\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.231073\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.238762\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.246375\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.243911\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.231472\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.219157\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.216966\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.224796\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.212548\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.220423\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.208218\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.196136\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.194175\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.192233\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.200311\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.198308\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.186325\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.194461\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.202517\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.210491\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.218387\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.216203\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.224041\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.231800\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.239482\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.247087\n",
            "resetting env. episode 452.000000, reward total was -18.000000. running mean: -20.224617\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.222370\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.230147\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.227845\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.235567\n",
            "resetting env. episode 457.000000, reward total was -17.000000. running mean: -20.203211\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.211179\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.219067\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.206877\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.194808\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.192860\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.180931\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.189122\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.187231\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.195358\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.173405\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.171671\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.179954\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.188154\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.176273\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.164510\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.172865\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.181136\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.179325\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.187532\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.195656\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.193700\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.181763\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.169945\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.178246\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.186463\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.184599\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.192753\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.200825\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.188817\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.196929\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.204959\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.212910\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.210781\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.218673\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.226486\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.224221\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.211979\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.219859\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.227661\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.215384\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.203230\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.191198\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.199286\n",
            "CPU times: user 36min 17s, sys: 11min 46s, total: 48min 4s\n",
            "Wall time: 25min 5s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "10a84bfe-08e6-4e92-c330-a491692ed565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "a76fd45f-00b8-4542-fa77-01b3f7bc2510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.970398\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -20.940694\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.941287\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.941874\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.932455\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.933131\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.913800\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -20.894662\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.885715\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.866858\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.848189\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.849707\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.851210\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.852698\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.854171\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.855629\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.857073\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.858502\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -20.839917\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.841518\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.833103\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.824772\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.816524\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.818359\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.820176\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.821974\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.823754\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.825516\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.807261\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.809189\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.801097\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.803086\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.805055\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.807004\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.808934\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.810845\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.812737\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.804609\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.796563\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.788598\n",
            "resetting env. episode 44.000000, reward total was -19.000000. running mean: -20.770712\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.753004\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.745474\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.748020\n",
            "resetting env. episode 48.000000, reward total was -18.000000. running mean: -20.720539\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.713334\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.696201\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.699239\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -20.672246\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.675524\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.678769\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.681981\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.685161\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.688309\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.671426\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.654712\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -20.638165\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.631783\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.625466\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.629211\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.622919\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.616690\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.620523\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.624317\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.618074\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.621894\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.605675\n",
            "resetting env. episode 71.000000, reward total was -18.000000. running mean: -20.579618\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.583822\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.587983\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.582104\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.576283\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.570520\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.574815\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.569066\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.573376\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.567642\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.571966\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.576246\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.570483\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.554779\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.559231\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.563639\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.568002\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.572322\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.576599\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.580833\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.575025\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.579274\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.583482\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.577647\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.581870\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.586052\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.580191\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.584389\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.578545\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.582760\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.586932\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.591063\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.595152\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.589201\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.593309\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.587376\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.591502\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.595587\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.589631\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.583735\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.577897\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.562118\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.556497\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.550932\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.545423\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.549969\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.554469\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.538924\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.543535\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.528100\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.532819\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.537491\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.542116\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.526694\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.531428\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.516113\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.500952\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.495943\n",
            "resetting env. episode 129.000000, reward total was -18.000000. running mean: -20.470983\n",
            "resetting env. episode 130.000000, reward total was -18.000000. running mean: -20.446273\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.441811\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.427393\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.433119\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.438787\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.434400\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.430056\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.435755\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.431397\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.427083\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.432813\n",
            "resetting env. episode 141.000000, reward total was -18.000000. running mean: -20.408484\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.414400\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.420256\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.416053\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.411893\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.407774\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.413696\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.419559\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.425363\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.421110\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.426899\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.422630\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.428403\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.434119\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.429778\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.435480\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.441126\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.436714\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.442347\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.447924\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.423444\n",
            "resetting env. episode 162.000000, reward total was -17.000000. running mean: -20.389210\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.395318\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.391365\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.397451\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.393477\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.389542\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.375646\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.371890\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.378171\n",
            "resetting env. episode 171.000000, reward total was -18.000000. running mean: -20.354389\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.340845\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.337437\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.344063\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.350622\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.357116\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.363545\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.369909\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.376210\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.382448\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -20.368623\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.364937\n",
            "resetting env. episode 183.000000, reward total was -17.000000. running mean: -20.331288\n",
            "resetting env. episode 184.000000, reward total was -18.000000. running mean: -20.307975\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.304895\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.301846\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.308828\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.305740\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.302682\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.299655\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.306659\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.313592\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.320456\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.317252\n",
            "resetting env. episode 195.000000, reward total was -19.000000. running mean: -20.304079\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.311038\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.317928\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.314749\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.321601\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.318385\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.325201\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.331949\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.328630\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.325344\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.332090\n",
            "resetting env. episode 206.000000, reward total was -19.000000. running mean: -20.318769\n",
            "resetting env. episode 207.000000, reward total was -18.000000. running mean: -20.295582\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.292626\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.279699\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.286902\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.294033\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.301093\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.308082\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.315001\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.321851\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.328633\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.325346\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.332093\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.328772\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.315484\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.312330\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.309206\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.316114\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.322953\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.319723\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.316526\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.313361\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.320227\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.327025\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.323755\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.310517\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.307412\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.314338\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.321195\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.317983\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.304803\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.301755\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.308737\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.315650\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.312493\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.319368\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.316175\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.323013\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.329783\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.326485\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.333220\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.319888\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.326689\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.333422\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.340088\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.346687\n",
            "resetting env. episode 252.000000, reward total was -18.000000. running mean: -20.323220\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.319988\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.326788\n",
            "resetting env. episode 255.000000, reward total was -18.000000. running mean: -20.303520\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.290485\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.297580\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.304604\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.311558\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.318443\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.315258\n",
            "resetting env. episode 262.000000, reward total was -18.000000. running mean: -20.292106\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.289185\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.296293\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.303330\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.300297\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.307294\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.294221\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.291279\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.288366\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.295482\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.292527\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.299602\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.286606\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.293740\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.300803\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.287795\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.294917\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.301967\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.298948\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.295958\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.302999\n",
            "resetting env. episode 283.000000, reward total was -18.000000. running mean: -20.279969\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.287169\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.284297\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.281454\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.268640\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.265953\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.253294\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.250761\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.258253\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.265671\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.273014\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.280284\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.277481\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.274706\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.261959\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.259340\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.256746\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.264179\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.271537\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.278822\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.286033\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.283173\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.280341\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.287538\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.294663\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.301716\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.288699\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.295812\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.292854\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.299925\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.306926\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.313857\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.310718\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.317611\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.324435\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.321190\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.317979\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.324799\n",
            "resetting env. episode 321.000000, reward total was -18.000000. running mean: -20.301551\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.298535\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.305550\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.302494\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.299469\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.306475\n",
            "resetting env. episode 327.000000, reward total was -17.000000. running mean: -20.273410\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.270676\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.277969\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.285189\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.292338\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.299414\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.296420\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.293456\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.300521\n",
            "resetting env. episode 336.000000, reward total was -18.000000. running mean: -20.277516\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.274741\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.281993\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.279174\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.276382\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.273618\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.280882\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.278073\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.265292\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.272639\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.279913\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.277114\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.284343\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.291499\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.288584\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.295698\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.282741\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.279914\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.287115\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.284244\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.271401\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.278687\n",
            "resetting env. episode 358.000000, reward total was -18.000000. running mean: -20.255900\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.243341\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.230908\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.238599\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.246213\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.253751\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.261213\n",
            "resetting env. episode 365.000000, reward total was -18.000000. running mean: -20.238601\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.246215\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.243753\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.231315\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.239002\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.246612\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.244146\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.251705\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.249188\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.246696\n",
            "resetting env. episode 375.000000, reward total was -18.000000. running mean: -20.224229\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.231987\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.229667\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.217370\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.215196\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.213044\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.220914\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.228705\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.236418\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.244054\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.251613\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.259097\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.266506\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.253841\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.251302\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.258789\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.266202\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.263539\n",
            "resetting env. episode 393.000000, reward total was -18.000000. running mean: -20.240904\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.248495\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.256010\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.263450\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.260816\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.258207\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.265625\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.272969\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.280239\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.277437\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.274663\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.271916\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.279197\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.276405\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.263641\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.271004\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.258294\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.255711\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.253154\n",
            "resetting env. episode 412.000000, reward total was -18.000000. running mean: -20.230623\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.228316\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.216033\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.223873\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.221634\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.229418\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.237124\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.234753\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.242405\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.249981\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.257481\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.254906\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.252357\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.249834\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.237335\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.234962\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.242612\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.250186\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.247684\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.255208\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.262655\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.260029\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.267429\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.254754\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.242207\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.239785\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.237387\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.235013\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.242663\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.240236\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.237834\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.235456\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.243101\n",
            "resetting env. episode 445.000000, reward total was -18.000000. running mean: -20.220670\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.208463\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.216379\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.204215\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.212173\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.210051\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.217950\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.215771\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.203613\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.201577\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.189561\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.197666\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.195689\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.193732\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.191795\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.179877\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.168078\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.156397\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.144833\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.153385\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.151851\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.150333\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.138829\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.147441\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.145967\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.154507\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.162962\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.171332\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.179619\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.177823\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.186045\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.174184\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.182442\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.180618\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.168812\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.167124\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.155452\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.143898\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.152459\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -20.130934\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.119625\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.128429\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.127144\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.135873\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.144514\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.133069\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.131738\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.130421\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.129117\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.117826\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.126647\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.115381\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.124227\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.112985\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.121855\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.120636\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.129430\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -20.128136\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.126854\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.125586\n",
            "resetting env. episode 505.000000, reward total was -19.000000. running mean: -20.114330\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.113187\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.112055\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.120934\n",
            "resetting env. episode 509.000000, reward total was -20.000000. running mean: -20.119725\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.128528\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.127242\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.135970\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.134610\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.133264\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.141932\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -20.130512\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.129207\n",
            "resetting env. episode 518.000000, reward total was -19.000000. running mean: -20.117915\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.116736\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.115569\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.114413\n",
            "resetting env. episode 522.000000, reward total was -19.000000. running mean: -20.103269\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.092236\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.101314\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.110301\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.119197\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.128006\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.136725\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.135358\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.134005\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.132665\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.141338\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.139925\n",
            "resetting env. episode 534.000000, reward total was -19.000000. running mean: -20.128525\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.137240\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.145868\n",
            "resetting env. episode 537.000000, reward total was -20.000000. running mean: -20.144409\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.152965\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.161435\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.169821\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.178123\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.186341\n",
            "resetting env. episode 543.000000, reward total was -20.000000. running mean: -20.184478\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.192633\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.190707\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -20.178800\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.187012\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.185142\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.193290\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.191357\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.189444\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.197549\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.185574\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.183718\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.191881\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.189962\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.198063\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.206082\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.214021\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.221881\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.219662\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.217465\n",
            "resetting env. episode 563.000000, reward total was -20.000000. running mean: -20.215291\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -20.203138\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.211107\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.218995\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.226806\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.224537\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -20.212292\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.220169\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.217967\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.225788\n",
            "resetting env. episode 573.000000, reward total was -18.000000. running mean: -20.203530\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.211495\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.219380\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.217186\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.225014\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.232764\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.240436\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.238032\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.245652\n",
            "resetting env. episode 582.000000, reward total was -19.000000. running mean: -20.233195\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.240863\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.248454\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.255970\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.263410\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.260776\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.248168\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.255687\n",
            "resetting env. episode 590.000000, reward total was -18.000000. running mean: -20.233130\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.240799\n",
            "resetting env. episode 592.000000, reward total was -19.000000. running mean: -20.228391\n",
            "resetting env. episode 593.000000, reward total was -19.000000. running mean: -20.216107\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.223946\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.231706\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.239389\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.236995\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.234625\n",
            "resetting env. episode 599.000000, reward total was -19.000000. running mean: -20.222279\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -20.220056\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.227856\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.235577\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.243221\n",
            "resetting env. episode 604.000000, reward total was -19.000000. running mean: -20.230789\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.238481\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.246096\n",
            "resetting env. episode 607.000000, reward total was -19.000000. running mean: -20.233635\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.241299\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.238886\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.246497\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.244032\n",
            "resetting env. episode 612.000000, reward total was -20.000000. running mean: -20.241592\n",
            "resetting env. episode 613.000000, reward total was -19.000000. running mean: -20.229176\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.236884\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.234515\n",
            "resetting env. episode 616.000000, reward total was -17.000000. running mean: -20.202170\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.200149\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.198147\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.206166\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.214104\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.221963\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.229743\n",
            "resetting env. episode 623.000000, reward total was -20.000000. running mean: -20.227446\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -20.225171\n",
            "resetting env. episode 625.000000, reward total was -17.000000. running mean: -20.192920\n",
            "resetting env. episode 626.000000, reward total was -20.000000. running mean: -20.190990\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.199081\n",
            "resetting env. episode 628.000000, reward total was -19.000000. running mean: -20.187090\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -20.185219\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.193367\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.201433\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.209419\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.207324\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.215251\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.223099\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.220868\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.228659\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.236372\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.244009\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.241569\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.249153\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.246661\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.254195\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.251653\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.259136\n",
            "resetting env. episode 646.000000, reward total was -19.000000. running mean: -20.246545\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -20.244080\n",
            "resetting env. episode 648.000000, reward total was -19.000000. running mean: -20.231639\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -20.219322\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.217129\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.214958\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.222808\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.230580\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.238274\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.245892\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.243433\n",
            "resetting env. episode 657.000000, reward total was -18.000000. running mean: -20.220998\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.218788\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.226600\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.224334\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.232091\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.229770\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -20.217473\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.215298\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.213145\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.221013\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.208803\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.216715\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.224548\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.232303\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.229980\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.227680\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.225403\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.233149\n",
            "resetting env. episode 675.000000, reward total was -19.000000. running mean: -20.220817\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.208609\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.206523\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -20.204458\n",
            "resetting env. episode 679.000000, reward total was -19.000000. running mean: -20.192413\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.200489\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.198484\n",
            "resetting env. episode 682.000000, reward total was -17.000000. running mean: -20.166499\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.174835\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.173086\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.181355\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.189542\n",
            "resetting env. episode 687.000000, reward total was -19.000000. running mean: -20.177646\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.185870\n",
            "resetting env. episode 689.000000, reward total was -20.000000. running mean: -20.184011\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.182171\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.180349\n",
            "resetting env. episode 692.000000, reward total was -19.000000. running mean: -20.168546\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.176860\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.185092\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -20.173241\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.181508\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.189693\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.197796\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -20.185818\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.183960\n",
            "resetting env. episode 701.000000, reward total was -18.000000. running mean: -20.162121\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.170499\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.168794\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.167107\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.165435\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.153781\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.152243\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.160721\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.169114\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.167423\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -20.155748\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.154191\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.152649\n",
            "resetting env. episode 714.000000, reward total was -18.000000. running mean: -20.131122\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.129811\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.138513\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.137128\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.145757\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.144299\n",
            "resetting env. episode 720.000000, reward total was -19.000000. running mean: -20.132856\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.141528\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.150112\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.158611\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.167025\n",
            "resetting env. episode 725.000000, reward total was -19.000000. running mean: -20.155355\n",
            "resetting env. episode 726.000000, reward total was -18.000000. running mean: -20.133801\n",
            "resetting env. episode 727.000000, reward total was -19.000000. running mean: -20.122463\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.131239\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -20.129926\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.138627\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.147241\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.145768\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -20.144311\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.142867\n",
            "resetting env. episode 735.000000, reward total was -18.000000. running mean: -20.121439\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.120224\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.129022\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -20.127732\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.136455\n",
            "resetting env. episode 740.000000, reward total was -19.000000. running mean: -20.125090\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.133839\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.142501\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.141076\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.149665\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -20.148168\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.146687\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.145220\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.153768\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.162230\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.170608\n",
            "resetting env. episode 751.000000, reward total was -18.000000. running mean: -20.148902\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.147413\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.155938\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.154379\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.152835\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.161307\n",
            "resetting env. episode 757.000000, reward total was -18.000000. running mean: -20.139694\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.148297\n",
            "resetting env. episode 759.000000, reward total was -19.000000. running mean: -20.136814\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.145446\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -20.143991\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.152551\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.161026\n",
            "resetting env. episode 764.000000, reward total was -19.000000. running mean: -20.149416\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.157922\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.166342\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.164679\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -20.163032\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -20.151402\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.159888\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.168289\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.166606\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.164940\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -20.163291\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.171658\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.179941\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.188142\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -20.176260\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.174498\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.182753\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.190925\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.199016\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.197026\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.195055\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.203105\n",
            "resetting env. episode 786.000000, reward total was -19.000000. running mean: -20.191074\n",
            "resetting env. episode 787.000000, reward total was -19.000000. running mean: -20.179163\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.177371\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -20.165598\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -20.153942\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -20.142402\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.150978\n",
            "resetting env. episode 793.000000, reward total was -20.000000. running mean: -20.149469\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.157974\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.166394\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.174730\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.182983\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.191153\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.199242\n",
            "resetting env. episode 800.000000, reward total was -18.000000. running mean: -20.177249\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.185477\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.193622\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -20.191686\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.199769\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.207771\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -20.195693\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.193736\n",
            "resetting env. episode 808.000000, reward total was -20.000000. running mean: -20.191799\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.199881\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.207882\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.205803\n",
            "resetting env. episode 812.000000, reward total was -19.000000. running mean: -20.193745\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.191808\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.199890\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.197891\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -20.195912\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.203953\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.211913\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.219794\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.227596\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -20.215320\n",
            "resetting env. episode 822.000000, reward total was -19.000000. running mean: -20.203167\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.211136\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -20.199024\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.197034\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.195064\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.193113\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.201182\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.199170\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -20.187178\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.185307\n",
            "resetting env. episode 832.000000, reward total was -17.000000. running mean: -20.153453\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.161919\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.160300\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.158697\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.167110\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.175439\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.183684\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.191847\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.199929\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.207930\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -20.195850\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -20.183892\n",
            "resetting env. episode 844.000000, reward total was -19.000000. running mean: -20.172053\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.180332\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.188529\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.186644\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.194777\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.202830\n",
            "resetting env. episode 850.000000, reward total was -19.000000. running mean: -20.190801\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.188893\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.187004\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.195134\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.193183\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.201251\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.199239\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.207246\n",
            "resetting env. episode 858.000000, reward total was -19.000000. running mean: -20.195174\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.193222\n",
            "resetting env. episode 860.000000, reward total was -19.000000. running mean: -20.181290\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -20.179477\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.187682\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.195805\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -20.193847\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.201909\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.209890\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.217791\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.215613\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.213457\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.211322\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.219209\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.227017\n",
            "resetting env. episode 873.000000, reward total was -19.000000. running mean: -20.214747\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -20.212599\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.220473\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.218269\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.226086\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.233825\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -20.221487\n",
            "resetting env. episode 880.000000, reward total was -19.000000. running mean: -20.209272\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.217179\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.225007\n",
            "resetting env. episode 883.000000, reward total was -19.000000. running mean: -20.212757\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.210630\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.208523\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.216438\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.214274\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.222131\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -20.209910\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.217811\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.225633\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.233376\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.241042\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.238632\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -20.226246\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.233983\n",
            "resetting env. episode 897.000000, reward total was -19.000000. running mean: -20.221643\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.229427\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -20.227133\n",
            "resetting env. episode 900.000000, reward total was -18.000000. running mean: -20.204861\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.202813\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.210785\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.218677\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.216490\n",
            "resetting env. episode 905.000000, reward total was -19.000000. running mean: -20.204325\n",
            "resetting env. episode 906.000000, reward total was -17.000000. running mean: -20.172282\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.170559\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.178853\n",
            "resetting env. episode 909.000000, reward total was -19.000000. running mean: -20.167065\n",
            "resetting env. episode 910.000000, reward total was -19.000000. running mean: -20.155394\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.153840\n",
            "resetting env. episode 912.000000, reward total was -20.000000. running mean: -20.152302\n",
            "resetting env. episode 913.000000, reward total was -17.000000. running mean: -20.120779\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.129571\n",
            "resetting env. episode 915.000000, reward total was -18.000000. running mean: -20.108275\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.117193\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.126021\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.124761\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.133513\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.142178\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.150756\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.149248\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.147756\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.156278\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.154716\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.163168\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.171537\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.179821\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.178023\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -20.176243\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.184481\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.182636\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.190809\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.178901\n",
            "resetting env. episode 935.000000, reward total was -18.000000. running mean: -20.157112\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.155541\n",
            "resetting env. episode 937.000000, reward total was -19.000000. running mean: -20.143986\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.152546\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -20.141020\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.139610\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.138214\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.146832\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.155364\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.153810\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -20.152272\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -20.140749\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -20.129342\n",
            "resetting env. episode 948.000000, reward total was -19.000000. running mean: -20.118048\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.116868\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.125699\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.134442\n",
            "resetting env. episode 952.000000, reward total was -18.000000. running mean: -20.113098\n",
            "resetting env. episode 953.000000, reward total was -18.000000. running mean: -20.091967\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.091047\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.090137\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.099235\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.108243\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.107160\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.116089\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.114928\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.113779\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -20.102641\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.101614\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.100598\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -20.099592\n",
            "resetting env. episode 966.000000, reward total was -19.000000. running mean: -20.088596\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.087710\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.086833\n",
            "resetting env. episode 969.000000, reward total was -18.000000. running mean: -20.065965\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.065305\n",
            "resetting env. episode 971.000000, reward total was -18.000000. running mean: -20.044652\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.054206\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.053664\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.053127\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.062596\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.061970\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.061350\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.070737\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.080029\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.089229\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.098337\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.107353\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -20.106280\n",
            "resetting env. episode 984.000000, reward total was -19.000000. running mean: -20.095217\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.104265\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -20.093222\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.092290\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.091367\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.090453\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.089549\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.088653\n",
            "resetting env. episode 992.000000, reward total was -19.000000. running mean: -20.077767\n",
            "resetting env. episode 993.000000, reward total was -19.000000. running mean: -20.066989\n",
            "resetting env. episode 994.000000, reward total was -17.000000. running mean: -20.036319\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -20.035956\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.045597\n",
            "resetting env. episode 997.000000, reward total was -20.000000. running mean: -20.045141\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.054689\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.064142\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.073501\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.082766\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.091938\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -20.091019\n",
            "resetting env. episode 1004.000000, reward total was -18.000000. running mean: -20.070109\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.079408\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.088613\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -20.077727\n",
            "resetting env. episode 1008.000000, reward total was -19.000000. running mean: -20.066950\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.066281\n",
            "resetting env. episode 1010.000000, reward total was -18.000000. running mean: -20.045618\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.045162\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.054710\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.064163\n",
            "resetting env. episode 1014.000000, reward total was -16.000000. running mean: -20.023521\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.033286\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -20.022953\n",
            "resetting env. episode 1017.000000, reward total was -19.000000. running mean: -20.012724\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -20.002596\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.012570\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -20.012445\n",
            "resetting env. episode 1021.000000, reward total was -18.000000. running mean: -19.992320\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.002397\n",
            "resetting env. episode 1023.000000, reward total was -19.000000. running mean: -19.992373\n",
            "resetting env. episode 1024.000000, reward total was -19.000000. running mean: -19.982449\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -19.982625\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -19.992799\n",
            "resetting env. episode 1027.000000, reward total was -18.000000. running mean: -19.972871\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -19.963142\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -19.963511\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -19.963875\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -19.974237\n",
            "resetting env. episode 1032.000000, reward total was -19.000000. running mean: -19.964494\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -19.964849\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -19.965201\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -19.965549\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -19.975893\n",
            "resetting env. episode 1037.000000, reward total was -20.000000. running mean: -19.976134\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -19.976373\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -19.986609\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -19.986743\n",
            "resetting env. episode 1041.000000, reward total was -17.000000. running mean: -19.956876\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -19.967307\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -19.977634\n",
            "resetting env. episode 1044.000000, reward total was -19.000000. running mean: -19.967858\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -19.978179\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -19.968397\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -19.968713\n",
            "resetting env. episode 1048.000000, reward total was -17.000000. running mean: -19.939026\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -19.939636\n",
            "resetting env. episode 1050.000000, reward total was -19.000000. running mean: -19.930240\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -19.930937\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -19.941628\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -19.952212\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -19.952689\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -19.963163\n",
            "resetting env. episode 1056.000000, reward total was -19.000000. running mean: -19.953531\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -19.963996\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -19.974356\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -19.984612\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -19.994766\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.004818\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.014770\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.014622\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.024476\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.034231\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.043889\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.043450\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.043016\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.052586\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.052060\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -20.051539\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -20.041024\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.040613\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.050207\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.049705\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.059208\n",
            "resetting env. episode 1077.000000, reward total was -20.000000. running mean: -20.058616\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.068030\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -20.067350\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -20.056676\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -20.046109\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.055648\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.065092\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -20.054441\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.063896\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.073258\n",
            "resetting env. episode 1087.000000, reward total was -19.000000. running mean: -20.062525\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -20.061900\n",
            "resetting env. episode 1089.000000, reward total was -21.000000. running mean: -20.071281\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -20.070568\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.079862\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.069064\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.078373\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.077589\n",
            "resetting env. episode 1095.000000, reward total was -19.000000. running mean: -20.066813\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -20.066145\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.065484\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.074829\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.084081\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.093240\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -20.082307\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -20.061484\n",
            "resetting env. episode 1103.000000, reward total was -17.000000. running mean: -20.030870\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.040561\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.050155\n",
            "resetting env. episode 1106.000000, reward total was -19.000000. running mean: -20.039654\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.049257\n",
            "resetting env. episode 1108.000000, reward total was -18.000000. running mean: -20.028765\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.038477\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.038092\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.047711\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.047234\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.046762\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.056294\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.065731\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.075074\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.074323\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.073580\n",
            "resetting env. episode 1119.000000, reward total was -15.000000. running mean: -20.022844\n",
            "resetting env. episode 1120.000000, reward total was -19.000000. running mean: -20.012616\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -20.012490\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -20.012365\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -20.012241\n",
            "resetting env. episode 1124.000000, reward total was -19.000000. running mean: -20.002119\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.012097\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.021976\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.031757\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.041439\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.051025\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.060514\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.069909\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -20.069210\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -20.068518\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -20.057833\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.067255\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.066582\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.075916\n",
            "resetting env. episode 1138.000000, reward total was -19.000000. running mean: -20.065157\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.074505\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -20.063760\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.063123\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -20.052492\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.061967\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.061347\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.060734\n",
            "resetting env. episode 1146.000000, reward total was -18.000000. running mean: -20.040126\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.049725\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.049228\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.058735\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.058148\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -20.057567\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.066991\n",
            "resetting env. episode 1153.000000, reward total was -18.000000. running mean: -20.046321\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -20.045858\n",
            "resetting env. episode 1155.000000, reward total was -19.000000. running mean: -20.035399\n",
            "resetting env. episode 1156.000000, reward total was -19.000000. running mean: -20.025045\n",
            "resetting env. episode 1157.000000, reward total was -17.000000. running mean: -19.994795\n",
            "resetting env. episode 1158.000000, reward total was -19.000000. running mean: -19.984847\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -19.984998\n",
            "resetting env. episode 1160.000000, reward total was -19.000000. running mean: -19.975148\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -19.975397\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -19.985643\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -19.995787\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -19.995829\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -19.995870\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.005912\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -19.995853\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.005894\n",
            "resetting env. episode 1169.000000, reward total was -19.000000. running mean: -19.995835\n",
            "resetting env. episode 1170.000000, reward total was -20.000000. running mean: -19.995877\n",
            "resetting env. episode 1171.000000, reward total was -19.000000. running mean: -19.985918\n",
            "resetting env. episode 1172.000000, reward total was -19.000000. running mean: -19.976059\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -19.976298\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -19.986535\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -19.996670\n",
            "resetting env. episode 1176.000000, reward total was -19.000000. running mean: -19.986703\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -19.996836\n",
            "resetting env. episode 1178.000000, reward total was -17.000000. running mean: -19.966868\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -19.967199\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -19.957527\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -19.957952\n",
            "resetting env. episode 1182.000000, reward total was -18.000000. running mean: -19.938372\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -19.948989\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -19.939499\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -19.950104\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -19.960603\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -19.970997\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -19.971287\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -19.971574\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -19.981858\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -19.972039\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -19.982319\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -19.992496\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.002571\n",
            "resetting env. episode 1195.000000, reward total was -19.000000. running mean: -19.992545\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.002620\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -20.002594\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.012568\n",
            "resetting env. episode 1199.000000, reward total was -19.000000. running mean: -20.002442\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.012418\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.002293\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -20.002270\n",
            "resetting env. episode 1203.000000, reward total was -18.000000. running mean: -19.982248\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -19.992425\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -19.982501\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -19.992676\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.002749\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -20.002722\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.002695\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.012668\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.012541\n",
            "resetting env. episode 1212.000000, reward total was -18.000000. running mean: -19.992415\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -19.982491\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -19.982666\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -19.982840\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -19.973011\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -19.983281\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -19.993448\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.003514\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.003479\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.013444\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -20.003310\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -20.003276\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -19.993244\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -19.993311\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.003378\n",
            "resetting env. episode 1227.000000, reward total was -19.000000. running mean: -19.993344\n",
            "resetting env. episode 1228.000000, reward total was -17.000000. running mean: -19.963411\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -19.973777\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -19.974039\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -19.964299\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -19.974656\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.984909\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -19.985060\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -19.975209\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -19.975457\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -19.985703\n",
            "resetting env. episode 1238.000000, reward total was -19.000000. running mean: -19.975846\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -19.986087\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -19.976226\n",
            "resetting env. episode 1241.000000, reward total was -19.000000. running mean: -19.966464\n",
            "resetting env. episode 1242.000000, reward total was -20.000000. running mean: -19.966800\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -19.977132\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -19.977360\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -19.987587\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -19.987711\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -19.997834\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.007855\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.017777\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -20.007599\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.007523\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.007448\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.007373\n",
            "resetting env. episode 1254.000000, reward total was -18.000000. running mean: -19.987300\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -19.977427\n",
            "resetting env. episode 1256.000000, reward total was -18.000000. running mean: -19.957652\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -19.958076\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -19.948495\n",
            "resetting env. episode 1259.000000, reward total was -19.000000. running mean: -19.939010\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -19.949620\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -19.940124\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -19.940723\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -19.951315\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -19.961802\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -19.972184\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -19.982462\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -19.992638\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.002711\n",
            "resetting env. episode 1269.000000, reward total was -19.000000. running mean: -19.992684\n",
            "resetting env. episode 1270.000000, reward total was -19.000000. running mean: -19.982757\n",
            "resetting env. episode 1271.000000, reward total was -17.000000. running mean: -19.952930\n",
            "resetting env. episode 1272.000000, reward total was -17.000000. running mean: -19.923400\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -19.934166\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -19.924825\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -19.935577\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -19.946221\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -19.956759\n",
            "resetting env. episode 1278.000000, reward total was -19.000000. running mean: -19.947191\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -19.957719\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -19.958142\n",
            "resetting env. episode 1281.000000, reward total was -20.000000. running mean: -19.958560\n",
            "resetting env. episode 1282.000000, reward total was -19.000000. running mean: -19.948975\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -19.939485\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -19.950090\n",
            "resetting env. episode 1285.000000, reward total was -18.000000. running mean: -19.930589\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -19.941283\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -19.941871\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -19.952452\n",
            "resetting env. episode 1289.000000, reward total was -18.000000. running mean: -19.932927\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -19.933598\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -19.944262\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -19.944820\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -19.945371\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -19.945918\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -19.946458\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -19.956994\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -19.967424\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -19.977750\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -19.977972\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -19.978192\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -19.988411\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -19.998526\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -19.998541\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -19.988556\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.988670\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -19.998783\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.008796\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.008708\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.008621\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -19.998534\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -19.998549\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -19.988564\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -19.988678\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -19.998791\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.008803\n",
            "resetting env. episode 1316.000000, reward total was -18.000000. running mean: -19.988715\n",
            "resetting env. episode 1317.000000, reward total was -19.000000. running mean: -19.978828\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -19.989040\n",
            "resetting env. episode 1319.000000, reward total was -19.000000. running mean: -19.979149\n",
            "resetting env. episode 1320.000000, reward total was -18.000000. running mean: -19.959358\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -19.959764\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -19.960167\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -19.970565\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -19.980859\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -19.991051\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.001140\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.011129\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -20.011018\n",
            "resetting env. episode 1329.000000, reward total was -18.000000. running mean: -19.990907\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -19.990998\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -19.991088\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -19.981177\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -19.991366\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.001452\n",
            "resetting env. episode 1335.000000, reward total was -19.000000. running mean: -19.991437\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -19.991523\n",
            "resetting env. episode 1337.000000, reward total was -18.000000. running mean: -19.971608\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -19.971892\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -19.982173\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -19.982351\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -19.992528\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -19.992602\n",
            "resetting env. episode 1343.000000, reward total was -15.000000. running mean: -19.942676\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -19.933250\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -19.943917\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -19.954478\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -19.964933\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -19.975284\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -19.975531\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -19.985776\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -19.995918\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -19.995959\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -19.995999\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.006039\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -19.995979\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -19.996019\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -19.996059\n",
            "resetting env. episode 1358.000000, reward total was -16.000000. running mean: -19.956098\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -19.946537\n",
            "resetting env. episode 1360.000000, reward total was -19.000000. running mean: -19.937072\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -19.947701\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -19.958224\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -19.958642\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -19.969055\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -19.969365\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -19.979671\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -19.989875\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -19.989976\n",
            "resetting env. episode 1369.000000, reward total was -18.000000. running mean: -19.970076\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -19.980375\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -19.990572\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -19.990666\n",
            "resetting env. episode 1373.000000, reward total was -18.000000. running mean: -19.970759\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -19.971052\n",
            "resetting env. episode 1375.000000, reward total was -18.000000. running mean: -19.951341\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -19.951828\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -19.952309\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -19.952786\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -19.953258\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -19.953726\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -19.964189\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -19.974547\n",
            "resetting env. episode 1383.000000, reward total was -16.000000. running mean: -19.934801\n",
            "resetting env. episode 1384.000000, reward total was -19.000000. running mean: -19.925453\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -19.936199\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -19.946837\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -19.947368\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -19.957895\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -19.958316\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -19.968733\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -19.969045\n",
            "resetting env. episode 1392.000000, reward total was -19.000000. running mean: -19.959355\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -19.969761\n",
            "resetting env. episode 1394.000000, reward total was -19.000000. running mean: -19.960064\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -19.960463\n",
            "resetting env. episode 1396.000000, reward total was -16.000000. running mean: -19.920858\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -19.931650\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -19.932333\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -19.943010\n",
            "resetting env. episode 1400.000000, reward total was -19.000000. running mean: -19.933580\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -19.934244\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -19.944902\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -19.945453\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -19.955998\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -19.956438\n",
            "resetting env. episode 1406.000000, reward total was -17.000000. running mean: -19.926874\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -19.937605\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -19.938229\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -19.948847\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -19.959358\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -19.959765\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -19.970167\n",
            "resetting env. episode 1413.000000, reward total was -18.000000. running mean: -19.950465\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -19.940961\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.941551\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -19.952135\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.952614\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -19.963088\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -19.963457\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -19.963823\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -19.964184\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -19.964542\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -19.974897\n",
            "resetting env. episode 1424.000000, reward total was -19.000000. running mean: -19.965148\n",
            "resetting env. episode 1425.000000, reward total was -18.000000. running mean: -19.945497\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -19.946042\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -19.946581\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -19.947115\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -19.947644\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -19.948168\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -19.948686\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -19.939199\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -19.949807\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -19.950309\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -19.950806\n",
            "resetting env. episode 1436.000000, reward total was -19.000000. running mean: -19.941298\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -19.941885\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -19.952466\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -19.942942\n",
            "resetting env. episode 1440.000000, reward total was -18.000000. running mean: -19.923512\n",
            "resetting env. episode 1441.000000, reward total was -17.000000. running mean: -19.894277\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -19.905334\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -19.916281\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -19.907118\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -19.908047\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -19.918966\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -19.919777\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -19.920579\n",
            "resetting env. episode 1449.000000, reward total was -19.000000. running mean: -19.911373\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -19.902259\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -19.893237\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -19.904305\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -19.905261\n",
            "resetting env. episode 1454.000000, reward total was -18.000000. running mean: -19.886209\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -19.887347\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -19.888473\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -19.889589\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -19.900693\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -19.911686\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -19.922569\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -19.933343\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -19.944010\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -19.944570\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -19.945124\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -19.945673\n",
            "resetting env. episode 1466.000000, reward total was -19.000000. running mean: -19.936216\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -19.946854\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -19.947385\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -19.947911\n",
            "resetting env. episode 1470.000000, reward total was -18.000000. running mean: -19.928432\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -19.929148\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -19.939857\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -19.940458\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -19.951053\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -19.951543\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.962027\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -19.952407\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.962883\n",
            "resetting env. episode 1479.000000, reward total was -16.000000. running mean: -19.923254\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -19.934022\n",
            "resetting env. episode 1481.000000, reward total was -19.000000. running mean: -19.924681\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -19.925435\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -19.936180\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -19.946819\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -19.957350\n",
            "resetting env. episode 1486.000000, reward total was -20.000000. running mean: -19.957777\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -19.968199\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.968517\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -19.968832\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -19.969144\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -19.979452\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -19.979658\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -19.989861\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -19.989962\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -19.990063\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -19.990162\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.000261\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -20.000258\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.010255\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -20.010153\n",
            "CPU times: user 1h 57min 23s, sys: 38min 13s, total: 2h 35min 37s\n",
            "Wall time: 1h 21min 15s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "w2NblmwDsL3y",
        "outputId": "6b8a8c0b-c5ab-412f-8a4c-a371e5252bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=400_le_4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}