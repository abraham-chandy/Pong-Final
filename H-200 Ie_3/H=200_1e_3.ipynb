{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cWACPRL869I4","executionInfo":{"status":"ok","timestamp":1660931965143,"user_tz":420,"elapsed":3693,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[],"source":["!pip install gym >/dev/null"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2Os6feRY6ec_","executionInfo":{"status":"ok","timestamp":1660931970357,"user_tz":420,"elapsed":5220,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[],"source":["!pip install JSAnimation >/dev/null"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wotUOa_e6edP","executionInfo":{"status":"ok","timestamp":1660931970358,"user_tz":420,"elapsed":14,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[],"source":["%matplotlib inline\n","from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"R66_INeZ9nYX"},"source":["## Step 2: Playing Pong"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ngMhg3fB9aA","outputId":"23bbefe9-6d79-48c1-d7d5-61f65915418d","executionInfo":{"status":"ok","timestamp":1660931998258,"user_tz":420,"elapsed":27913,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n","Collecting ale-py~=0.7.5\n","  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 30.6 MB/s \n","\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=e3c90cc4d230153975397fe4e68cb3dcb3f1d2e4e6fa74be5e42fcb9e1b5ba5f\n","  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"]}],"source":["%pip install -U gym>=0.21.0\n","%pip install -U gym[atari,accept-rom-license]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtT2GyK_6edc","outputId":"fe483e61-3329-449e-f8aa-824d014b8195","executionInfo":{"status":"ok","timestamp":1660931998747,"user_tz":420,"elapsed":506,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  f\"The environment {id} is out of date. You should consider \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"]}],"source":["import gym\n","env = gym.make('Pong-v0')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRE6WmXQJ1Z0","outputId":"877d1a99-e496-48eb-d7d4-170197bd7f6d","executionInfo":{"status":"ok","timestamp":1660931998748,"user_tz":420,"elapsed":22,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(6)"]},"metadata":{},"execution_count":6}],"source":["env.action_space"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yl_9d4HFJ31W","outputId":"c4c3df56-d646-4fdd-f624-e38352d0d0ba","executionInfo":{"status":"ok","timestamp":1660931998750,"user_tz":420,"elapsed":21,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0, 255, (210, 160, 3), uint8)"]},"metadata":{},"execution_count":7}],"source":["env.observation_space"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trwRXI-h6eeI","outputId":"0bc0f7f1-dc7a-4370-809d-2fd59cadb0b6","executionInfo":{"status":"ok","timestamp":1660931999117,"user_tz":420,"elapsed":385,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  \"The argument mode in render method is deprecated; \"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n","  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  \"Core environment is written in old step API which returns one bool instead of two. \"\n"]},{"output_type":"stream","name":"stdout","text":["Episode finished without success, accumulated reward = -16.0\n"]}],"source":["# Run a demo of the environment\n","observation = env.reset()\n","cumulated_reward = 0\n","\n","frames = []\n","for t in range(1000):\n","#     print(observation)\n","    frames.append(env.render(mode = 'rgb_array'))\n","    # very stupid agent, just makes a random action within the allowd action space\n","    action = env.action_space.sample()\n","#     print(\"Action: {}\".format(t+1))    \n","    observation, reward, done, info = env.step(action)\n","#     print(reward)\n","    cumulated_reward += reward\n","    if done:\n","        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","        break\n","print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","\n","env.close()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"3zZTecVWLLes","executionInfo":{"status":"ok","timestamp":1660931999118,"user_tz":420,"elapsed":10,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[],"source":["def sigmoid(x): \n","  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n","\n","def prepro(I):\n","  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n","  I = I[35:195] # crop\n","  I = I[::2,::2,0] # downsample by factor of 2\n","  I[I == 144] = 0 # erase background (background type 1)\n","  I[I == 109] = 0 # erase background (background type 2)\n","  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n","  return I.astype(np.float).ravel()\n","\n","def policy_forward(x):\n","  h = np.dot(model['W1'], x)\n","  h[h<0] = 0 # ReLU nonlinearity\n","  logp = np.dot(model['W2'], h)\n","  p = sigmoid(logp)\n","  return p, h # return probability of taking action 2, and hidden state\n","\n","def model_step(model, observation, prev_x):\n","  # preprocess the observation, set input to network to be difference image\n","  cur_x = prepro(observation)\n","  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","  prev_x = cur_x\n","  \n","  # forward the policy network and sample an action from the returned probability\n","  aprob, _ = policy_forward(x)\n","  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n","  \n","  return action, prev_x\n","\n","def play_game(env, model):\n","  observation = env.reset()\n","\n","  frames = []\n","  cumulated_reward = 0\n","\n","  prev_x = None # used in computing the difference frame\n","\n","  for t in range(1000):\n","      frames.append(env.render(mode = 'rgb_array'))\n","      action, prev_x = model_step(model, observation, prev_x)\n","      observation, reward, done, info = env.step(action)\n","      cumulated_reward += reward\n","      if done:\n","          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","          break\n","  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","  display_frames_as_gif(frames)\n","  env.close()"]},{"cell_type":"markdown","metadata":{"id":"6gWvZQ7AQLQt"},"source":["## Step 3: Policy Gradient from Scratch"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"eqFm7hqcItWl","executionInfo":{"status":"ok","timestamp":1660931999119,"user_tz":420,"elapsed":11,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[],"source":["import numpy as np\n","\n","# model initialization\n","H = 200 # number of hidden layer neurons\n","D = 80 * 80 # input dimensionality: 80x80 grid\n","model = {}\n","model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n","model['W2'] = np.random.randn(H) / np.sqrt(H)\n","\n","# import pickle\n","# model = pickle.load(open('model.pkl', 'rb'))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"TwjiwKisQM19","executionInfo":{"status":"ok","timestamp":1660931999119,"user_tz":420,"elapsed":11,"user":{"displayName":"BBS MDS 2018","userId":"12277414942659798725"}}},"outputs":[],"source":["# hyperparameters\n","batch_size = 10 # every how many episodes to do a param update?\n","# learning_rate = 1e-4\n","learning_rate = 1e-3\n"," \n","gamma = 0.99 # discount factor for reward\n","decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n","  \n","grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n","rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n","\n","def discount_rewards(r):\n","  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n","  discounted_r = np.zeros_like(r, dtype=np.float32)\n","  running_add = 0\n","  for t in reversed(range(0, r.size)):\n","    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n","    running_add = running_add * gamma + r[t]\n","    discounted_r[t] = running_add\n","  return discounted_r\n","\n","def policy_backward(epx, eph, epdlogp):\n","  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n","  dW2 = np.dot(eph.T, epdlogp).ravel()\n","  dh = np.outer(epdlogp, model['W2'])\n","  dh[eph <= 0] = 0 # backpro prelu\n","  dW1 = np.dot(dh.T, epx)\n","  return {'W1':dW1, 'W2':dW2}\n","\n","def train_model(env, model, total_episodes = 100):\n","  hist = []\n","  observation = env.reset()\n","\n","  prev_x = None # used in computing the difference frame\n","  xs,hs,dlogps,drs = [],[],[],[]\n","  running_reward = None\n","  reward_sum = 0\n","  episode_number = 0\n","\n","  while True:\n","    # preprocess the observation, set input to network to be difference image\n","    cur_x = prepro(observation)\n","    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","    prev_x = cur_x\n","\n","    # forward the policy network and sample an action from the returned probability\n","    aprob, h = policy_forward(x)\n","    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n","\n","    # record various intermediates (needed later for backprop)\n","    xs.append(x) # observation\n","    hs.append(h) # hidden state\n","    y = 1 if action == 2 else 0 # a \"fake label\"\n","    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n","\n","    # step the environment and get new measurements\n","    observation, reward, done, info = env.step(action)\n","    reward_sum += reward\n","\n","    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n","\n","    if done: # an episode finished\n","      episode_number += 1\n","\n","      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n","      epx = np.vstack(xs)\n","      eph = np.vstack(hs)\n","      epdlogp = np.vstack(dlogps)\n","      epr = np.vstack(drs)\n","      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n","\n","      # compute the discounted reward backwards through time\n","      discounted_epr = discount_rewards(epr)\n","      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n","      discounted_epr -= np.mean(discounted_epr)\n","      discounted_epr /= np.std(discounted_epr)\n","\n","      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n","      grad = policy_backward(epx, eph, epdlogp)\n","      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n","\n","      # perform rmsprop parameter update every batch_size episodes\n","      if episode_number % batch_size == 0:\n","        for k,v in model.items():\n","          g = grad_buffer[k] # gradient\n","          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n","          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n","          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n","\n","      # boring book-keeping\n","      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n","      hist.append((episode_number, reward_sum, running_reward))\n","      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n","      reward_sum = 0\n","      observation = env.reset() # reset env\n","      prev_x = None\n","      if episode_number == total_episodes: return hist\n","\n","      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n","        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6Ka_5Vl9Orm","outputId":"c9365eb6-c9c2-4da8-83da-9833bdf78ed5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 2.000000, reward total was -19.000000. running mean: -20.980000\n","resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.980200\n","resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980398\n","resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.970594\n","resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.970888\n","resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.971179\n","resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.961467\n","resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.961853\n","resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.962234\n","resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.952612\n","resetting env. episode 12.000000, reward total was -19.000000. running mean: -20.933086\n","resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.933755\n","resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.924417\n","resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.925173\n","resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.925921\n","resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.916662\n","resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.907496\n","resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.908421\n","resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.909336\n"]}],"source":["%time hist1 = train_model(env, model, total_episodes=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHYCDYwhlVLV"},"outputs":[],"source":["%time hist2 = train_model(env, model, total_episodes=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fheN9DRlWXQ"},"outputs":[],"source":["play_game(env, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AxOcQhIsKow"},"outputs":[],"source":["%time hist3 = train_model(env, model, total_episodes=1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2NblmwDsL3y"},"outputs":[],"source":["play_game(env, model)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"H=200_1e_3.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}